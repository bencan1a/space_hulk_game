# Quality Metrics Documentation

**Version:** 1.0
**Created:** November 9, 2025
**Part of:** Phase 3, Chunk 3.1

---

## Overview

This document describes the quality metrics system for the Space Hulk Game project. The metrics provide objective, measurable criteria for evaluating the quality of content generated by the AI crew, including plot outlines, narrative maps, puzzles, scenes, and game mechanics.

**Purpose:** Enable automated quality evaluation, feedback generation, and iterative improvement of generated content.

---

## Metrics Modules

The quality metrics system consists of five specialized modules, each focused on evaluating a specific type of generated content:

1. **PlotMetrics** - Evaluate plot outlines
2. **NarrativeMetrics** - Evaluate narrative maps
3. **PuzzleMetrics** - Evaluate puzzles and artifacts
4. **SceneMetrics** - Evaluate scene descriptions
5. **MechanicsMetrics** - Evaluate game mechanics (PRD documents)

---

## 1. PlotMetrics

### Purpose

Evaluate the quality and completeness of plot outlines generated by the PlotMasterAgent.

### Location

`src/space_hulk_game/quality/plot_metrics.py`

### Measured Criteria

| Metric | Type | Description | Threshold |
|--------|------|-------------|-----------|
| `has_clear_setting` | Boolean | Whether the plot has a clearly defined setting | Required |
| `branching_paths_count` | Count | Number of branching paths in the plot | ≥ 2 |
| `endings_count` | Count | Number of defined endings | ≥ 2 |
| `themes_defined` | Boolean | Whether themes are clearly stated | Required |
| `word_count` | Number | Total word count of the plot outline | ≥ 500 |
| `has_title` | Boolean | Whether the plot has a title | N/A |
| `has_prologue` | Boolean | Whether the plot includes a prologue | N/A |
| `has_acts` | Boolean | Whether the plot is structured into acts | N/A |

### Pass/Fail Criteria

A plot outline **passes** if:

- ✅ Has a clear setting
- ✅ Contains at least 2 branching paths
- ✅ Defines at least 2 endings
- ✅ Has clearly stated themes
- ✅ Word count ≥ 500

### Scoring

Score range: 0.0 to 10.0

- Each passed criterion contributes equally to the score
- Final score = (passed_checks / total_checks) × 10.0

### Usage Example

```python
from space_hulk_game.quality import PlotMetrics
import json

# From JSON file
with open('game-config/plot_outline.json', 'r') as f:
    content = json.load(f)

metrics = PlotMetrics.from_dict(content)

print(f"Score: {metrics.get_score()}/10.0")
print(f"Passes: {metrics.passes_threshold()}")
print(f"Failures: {metrics.get_failures()}")
```

---

## 2. NarrativeMetrics

### Purpose

Evaluate the quality and structural integrity of narrative maps generated by the NarrativeArchitectAgent.

### Location

`src/space_hulk_game/quality/narrative_metrics.py`

### Measured Criteria

| Metric | Type | Description | Threshold |
|--------|------|-------------|-----------|
| `total_scenes` | Count | Total number of scenes | ≥ 5 |
| `scenes_with_descriptions` | Count | Scenes that have descriptions | N/A |
| `completeness_percentage` | Float | % of scenes with descriptions | ≥ 90% |
| `all_connections_valid` | Boolean | All connections reference valid scenes | Required |
| `has_orphaned_scenes` | Boolean | Whether unreachable scenes exist | Must be False |
| `orphaned_scenes` | List | IDs of orphaned scenes | [] |
| `has_start_scene` | Boolean | Whether a start scene is defined | Required |

### Pass/Fail Criteria

A narrative map **passes** if:

- ✅ Has a defined start scene
- ✅ Contains at least 5 scenes
- ✅ At least 90% of scenes have descriptions
- ✅ All connections reference valid scenes
- ✅ No orphaned (unreachable) scenes

### Scoring

Score range: 0.0 to 10.0

- Start scene: 2 points
- Minimum scenes: 2 points
- Completeness: 3 points
- Valid connections: 2 points
- No orphans: 1 point

### Usage Example

```python
from space_hulk_game.quality import NarrativeMetrics

metrics = NarrativeMetrics.from_yaml_content(yaml_content)

if not metrics.passes_threshold():
    for failure in metrics.get_failures():
        print(f"❌ {failure}")

if metrics.has_orphaned_scenes:
    print(f"Orphaned scenes: {metrics.orphaned_scenes}")
```

---

## 3. PuzzleMetrics

### Purpose

Evaluate puzzles, artifacts, and game challenges designed by the PuzzleSmithAgent.

### Location

`src/space_hulk_game/quality/puzzle_metrics.py`

### Measured Criteria

| Metric | Type | Description | Threshold |
|--------|------|-------------|-----------|
| `total_puzzles` | Count | Total number of puzzles | ≥ 2 |
| `puzzles_with_solutions` | Count | Puzzles with clear solutions | ≥ 80% |
| `puzzles_with_narrative_ties` | Count | Puzzles tied to narrative | N/A |
| `puzzles_with_difficulty` | Count | Puzzles with difficulty ratings | N/A |
| `has_artifacts` | Boolean | Whether artifacts are defined | N/A |
| `has_monsters` | Boolean | Whether monsters/enemies are defined | N/A |
| `has_npcs` | Boolean | Whether NPCs are defined | N/A |

### Pass/Fail Criteria

Puzzle design **passes** if:

- ✅ Contains at least 2 puzzles
- ✅ At least 80% of puzzles have clear solutions

### Scoring

Score range: 0.0 to 10.0

- Minimum puzzles: 3 points
- Solutions: 3 points
- Narrative ties: 2 points
- Difficulty ratings: 1 point
- Artifacts/monsters/NPCs: 1 point bonus

### Usage Example

```python
from space_hulk_game.quality import PuzzleMetrics

metrics = PuzzleMetrics.from_yaml_content(yaml_content)

print(f"Total puzzles: {metrics.total_puzzles}")
print(f"With solutions: {metrics.puzzles_with_solutions}")
print(f"Score: {metrics.get_score()}/10.0")
```

---

## 4. SceneMetrics

### Purpose

Evaluate scene descriptions and text quality produced by the CreativeScribeAgent.

### Location

`src/space_hulk_game/quality/scene_metrics.py`

### Measured Criteria

| Metric | Type | Description | Threshold |
|--------|------|-------------|-----------|
| `total_scenes` | Count | Total number of scenes | ≥ 5 |
| `scenes_with_vivid_descriptions` | Count | Scenes with quality descriptions | ≥ 70% |
| `scenes_with_dialogue` | Count | Scenes containing dialogue | N/A |
| `average_description_length` | Float | Avg scene description (words) | ≥ 50 |
| `tone_consistency_score` | Float | Score for tone consistency (0-10) | N/A |
| `has_sensory_details` | Boolean | Scenes include sensory details | N/A |

### Pass/Fail Criteria

Scene texts **pass** if:

- ✅ Contains at least 5 scenes
- ✅ Average description length ≥ 50 words
- ✅ At least 70% of scenes have vivid descriptions

### Vivid Description Criteria

A description is considered "vivid" if it:

- Is at least 50 words long
- Contains descriptive adjectives (dark, ancient, twisted, etc.)
- Includes specific, concrete details

### Scoring

Score range: 0.0 to 10.0

- Minimum scenes: 2 points
- Description length: 2 points
- Vivid descriptions: 3 points
- Dialogue: 1 point
- Tone consistency: 1 point
- Sensory details: 1 point

### Usage Example

```python
from space_hulk_game.quality import SceneMetrics

metrics = SceneMetrics.from_yaml_content(yaml_content)

print(f"Average description: {metrics.average_description_length:.0f} words")
print(f"Vivid scenes: {metrics.scenes_with_vivid_descriptions}/{metrics.total_scenes}")
print(f"Tone consistency: {metrics.tone_consistency_score:.1f}/10.0")
```

---

## 5. MechanicsMetrics

### Purpose

Evaluate game mechanics and PRD documents created by the MechanicsGuruAgent.

### Location

`src/space_hulk_game/quality/mechanics_metrics.py`

### Measured Criteria

| Metric | Type | Description | Threshold |
|--------|------|-------------|-----------|
| `total_systems` | Count | Total game systems described | ≥ 3 |
| `systems_with_rules` | Count | Systems with clear rules | N/A |
| `completeness_percentage` | Float | % of expected systems present | ≥ 60% |
| `average_rule_clarity` | Float | Avg clarity score for rules (0-10) | ≥ 6.0 |
| `has_combat_system` | Boolean | Combat mechanics described | N/A |
| `has_movement_system` | Boolean | Movement mechanics described | N/A |
| `has_inventory_system` | Boolean | Inventory mechanics described | N/A |
| `has_progression_system` | Boolean | Character progression described | N/A |
| `has_balance_notes` | Boolean | Difficulty/balance discussed | N/A |

### Pass/Fail Criteria

Game mechanics **pass** if:

- ✅ Describes at least 3 game systems
- ✅ At least 60% of expected core systems are present
- ✅ Average rule clarity ≥ 6.0/10.0

### Rule Clarity Scoring

Rules are scored based on:

- Presence of structured rules/descriptions
- Inclusion of examples
- Word count (length/detail)
- Use of clear, prescriptive language

### Scoring

Score range: 0.0 to 10.0

- Total systems: 2 points
- Completeness: 3 points
- Rule clarity: 3 points
- Core systems: 1.5 points
- Balance notes: 0.5 points

### Usage Example

```python
from space_hulk_game.quality import MechanicsMetrics

metrics = MechanicsMetrics.from_yaml_content(yaml_content)

print(f"Systems defined: {metrics.total_systems}")
print(f"Completeness: {metrics.completeness_percentage:.0f}%")
print(f"Rule clarity: {metrics.average_rule_clarity:.1f}/10.0")
print(f"Combat system: {'✓' if metrics.has_combat_system else '✗'}")
```

---

## Integration Usage

### Evaluating All Output Types

```python
from space_hulk_game.quality import (
    PlotMetrics, NarrativeMetrics, PuzzleMetrics,
    SceneMetrics, MechanicsMetrics
)
import yaml

# Load all generated files
files = {
    'plot': 'game-config/plot_outline.json',
    'narrative': 'game-config/narrative_map.json',
    'puzzles': 'game-config/puzzle_design.json',
    'scenes': 'game-config/scene_texts.json',
    'mechanics': 'game-config/prd_document.json',
}

metrics_classes = {
    'plot': PlotMetrics,
    'narrative': NarrativeMetrics,
    'puzzles': PuzzleMetrics,
    'scenes': SceneMetrics,
    'mechanics': MechanicsMetrics,
}

# Evaluate each output
results = {}
for key, filepath in files.items():
    with open(filepath, 'r') as f:
        content = f.read()

    metrics_class = metrics_classes[key]
    metrics = metrics_class.from_yaml_content(content)

    results[key] = {
        'score': metrics.get_score(),
        'passes': metrics.passes_threshold(),
        'failures': metrics.get_failures(),
        'details': metrics.to_dict(),
    }

# Print summary
for key, result in results.items():
    print(f"\n{key.upper()}")
    print(f"Score: {result['score']:.1f}/10.0")
    print(f"Status: {'✅ PASS' if result['passes'] else '❌ FAIL'}")

    if result['failures']:
        print("Issues:")
        for failure in result['failures']:
            print(f"  - {failure}")
```

### Generating Feedback for Retry

```python
def generate_feedback(metrics, output_type):
    """Generate actionable feedback for improving content."""
    feedback = []

    if not metrics.passes_threshold():
        feedback.append(
            f"The {output_type} does not meet quality thresholds. "
            f"Current score: {metrics.get_score():.1f}/10.0"
        )

        for failure in metrics.get_failures():
            feedback.append(f"- {failure}")

        # Add specific improvement suggestions
        if output_type == 'plot':
            if metrics.branching_paths_count < 2:
                feedback.append(
                    "Add more decision points where players can choose "
                    "different narrative paths (A/B choices)."
                )
            if metrics.word_count < 500:
                feedback.append(
                    "Expand the plot description with more detail about "
                    "the setting, characters, and story progression."
                )

    return "\n".join(feedback)
```

---

## Best Practices

### 1. Use Metrics Early

Run metrics validation immediately after content generation to catch issues early.

### 2. Set Appropriate Thresholds

Adjust thresholds based on your project needs:

```python
# Custom thresholds
metrics = PlotMetrics.from_yaml_content(content)
metrics.min_word_count = 1000  # Require longer plots
metrics.min_endings = 3  # Require more endings
```

### 3. Track Metrics Over Time

Log metrics for each generation to track quality trends:

```python
import json
from datetime import datetime

metrics_log = {
    'timestamp': datetime.now().isoformat(),
    'plot_score': plot_metrics.get_score(),
    'narrative_score': narrative_metrics.get_score(),
    'overall': (plot_score + narrative_score) / 2,
}

with open('metrics_history.jsonl', 'a') as f:
    f.write(json.dumps(metrics_log) + '\n')
```

### 4. Combine with Manual Review

Metrics are objective but not perfect. Use them to:

- Identify obvious issues automatically
- Flag content for manual review
- Track improvement trends
- Set quality baselines

Don't rely solely on metrics for final quality assessment.

---

## Extending the Metrics

### Adding New Metrics

To add a new metric to an existing module:

```python
# In plot_metrics.py

@dataclass
class PlotMetrics:
    # ... existing fields ...

    # Add new metric
    has_character_arcs: bool = False

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'PlotMetrics':
        metrics = cls()
        # ... existing logic ...

        # Add new check
        metrics.has_character_arcs = cls._check_character_arcs(data)

        return metrics

    @staticmethod
    def _check_character_arcs(data: Dict[str, Any]) -> bool:
        """Check for character development."""
        # Implementation here
        pass
```

### Creating New Metrics Modules

For new content types, follow the established pattern:

1. Create new file: `src/space_hulk_game/quality/your_metrics.py`
2. Define dataclass with measured values and thresholds
3. Implement `from_yaml_content()` and `from_dict()` class methods
4. Implement `passes_threshold()`, `get_failures()`, `get_score()`, and `to_dict()` methods
5. Add to `__init__.py` exports
6. Document in this file

---

## Troubleshooting

### Common Issues

**Issue:** "Failed to parse YAML content"

- **Cause:** Output contains markdown code fences (```)
- **Solution:** The metrics automatically handle this, but ensure YAML is valid

**Issue:** Metrics count is unexpectedly zero

- **Cause:** YAML structure doesn't match expected format
- **Solution:** Check that your YAML uses expected keys (e.g., 'scenes', 'puzzles')

**Issue:** Score is very low despite good content

- **Cause:** Thresholds may be too strict or content structure is non-standard
- **Solution:** Adjust thresholds or modify metric logic for your use case

### Debug Mode

Enable detailed output:

```python
metrics = PlotMetrics.from_yaml_content(content)

# Print all measured values
print(metrics.to_dict())

# Check specific values
print(f"Word count: {metrics.word_count}")
print(f"Branching paths: {metrics.branching_paths_count}")
print(f"Endings: {metrics.endings_count}")
```

---

## Future Enhancements

Planned improvements for the quality metrics system:

1. **Machine Learning Integration**
   - Train models to detect quality patterns
   - Learn from human feedback on generated content

2. **Comparative Analysis**
   - Compare current output to historical best examples
   - Identify regression in quality

3. **Real-time Feedback**
   - Provide metrics feedback during generation
   - Allow agents to self-correct before completion

4. **Cross-Content Validation**
   - Ensure narrative map matches plot outline
   - Verify puzzle integration with scenes

5. **Performance Metrics**
   - Track generation time vs. quality
   - Optimize for speed/quality tradeoff

---

## Related Documentation

- [Master Implementation Plan](../project-plans/restart-project/master_implementation_plan.md)
- [Phase 3: Quality & Iteration System](../project-plans/restart-project/master_implementation_plan.md#phase-3-quality--iteration-system)
- [CLAUDE.md](../CLAUDE.md) - Project overview and standards

---

## Changelog

| Date | Version | Changes |
|------|---------|---------|
| 2025-11-09 | 1.0 | Initial quality metrics system created (Chunk 3.1) |

---

**End of Quality Metrics Documentation**
