<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>space_hulk_game.quality API documentation</title>
<meta name="description" content="Quality metrics and evaluation module for Space Hulk Game …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>space_hulk_game.quality</code></h1>
</header>
<section id="section-intro">
<p>Quality metrics and evaluation module for Space Hulk Game.</p>
<p>This module provides quality evaluation metrics and evaluators for generated
game content, including plot outlines, narrative maps, puzzles, scenes, and mechanics.</p>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="space_hulk_game.quality.evaluator" href="evaluator.html">space_hulk_game.quality.evaluator</a></code></dt>
<dd>
<div class="desc"><p>Base quality evaluator class for content evaluation …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.integration" href="integration.html">space_hulk_game.quality.integration</a></code></dt>
<dd>
<div class="desc"><p>Integration helpers for quality checking in CrewAI tasks …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.mechanics_evaluator" href="mechanics_evaluator.html">space_hulk_game.quality.mechanics_evaluator</a></code></dt>
<dd>
<div class="desc"><p>Game mechanics quality evaluator …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.mechanics_metrics" href="mechanics_metrics.html">space_hulk_game.quality.mechanics_metrics</a></code></dt>
<dd>
<div class="desc"><p>Quality metrics for GameMechanics evaluation …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.narrative_evaluator" href="narrative_evaluator.html">space_hulk_game.quality.narrative_evaluator</a></code></dt>
<dd>
<div class="desc"><p>Narrative map quality evaluator …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.narrative_metrics" href="narrative_metrics.html">space_hulk_game.quality.narrative_metrics</a></code></dt>
<dd>
<div class="desc"><p>Quality metrics for NarrativeMap evaluation …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.plot_evaluator" href="plot_evaluator.html">space_hulk_game.quality.plot_evaluator</a></code></dt>
<dd>
<div class="desc"><p>Plot outline quality evaluator …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.plot_metrics" href="plot_metrics.html">space_hulk_game.quality.plot_metrics</a></code></dt>
<dd>
<div class="desc"><p>Quality metrics for PlotOutline evaluation …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.puzzle_evaluator" href="puzzle_evaluator.html">space_hulk_game.quality.puzzle_evaluator</a></code></dt>
<dd>
<div class="desc"><p>Puzzle design quality evaluator …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.puzzle_metrics" href="puzzle_metrics.html">space_hulk_game.quality.puzzle_metrics</a></code></dt>
<dd>
<div class="desc"><p>Quality metrics for Puzzle evaluation …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.retry" href="retry.html">space_hulk_game.quality.retry</a></code></dt>
<dd>
<div class="desc"><p>Retry logic with quality feedback for task execution …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.scene_evaluator" href="scene_evaluator.html">space_hulk_game.quality.scene_evaluator</a></code></dt>
<dd>
<div class="desc"><p>Scene text quality evaluator …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.scene_metrics" href="scene_metrics.html">space_hulk_game.quality.scene_metrics</a></code></dt>
<dd>
<div class="desc"><p>Quality metrics for Scene evaluation …</p></div>
</dd>
<dt><code class="name"><a title="space_hulk_game.quality.score" href="score.html">space_hulk_game.quality.score</a></code></dt>
<dd>
<div class="desc"><p>Quality score data class for evaluation results …</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="space_hulk_game.quality.create_quality_config"><code class="name flex">
<span>def <span class="ident">create_quality_config</span></span>(<span>plot_threshold: float = 6.0,<br>narrative_threshold: float = 6.0,<br>puzzle_threshold: float = 6.0,<br>scene_threshold: float = 6.0,<br>mechanics_threshold: float = 6.0,<br>max_retries: int = 3) ‑> dict[<a title="space_hulk_game.quality.retry.TaskType" href="retry.html#space_hulk_game.quality.retry.TaskType">TaskType</a>, dict[str, typing.Any]]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_quality_config(
    plot_threshold: float = 6.0,
    narrative_threshold: float = 6.0,
    puzzle_threshold: float = 6.0,
    scene_threshold: float = 6.0,
    mechanics_threshold: float = 6.0,
    max_retries: int = 3,
) -&gt; dict[TaskType, dict[str, Any]]:
    &#34;&#34;&#34;
    Create a quality configuration dictionary for all task types.

    Args:
        plot_threshold: Pass threshold for plot tasks (0.0-10.0)
        narrative_threshold: Pass threshold for narrative tasks (0.0-10.0)
        puzzle_threshold: Pass threshold for puzzle tasks (0.0-10.0)
        scene_threshold: Pass threshold for scene tasks (0.0-10.0)
        mechanics_threshold: Pass threshold for mechanics tasks (0.0-10.0)
        max_retries: Maximum retry attempts for all tasks

    Returns:
        Dictionary mapping task types to their quality configurations

    Example:
        &gt;&gt;&gt; config = create_quality_config(
        ...     plot_threshold=7.0,
        ...     narrative_threshold=8.0,
        ...     max_retries=3
        ... )
        &gt;&gt;&gt; plot_config = config[TaskType.PLOT]
        &gt;&gt;&gt; print(plot_config[&#39;pass_threshold&#39;])  # 7.0
    &#34;&#34;&#34;
    return {
        TaskType.PLOT: {&#34;pass_threshold&#34;: plot_threshold, &#34;max_retries&#34;: max_retries},
        TaskType.NARRATIVE: {&#34;pass_threshold&#34;: narrative_threshold, &#34;max_retries&#34;: max_retries},
        TaskType.PUZZLE: {&#34;pass_threshold&#34;: puzzle_threshold, &#34;max_retries&#34;: max_retries},
        TaskType.SCENE: {&#34;pass_threshold&#34;: scene_threshold, &#34;max_retries&#34;: max_retries},
        TaskType.MECHANICS: {&#34;pass_threshold&#34;: mechanics_threshold, &#34;max_retries&#34;: max_retries},
    }</code></pre>
</details>
<div class="desc"><p>Create a quality configuration dictionary for all task types.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>plot_threshold</code></strong></dt>
<dd>Pass threshold for plot tasks (0.0-10.0)</dd>
<dt><strong><code>narrative_threshold</code></strong></dt>
<dd>Pass threshold for narrative tasks (0.0-10.0)</dd>
<dt><strong><code>puzzle_threshold</code></strong></dt>
<dd>Pass threshold for puzzle tasks (0.0-10.0)</dd>
<dt><strong><code>scene_threshold</code></strong></dt>
<dd>Pass threshold for scene tasks (0.0-10.0)</dd>
<dt><strong><code>mechanics_threshold</code></strong></dt>
<dd>Pass threshold for mechanics tasks (0.0-10.0)</dd>
<dt><strong><code>max_retries</code></strong></dt>
<dd>Maximum retry attempts for all tasks</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dictionary mapping task types to their quality configurations</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; config = create_quality_config(
...     plot_threshold=7.0,
...     narrative_threshold=8.0,
...     max_retries=3
... )
&gt;&gt;&gt; plot_config = config[TaskType.PLOT]
&gt;&gt;&gt; print(plot_config['pass_threshold'])  # 7.0
</code></pre></div>
</dd>
<dt id="space_hulk_game.quality.execute_with_optional_quality_check"><code class="name flex">
<span>def <span class="ident">execute_with_optional_quality_check</span></span>(<span>task_function: Callable[..., str],<br>task_type: <a title="space_hulk_game.quality.retry.TaskType" href="retry.html#space_hulk_game.quality.retry.TaskType">TaskType</a>,<br>task_name: str = 'Task',<br>**kwargs) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute_with_optional_quality_check(
    task_function: Callable[..., str], task_type: TaskType, task_name: str = &#34;Task&#34;, **kwargs
) -&gt; str:
    &#34;&#34;&#34;
    Convenience function to execute task with optional quality checking.

    Uses the default executor singleton.

    Args:
        task_function: Function to execute
        task_type: Type of task
        task_name: Human-readable task name
        **kwargs: Arguments to pass to task function

    Returns:
        Task output string
    &#34;&#34;&#34;
    executor = get_default_executor()
    return executor.execute_task(task_function, task_type, task_name, **kwargs)</code></pre>
</details>
<div class="desc"><p>Convenience function to execute task with optional quality checking.</p>
<p>Uses the default executor singleton.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task_function</code></strong></dt>
<dd>Function to execute</dd>
<dt><strong><code>task_type</code></strong></dt>
<dd>Type of task</dd>
<dt><strong><code>task_name</code></strong></dt>
<dd>Human-readable task name</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Arguments to pass to task function</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Task output string</p></div>
</dd>
<dt id="space_hulk_game.quality.execute_with_quality_check"><code class="name flex">
<span>def <span class="ident">execute_with_quality_check</span></span>(<span>task_function: Callable[..., str],<br>task_type: <a title="space_hulk_game.quality.retry.TaskType" href="retry.html#space_hulk_game.quality.retry.TaskType">TaskType</a>,<br>task_name: str = 'Task',<br>pass_threshold: float = 6.0,<br>max_retries: int = 3,<br>**kwargs) ‑> tuple[str, <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a>, int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute_with_quality_check(
    task_function: Callable[..., str],
    task_type: TaskType,
    task_name: str = &#34;Task&#34;,
    pass_threshold: float = 6.0,
    max_retries: int = 3,
    **kwargs,
) -&gt; tuple[str, QualityScore, int]:
    &#34;&#34;&#34;
    Execute a task with quality checking and retry logic (functional interface).

    This is a convenience function that wraps TaskWithQualityCheck for simpler usage.

    Args:
        task_function: Function to execute (should return output string)
        task_type: Type of task (determines which evaluator to use)
        task_name: Human-readable name for logging
        pass_threshold: Minimum quality score required to pass (0.0-10.0)
        max_retries: Maximum number of retry attempts (default: 3)
        **kwargs: Additional arguments to pass to task_function

    Returns:
        Tuple of (output, final_quality_score, attempts_made)

    Example:
        &gt;&gt;&gt; def generate_plot(**kwargs):
        ...     # Your plot generation logic
        ...     return &#34;title: My Plot\\nsetting: Space station&#34;
        &gt;&gt;&gt;
        &gt;&gt;&gt; output, quality, attempts = execute_with_quality_check(
        ...     task_function=generate_plot,
        ...     task_type=TaskType.PLOT,
        ...     task_name=&#34;Generate Plot&#34;,
        ...     pass_threshold=7.0,
        ...     max_retries=3
        ... )
        &gt;&gt;&gt; print(f&#34;Completed in {attempts} attempts with score {quality.score}&#34;)
    &#34;&#34;&#34;
    wrapper = TaskWithQualityCheck(
        task_type=task_type, pass_threshold=pass_threshold, max_retries=max_retries
    )

    return wrapper.execute(task_function, task_name=task_name, **kwargs)</code></pre>
</details>
<div class="desc"><p>Execute a task with quality checking and retry logic (functional interface).</p>
<p>This is a convenience function that wraps TaskWithQualityCheck for simpler usage.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task_function</code></strong></dt>
<dd>Function to execute (should return output string)</dd>
<dt><strong><code>task_type</code></strong></dt>
<dd>Type of task (determines which evaluator to use)</dd>
<dt><strong><code>task_name</code></strong></dt>
<dd>Human-readable name for logging</dd>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum quality score required to pass (0.0-10.0)</dd>
<dt><strong><code>max_retries</code></strong></dt>
<dd>Maximum number of retry attempts (default: 3)</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments to pass to task_function</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tuple of (output, final_quality_score, attempts_made)</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; def generate_plot(**kwargs):
...     # Your plot generation logic
...     return &quot;title: My Plot\nsetting: Space station&quot;
&gt;&gt;&gt;
&gt;&gt;&gt; output, quality, attempts = execute_with_quality_check(
...     task_function=generate_plot,
...     task_type=TaskType.PLOT,
...     task_name=&quot;Generate Plot&quot;,
...     pass_threshold=7.0,
...     max_retries=3
... )
&gt;&gt;&gt; print(f&quot;Completed in {attempts} attempts with score {quality.score}&quot;)
</code></pre></div>
</dd>
<dt id="space_hulk_game.quality.get_default_executor"><code class="name flex">
<span>def <span class="ident">get_default_executor</span></span>(<span>) ‑> <a title="space_hulk_game.quality.integration.TaskExecutor" href="integration.html#space_hulk_game.quality.integration.TaskExecutor">TaskExecutor</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_default_executor() -&gt; TaskExecutor:
    &#34;&#34;&#34;
    Get default task executor singleton.

    Returns:
        Default TaskExecutor instance
    &#34;&#34;&#34;
    global _default_executor
    if _default_executor is None:
        _default_executor = TaskExecutor()
    return _default_executor</code></pre>
</details>
<div class="desc"><p>Get default task executor singleton.</p>
<h2 id="returns">Returns</h2>
<p>Default TaskExecutor instance</p></div>
</dd>
<dt id="space_hulk_game.quality.get_task_type_for_crew_task"><code class="name flex">
<span>def <span class="ident">get_task_type_for_crew_task</span></span>(<span>task_name: str) ‑> <a title="space_hulk_game.quality.retry.TaskType" href="retry.html#space_hulk_game.quality.retry.TaskType">TaskType</a> | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_task_type_for_crew_task(task_name: str) -&gt; TaskType | None:
    &#34;&#34;&#34;
    Get TaskType for a CrewAI task name.

    Args:
        task_name: Name of CrewAI task

    Returns:
        TaskType if mapping exists, None otherwise
    &#34;&#34;&#34;
    return CREW_TASK_MAPPING.get(task_name)</code></pre>
</details>
<div class="desc"><p>Get TaskType for a CrewAI task name.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task_name</code></strong></dt>
<dd>Name of CrewAI task</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>TaskType if mapping exists, None otherwise</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="space_hulk_game.quality.MechanicsEvaluator"><code class="flex name class">
<span>class <span class="ident">MechanicsEvaluator</span></span>
<span>(</span><span>pass_threshold: float = 6.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MechanicsEvaluator(QualityEvaluator):
    &#34;&#34;&#34;
    Evaluator for game mechanics content.

    Uses MechanicsMetrics to evaluate game mechanics against quality criteria
    including systems completeness, rules clarity, and balance.

    Example:
        &gt;&gt;&gt; evaluator = MechanicsEvaluator()
        &gt;&gt;&gt; score = evaluator.evaluate(mechanics_yaml_content)
        &gt;&gt;&gt; print(f&#34;Score: {score.score}/10&#34;)
        &gt;&gt;&gt; print(score.feedback)
    &#34;&#34;&#34;

    def __init__(self, pass_threshold: float = 6.0):
        &#34;&#34;&#34;
        Initialize the mechanics evaluator.

        Args:
            pass_threshold: Minimum score required to pass (default: 6.0)
        &#34;&#34;&#34;
        super().__init__(pass_threshold)
        logger.info(f&#34;MechanicsEvaluator initialized with threshold {pass_threshold}&#34;)

    def evaluate(self, content: str) -&gt; QualityScore:
        &#34;&#34;&#34;
        Evaluate game mechanics content.

        Args:
            content: YAML string containing game mechanics

        Returns:
            QualityScore with evaluation results

        Raises:
            ValueError: If content cannot be parsed
        &#34;&#34;&#34;
        try:
            # Parse YAML content using metrics class
            metrics = MechanicsMetrics.from_yaml_content(content)

            # Get score and check if passes threshold
            score = metrics.get_score()
            passed = score &gt;= self.pass_threshold

            # Get failures for detailed feedback
            failures = metrics.get_failures()

            # Build feedback message
            feedback = self._build_feedback(score, failures)

            # Build details dictionary
            details = {
                &#34;total_systems&#34;: metrics.total_systems,
                &#34;systems_with_rules&#34;: metrics.systems_with_rules,
                &#34;has_combat_system&#34;: metrics.has_combat_system,
                &#34;has_movement_system&#34;: metrics.has_movement_system,
                &#34;has_inventory_system&#34;: metrics.has_inventory_system,
                &#34;has_progression_system&#34;: metrics.has_progression_system,
                &#34;completeness_percentage&#34;: metrics.completeness_percentage,
                &#34;average_rule_clarity&#34;: metrics.average_rule_clarity,
                &#34;has_balance_notes&#34;: metrics.has_balance_notes,
                &#34;total_word_count&#34;: 0,  # Can be computed if needed
                &#34;systems_without_descriptions&#34;: [],
                &#34;unclear_systems&#34;: [],
                &#34;failures&#34;: failures,
                &#34;threshold&#34;: self.pass_threshold,
            }

            logger.info(
                f&#34;Mechanics evaluation complete: score={score:.1f}, passed={passed}, &#34;
                f&#34;systems={metrics.total_systems}, clarity={metrics.average_rule_clarity:.1f}&#34;
            )

            return self._create_score(score, passed, feedback, details)

        except ValueError as e:
            logger.error(f&#34;Failed to evaluate mechanics content: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Failed to parse mechanics content: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )
        except Exception as e:
            logger.exception(f&#34;Unexpected error evaluating mechanics: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )

    def generate_detailed_feedback(self, content: str) -&gt; str:
        &#34;&#34;&#34;
        Generate detailed, actionable feedback for improvement.

        Args:
            content: YAML string containing game mechanics

        Returns:
            Multi-line feedback with specific suggestions
        &#34;&#34;&#34;
        result = self.evaluate(content)

        lines = [
            f&#34;Game Mechanics Quality Score: {result.score:.1f}/10.0&#34;,
            f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
            &#34;&#34;,
        ]

        if result.passed:
            lines.append(&#34;The game mechanics meet all quality requirements!&#34;)
        else:
            lines.append(&#34;The game mechanics need improvement:&#34;)

        lines.append(&#34;&#34;)

        # Add specific findings
        details = result.details
        failures = details.get(&#34;failures&#34;, [])

        if failures:
            lines.append(&#34;Issues to address:&#34;)
            for failure in failures:
                lines.append(f&#34;  • {failure}&#34;)
            lines.append(&#34;&#34;)

        # Add content statistics
        lines.append(f&#34;System count: {details.get(&#39;total_systems&#39;, 0)} (min: 3)&#34;)
        lines.append(f&#34;Systems with rules: {details.get(&#39;systems_with_rules&#39;, 0)}&#34;)
        lines.append(f&#34;Completeness: {details.get(&#39;completeness_percentage&#39;, 0):.1f}%&#34;)
        lines.append(f&#34;Average clarity: {details.get(&#39;average_rule_clarity&#39;, 0):.1f}/10&#34;)

        # Add positive feedback
        lines.append(&#34;&#34;)
        if details.get(&#34;has_combat_system&#34;):
            lines.append(&#34;✓ Combat system described&#34;)
        if details.get(&#34;has_movement_system&#34;):
            lines.append(&#34;✓ Movement system described&#34;)
        if details.get(&#34;has_inventory_system&#34;):
            lines.append(&#34;✓ Inventory system described&#34;)
        if details.get(&#34;has_progression_system&#34;):
            lines.append(&#34;✓ Progression system described&#34;)
        if details.get(&#34;average_rule_clarity&#34;, 0) &gt;= 7.0:
            lines.append(&#34;✓ Rules are clear and well-explained&#34;)
        if details.get(&#34;has_balance_notes&#34;):
            lines.append(&#34;✓ Difficulty/balance discussed&#34;)

        return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Evaluator for game mechanics content.</p>
<p>Uses MechanicsMetrics to evaluate game mechanics against quality criteria
including systems completeness, rules clarity, and balance.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; evaluator = MechanicsEvaluator()
&gt;&gt;&gt; score = evaluator.evaluate(mechanics_yaml_content)
&gt;&gt;&gt; print(f&quot;Score: {score.score}/10&quot;)
&gt;&gt;&gt; print(score.feedback)
</code></pre>
<p>Initialize the mechanics evaluator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum score required to pass (default: 6.0)</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.MechanicsEvaluator.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, content: str) ‑> <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, content: str) -&gt; QualityScore:
    &#34;&#34;&#34;
    Evaluate game mechanics content.

    Args:
        content: YAML string containing game mechanics

    Returns:
        QualityScore with evaluation results

    Raises:
        ValueError: If content cannot be parsed
    &#34;&#34;&#34;
    try:
        # Parse YAML content using metrics class
        metrics = MechanicsMetrics.from_yaml_content(content)

        # Get score and check if passes threshold
        score = metrics.get_score()
        passed = score &gt;= self.pass_threshold

        # Get failures for detailed feedback
        failures = metrics.get_failures()

        # Build feedback message
        feedback = self._build_feedback(score, failures)

        # Build details dictionary
        details = {
            &#34;total_systems&#34;: metrics.total_systems,
            &#34;systems_with_rules&#34;: metrics.systems_with_rules,
            &#34;has_combat_system&#34;: metrics.has_combat_system,
            &#34;has_movement_system&#34;: metrics.has_movement_system,
            &#34;has_inventory_system&#34;: metrics.has_inventory_system,
            &#34;has_progression_system&#34;: metrics.has_progression_system,
            &#34;completeness_percentage&#34;: metrics.completeness_percentage,
            &#34;average_rule_clarity&#34;: metrics.average_rule_clarity,
            &#34;has_balance_notes&#34;: metrics.has_balance_notes,
            &#34;total_word_count&#34;: 0,  # Can be computed if needed
            &#34;systems_without_descriptions&#34;: [],
            &#34;unclear_systems&#34;: [],
            &#34;failures&#34;: failures,
            &#34;threshold&#34;: self.pass_threshold,
        }

        logger.info(
            f&#34;Mechanics evaluation complete: score={score:.1f}, passed={passed}, &#34;
            f&#34;systems={metrics.total_systems}, clarity={metrics.average_rule_clarity:.1f}&#34;
        )

        return self._create_score(score, passed, feedback, details)

    except ValueError as e:
        logger.error(f&#34;Failed to evaluate mechanics content: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Failed to parse mechanics content: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )
    except Exception as e:
        logger.exception(f&#34;Unexpected error evaluating mechanics: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )</code></pre>
</details>
<div class="desc"><p>Evaluate game mechanics content.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing game mechanics</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>QualityScore with evaluation results</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If content cannot be parsed</dd>
</dl></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsEvaluator.generate_detailed_feedback"><code class="name flex">
<span>def <span class="ident">generate_detailed_feedback</span></span>(<span>self, content: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_detailed_feedback(self, content: str) -&gt; str:
    &#34;&#34;&#34;
    Generate detailed, actionable feedback for improvement.

    Args:
        content: YAML string containing game mechanics

    Returns:
        Multi-line feedback with specific suggestions
    &#34;&#34;&#34;
    result = self.evaluate(content)

    lines = [
        f&#34;Game Mechanics Quality Score: {result.score:.1f}/10.0&#34;,
        f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
        &#34;&#34;,
    ]

    if result.passed:
        lines.append(&#34;The game mechanics meet all quality requirements!&#34;)
    else:
        lines.append(&#34;The game mechanics need improvement:&#34;)

    lines.append(&#34;&#34;)

    # Add specific findings
    details = result.details
    failures = details.get(&#34;failures&#34;, [])

    if failures:
        lines.append(&#34;Issues to address:&#34;)
        for failure in failures:
            lines.append(f&#34;  • {failure}&#34;)
        lines.append(&#34;&#34;)

    # Add content statistics
    lines.append(f&#34;System count: {details.get(&#39;total_systems&#39;, 0)} (min: 3)&#34;)
    lines.append(f&#34;Systems with rules: {details.get(&#39;systems_with_rules&#39;, 0)}&#34;)
    lines.append(f&#34;Completeness: {details.get(&#39;completeness_percentage&#39;, 0):.1f}%&#34;)
    lines.append(f&#34;Average clarity: {details.get(&#39;average_rule_clarity&#39;, 0):.1f}/10&#34;)

    # Add positive feedback
    lines.append(&#34;&#34;)
    if details.get(&#34;has_combat_system&#34;):
        lines.append(&#34;✓ Combat system described&#34;)
    if details.get(&#34;has_movement_system&#34;):
        lines.append(&#34;✓ Movement system described&#34;)
    if details.get(&#34;has_inventory_system&#34;):
        lines.append(&#34;✓ Inventory system described&#34;)
    if details.get(&#34;has_progression_system&#34;):
        lines.append(&#34;✓ Progression system described&#34;)
    if details.get(&#34;average_rule_clarity&#34;, 0) &gt;= 7.0:
        lines.append(&#34;✓ Rules are clear and well-explained&#34;)
    if details.get(&#34;has_balance_notes&#34;):
        lines.append(&#34;✓ Difficulty/balance discussed&#34;)

    return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Generate detailed, actionable feedback for improvement.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing game mechanics</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Multi-line feedback with specific suggestions</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback">generate_feedback</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml">parse_yaml</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.score" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics"><code class="flex name class">
<span>class <span class="ident">MechanicsMetrics</span></span>
<span>(</span><span>has_combat_system: bool = False,<br>has_movement_system: bool = False,<br>has_inventory_system: bool = False,<br>has_progression_system: bool = False,<br>total_systems: int = 0,<br>systems_with_rules: int = 0,<br>completeness_percentage: float = 0.0,<br>average_rule_clarity: float = 0.0,<br>has_balance_notes: bool = False,<br>min_systems: int = 3,<br>min_completeness: float = 60.0,<br>min_clarity: float = 6.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class MechanicsMetrics:
    &#34;&#34;&#34;
    Quality metrics for evaluating game mechanics/PRD content.

    Attributes:
        has_combat_system: Whether combat mechanics are described
        has_movement_system: Whether movement mechanics are described
        has_inventory_system: Whether inventory mechanics are described
        has_progression_system: Whether character progression is described
        total_systems: Total number of game systems described
        systems_with_rules: Number of systems with clear rules
        completeness_percentage: Percentage of expected systems present
        average_rule_clarity: Average clarity score for rules
        has_balance_notes: Whether difficulty/balance is discussed
        min_systems: Minimum required systems (default: 3)
    &#34;&#34;&#34;

    # Measured values
    has_combat_system: bool = False
    has_movement_system: bool = False
    has_inventory_system: bool = False
    has_progression_system: bool = False
    total_systems: int = 0
    systems_with_rules: int = 0
    completeness_percentage: float = 0.0
    average_rule_clarity: float = 0.0
    has_balance_notes: bool = False

    # Thresholds
    min_systems: int = 3
    min_completeness: float = 60.0  # 60% of expected systems
    min_clarity: float = 6.0  # Out of 10.0

    @classmethod
    def from_yaml_content(cls, yaml_content: str) -&gt; &#34;MechanicsMetrics&#34;:
        &#34;&#34;&#34;
        Create MechanicsMetrics from YAML content string.

        Args:
            yaml_content: YAML string containing PRD/mechanics data

        Returns:
            MechanicsMetrics instance with measured values
        &#34;&#34;&#34;
        try:
            # Handle markdown-wrapped YAML
            content = yaml_content.strip()
            if content.startswith(&#34;```&#34;):
                lines = content.split(&#34;\n&#34;)
                content = &#34;\n&#34;.join(lines[1:-1])

            data = yaml.safe_load(content)
            return cls.from_dict(data)
        except Exception as e:
            raise ValueError(f&#34;Failed to parse YAML content: {e}&#34;) from e

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -&gt; &#34;MechanicsMetrics&#34;:
        &#34;&#34;&#34;
        Create MechanicsMetrics from a dictionary (parsed YAML).

        Args:
            data: Dictionary containing PRD/mechanics data

        Returns:
            MechanicsMetrics instance with measured values
        &#34;&#34;&#34;
        metrics = cls()

        # Get mechanics/systems section
        mechanics = data.get(&#34;mechanics&#34;, data)
        systems = mechanics.get(&#34;systems&#34;, {})

        # If no explicit systems section, analyze the entire document
        if not systems:
            systems = mechanics

        # Check for specific core systems
        metrics.has_combat_system = cls._has_system(systems, &#34;combat&#34;)
        metrics.has_movement_system = cls._has_system(systems, &#34;movement&#34;)
        metrics.has_inventory_system = cls._has_system(systems, &#34;inventory&#34;)
        metrics.has_progression_system = cls._has_system(systems, &#34;progression&#34;)

        # Count total systems
        metrics.total_systems = cls._count_systems(systems)

        # Count systems with clear rules
        metrics.systems_with_rules, clarity_scores = cls._analyze_rule_clarity(systems)

        # Calculate completeness (out of 5 expected core systems)
        expected_systems = 5
        present_systems = sum(
            [
                metrics.has_combat_system,
                metrics.has_movement_system,
                metrics.has_inventory_system,
                metrics.has_progression_system,
                metrics.total_systems &gt; 0,  # At least one other system
            ]
        )
        metrics.completeness_percentage = (present_systems / expected_systems) * 100.0

        # Calculate average clarity
        if clarity_scores:
            metrics.average_rule_clarity = sum(clarity_scores) / len(clarity_scores)

        # Check for balance/difficulty notes
        metrics.has_balance_notes = cls._has_balance_notes(mechanics)

        return metrics

    @staticmethod
    def _has_system(systems: dict[str, Any], system_name: str) -&gt; bool:
        &#34;&#34;&#34;
        Check if a specific system is described.

        Args:
            systems: Dictionary of game systems
            system_name: Name of system to check for

        Returns:
            True if system is present and described
        &#34;&#34;&#34;
        # Check for direct key
        if system_name in systems:
            return True

        # Check for variations
        variations = [
            system_name,
            f&#34;{system_name}_system&#34;,
            f&#34;{system_name}_mechanics&#34;,
            system_name.title(),
        ]

        for var in variations:
            if var in systems:
                return True

        # Check in nested structures
        for key, value in systems.items():
            if system_name.lower() in key.lower():
                return True
            if isinstance(value, str) and system_name.lower() in value.lower():
                # System mentioned in description
                if len(value) &gt;= 50:  # Substantial mention
                    return True

        return False

    @staticmethod
    def _count_systems(systems: dict[str, Any]) -&gt; int:
        &#34;&#34;&#34;
        Count total number of game systems described.

        Args:
            systems: Dictionary of game systems

        Returns:
            Number of systems
        &#34;&#34;&#34;
        count = 0

        # Count top-level system keys
        system_keywords = [
            &#34;combat&#34;,
            &#34;movement&#34;,
            &#34;inventory&#34;,
            &#34;progression&#34;,
            &#34;health&#34;,
            &#34;stamina&#34;,
            &#34;skill&#34;,
            &#34;puzzle&#34;,
            &#34;dialogue&#34;,
            &#34;stealth&#34;,
            &#34;resource&#34;,
            &#34;crafting&#34;,
            &#34;trading&#34;,
            &#34;reputation&#34;,
            &#34;time&#34;,
            &#34;environment&#34;,
        ]

        for key in systems.keys():
            # Check if key contains a system keyword
            key_lower = key.lower()
            if any(keyword in key_lower for keyword in system_keywords):
                count += 1
            # Or if it&#39;s a substantial section
            elif isinstance(systems[key], (dict, list)):
                count += 1

        return count

    @staticmethod
    def _analyze_rule_clarity(systems: dict[str, Any]) -&gt; tuple:
        &#34;&#34;&#34;
        Analyze clarity of rules for each system.

        Args:
            systems: Dictionary of game systems

        Returns:
            Tuple of (count of systems with clear rules, list of clarity scores)
        &#34;&#34;&#34;
        clear_count = 0
        clarity_scores = []

        for _key, value in systems.items():
            if not isinstance(value, (dict, str)):
                continue

            # Analyze this system
            score = 0.0

            if isinstance(value, dict):
                # Check for rules/description
                has_rules = &#34;rules&#34; in value or &#34;description&#34; in value
                has_examples = &#34;example&#34; in value or &#34;examples&#34; in value

                # Get text content
                text = str(value)
                word_count = len(text.split())

                # Scoring (adjusted for realistic content)
                if has_rules or has_examples:
                    score += 4.0  # Having rules/description is most important
                if word_count &gt;= 50:
                    score += 3.0  # Decent length
                elif word_count &gt;= 20:
                    score += 2.0

                # Structured data bonus
                if len(value) &gt;= 2:
                    score += 3.0

            elif isinstance(value, str):
                word_count = len(value.split())

                # Scoring based on length
                if word_count &gt;= 50:
                    score += 7.0
                elif word_count &gt;= 20:
                    score += 5.0
                elif word_count &gt;= 10:
                    score += 3.0

                # Check for structured language
                if any(word in value.lower() for word in [&#34;must&#34;, &#34;should&#34;, &#34;can&#34;, &#34;when&#34;, &#34;if&#34;]):
                    score += 2.0

            clarity_scores.append(min(score, 10.0))

            if score &gt;= 6.0:
                clear_count += 1

        return clear_count, clarity_scores

    @staticmethod
    def _has_balance_notes(mechanics: dict[str, Any]) -&gt; bool:
        &#34;&#34;&#34;
        Check if balance or difficulty is discussed.

        Args:
            mechanics: Mechanics dictionary

        Returns:
            True if balance is mentioned
        &#34;&#34;&#34;
        balance_keywords = [
            &#34;balance&#34;,
            &#34;difficulty&#34;,
            &#34;challenge&#34;,
            &#34;fair&#34;,
            &#34;balanced&#34;,
            &#34;easy&#34;,
            &#34;hard&#34;,
            &#34;moderate&#34;,
            &#34;scaling&#34;,
            &#34;progression curve&#34;,
        ]

        # Convert to string for searching
        mechanics_str = str(mechanics).lower()

        return any(keyword in mechanics_str for keyword in balance_keywords)

    def passes_threshold(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if the mechanics metrics pass all quality thresholds.

        Returns:
            True if all thresholds are met, False otherwise
        &#34;&#34;&#34;
        return (
            self.total_systems &gt;= self.min_systems
            and self.completeness_percentage &gt;= self.min_completeness
            and self.average_rule_clarity &gt;= self.min_clarity
        )

    def get_failures(self) -&gt; list[str]:
        &#34;&#34;&#34;
        Get list of failed quality checks.

        Returns:
            List of failure messages for metrics that don&#39;t meet thresholds
        &#34;&#34;&#34;
        failures = []

        if self.total_systems &lt; self.min_systems:
            failures.append(
                f&#34;Insufficient game systems: {self.total_systems} &#34; f&#34;(minimum: {self.min_systems})&#34;
            )

        if self.completeness_percentage &lt; self.min_completeness:
            failures.append(
                f&#34;Core systems incomplete: {self.completeness_percentage:.1f}% &#34;
                f&#34;(minimum: {self.min_completeness}%)&#34;
            )

        if self.average_rule_clarity &lt; self.min_clarity:
            failures.append(
                f&#34;Rules not clear enough: avg score {self.average_rule_clarity:.1f}/10 &#34;
                f&#34;(minimum: {self.min_clarity})&#34;
            )

        # Warnings for missing core systems
        missing_core = []
        if not self.has_combat_system:
            missing_core.append(&#34;combat&#34;)
        if not self.has_movement_system:
            missing_core.append(&#34;movement&#34;)
        if not self.has_inventory_system:
            missing_core.append(&#34;inventory&#34;)

        if missing_core:
            failures.append(f&#34;Warning: Missing core systems: {&#39;, &#39;.join(missing_core)}&#34;)

        return failures

    def get_score(self) -&gt; float:
        &#34;&#34;&#34;
        Calculate overall quality score (0.0 to 10.0).

        Returns:
            Quality score based on met criteria
        &#34;&#34;&#34;
        score = 0.0

        # Total systems (2 points)
        if self.total_systems &gt;= self.min_systems:
            score += 2.0
        elif self.total_systems &gt; 0:
            score += 2.0 * (self.total_systems / self.min_systems)

        # Completeness (3 points)
        score += 3.0 * (self.completeness_percentage / 100.0)

        # Rule clarity (3 points)
        score += 3.0 * (self.average_rule_clarity / 10.0)

        # Core systems (1.5 points total)
        core_systems = sum(
            [
                self.has_combat_system,
                self.has_movement_system,
                self.has_inventory_system,
            ]
        )
        score += 1.5 * (core_systems / 3.0)

        # Balance notes (0.5 points)
        if self.has_balance_notes:
            score += 0.5

        return min(score, 10.0)

    def to_dict(self) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Convert metrics to dictionary for serialization.

        Returns:
            Dictionary representation of metrics
        &#34;&#34;&#34;
        return {
            &#34;has_combat_system&#34;: self.has_combat_system,
            &#34;has_movement_system&#34;: self.has_movement_system,
            &#34;has_inventory_system&#34;: self.has_inventory_system,
            &#34;has_progression_system&#34;: self.has_progression_system,
            &#34;total_systems&#34;: self.total_systems,
            &#34;systems_with_rules&#34;: self.systems_with_rules,
            &#34;completeness_percentage&#34;: round(self.completeness_percentage, 1),
            &#34;average_rule_clarity&#34;: round(self.average_rule_clarity, 2),
            &#34;has_balance_notes&#34;: self.has_balance_notes,
            &#34;passes_threshold&#34;: self.passes_threshold(),
            &#34;score&#34;: self.get_score(),
            &#34;failures&#34;: self.get_failures(),
        }</code></pre>
</details>
<div class="desc"><p>Quality metrics for evaluating game mechanics/PRD content.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>has_combat_system</code></strong></dt>
<dd>Whether combat mechanics are described</dd>
<dt><strong><code>has_movement_system</code></strong></dt>
<dd>Whether movement mechanics are described</dd>
<dt><strong><code>has_inventory_system</code></strong></dt>
<dd>Whether inventory mechanics are described</dd>
<dt><strong><code>has_progression_system</code></strong></dt>
<dd>Whether character progression is described</dd>
<dt><strong><code>total_systems</code></strong></dt>
<dd>Total number of game systems described</dd>
<dt><strong><code>systems_with_rules</code></strong></dt>
<dd>Number of systems with clear rules</dd>
<dt><strong><code>completeness_percentage</code></strong></dt>
<dd>Percentage of expected systems present</dd>
<dt><strong><code>average_rule_clarity</code></strong></dt>
<dd>Average clarity score for rules</dd>
<dt><strong><code>has_balance_notes</code></strong></dt>
<dd>Whether difficulty/balance is discussed</dd>
<dt><strong><code>min_systems</code></strong></dt>
<dd>Minimum required systems (default: 3)</dd>
</dl></div>
<h3>Static methods</h3>
<dl>
<dt id="space_hulk_game.quality.MechanicsMetrics.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>data: dict[str, typing.Any]) ‑> <a title="space_hulk_game.quality.mechanics_metrics.MechanicsMetrics" href="mechanics_metrics.html#space_hulk_game.quality.mechanics_metrics.MechanicsMetrics">MechanicsMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create MechanicsMetrics from a dictionary (parsed YAML).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>Dictionary containing PRD/mechanics data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>MechanicsMetrics instance with measured values</p></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.from_yaml_content"><code class="name flex">
<span>def <span class="ident">from_yaml_content</span></span>(<span>yaml_content: str) ‑> <a title="space_hulk_game.quality.mechanics_metrics.MechanicsMetrics" href="mechanics_metrics.html#space_hulk_game.quality.mechanics_metrics.MechanicsMetrics">MechanicsMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create MechanicsMetrics from YAML content string.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yaml_content</code></strong></dt>
<dd>YAML string containing PRD/mechanics data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>MechanicsMetrics instance with measured values</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="space_hulk_game.quality.MechanicsMetrics.average_rule_clarity"><code class="name">var <span class="ident">average_rule_clarity</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.completeness_percentage"><code class="name">var <span class="ident">completeness_percentage</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.has_balance_notes"><code class="name">var <span class="ident">has_balance_notes</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.has_combat_system"><code class="name">var <span class="ident">has_combat_system</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.has_inventory_system"><code class="name">var <span class="ident">has_inventory_system</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.has_movement_system"><code class="name">var <span class="ident">has_movement_system</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.has_progression_system"><code class="name">var <span class="ident">has_progression_system</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.min_clarity"><code class="name">var <span class="ident">min_clarity</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.min_completeness"><code class="name">var <span class="ident">min_completeness</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.min_systems"><code class="name">var <span class="ident">min_systems</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.systems_with_rules"><code class="name">var <span class="ident">systems_with_rules</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.total_systems"><code class="name">var <span class="ident">total_systems</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.MechanicsMetrics.get_failures"><code class="name flex">
<span>def <span class="ident">get_failures</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_failures(self) -&gt; list[str]:
    &#34;&#34;&#34;
    Get list of failed quality checks.

    Returns:
        List of failure messages for metrics that don&#39;t meet thresholds
    &#34;&#34;&#34;
    failures = []

    if self.total_systems &lt; self.min_systems:
        failures.append(
            f&#34;Insufficient game systems: {self.total_systems} &#34; f&#34;(minimum: {self.min_systems})&#34;
        )

    if self.completeness_percentage &lt; self.min_completeness:
        failures.append(
            f&#34;Core systems incomplete: {self.completeness_percentage:.1f}% &#34;
            f&#34;(minimum: {self.min_completeness}%)&#34;
        )

    if self.average_rule_clarity &lt; self.min_clarity:
        failures.append(
            f&#34;Rules not clear enough: avg score {self.average_rule_clarity:.1f}/10 &#34;
            f&#34;(minimum: {self.min_clarity})&#34;
        )

    # Warnings for missing core systems
    missing_core = []
    if not self.has_combat_system:
        missing_core.append(&#34;combat&#34;)
    if not self.has_movement_system:
        missing_core.append(&#34;movement&#34;)
    if not self.has_inventory_system:
        missing_core.append(&#34;inventory&#34;)

    if missing_core:
        failures.append(f&#34;Warning: Missing core systems: {&#39;, &#39;.join(missing_core)}&#34;)

    return failures</code></pre>
</details>
<div class="desc"><p>Get list of failed quality checks.</p>
<h2 id="returns">Returns</h2>
<p>List of failure messages for metrics that don't meet thresholds</p></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.get_score"><code class="name flex">
<span>def <span class="ident">get_score</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_score(self) -&gt; float:
    &#34;&#34;&#34;
    Calculate overall quality score (0.0 to 10.0).

    Returns:
        Quality score based on met criteria
    &#34;&#34;&#34;
    score = 0.0

    # Total systems (2 points)
    if self.total_systems &gt;= self.min_systems:
        score += 2.0
    elif self.total_systems &gt; 0:
        score += 2.0 * (self.total_systems / self.min_systems)

    # Completeness (3 points)
    score += 3.0 * (self.completeness_percentage / 100.0)

    # Rule clarity (3 points)
    score += 3.0 * (self.average_rule_clarity / 10.0)

    # Core systems (1.5 points total)
    core_systems = sum(
        [
            self.has_combat_system,
            self.has_movement_system,
            self.has_inventory_system,
        ]
    )
    score += 1.5 * (core_systems / 3.0)

    # Balance notes (0.5 points)
    if self.has_balance_notes:
        score += 0.5

    return min(score, 10.0)</code></pre>
</details>
<div class="desc"><p>Calculate overall quality score (0.0 to 10.0).</p>
<h2 id="returns">Returns</h2>
<p>Quality score based on met criteria</p></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.passes_threshold"><code class="name flex">
<span>def <span class="ident">passes_threshold</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def passes_threshold(self) -&gt; bool:
    &#34;&#34;&#34;
    Check if the mechanics metrics pass all quality thresholds.

    Returns:
        True if all thresholds are met, False otherwise
    &#34;&#34;&#34;
    return (
        self.total_systems &gt;= self.min_systems
        and self.completeness_percentage &gt;= self.min_completeness
        and self.average_rule_clarity &gt;= self.min_clarity
    )</code></pre>
</details>
<div class="desc"><p>Check if the mechanics metrics pass all quality thresholds.</p>
<h2 id="returns">Returns</h2>
<p>True if all thresholds are met, False otherwise</p></div>
</dd>
<dt id="space_hulk_game.quality.MechanicsMetrics.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Convert metrics to dictionary for serialization.

    Returns:
        Dictionary representation of metrics
    &#34;&#34;&#34;
    return {
        &#34;has_combat_system&#34;: self.has_combat_system,
        &#34;has_movement_system&#34;: self.has_movement_system,
        &#34;has_inventory_system&#34;: self.has_inventory_system,
        &#34;has_progression_system&#34;: self.has_progression_system,
        &#34;total_systems&#34;: self.total_systems,
        &#34;systems_with_rules&#34;: self.systems_with_rules,
        &#34;completeness_percentage&#34;: round(self.completeness_percentage, 1),
        &#34;average_rule_clarity&#34;: round(self.average_rule_clarity, 2),
        &#34;has_balance_notes&#34;: self.has_balance_notes,
        &#34;passes_threshold&#34;: self.passes_threshold(),
        &#34;score&#34;: self.get_score(),
        &#34;failures&#34;: self.get_failures(),
    }</code></pre>
</details>
<div class="desc"><p>Convert metrics to dictionary for serialization.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary representation of metrics</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.NarrativeMapEvaluator"><code class="flex name class">
<span>class <span class="ident">NarrativeMapEvaluator</span></span>
<span>(</span><span>pass_threshold: float = 6.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NarrativeMapEvaluator(QualityEvaluator):
    &#34;&#34;&#34;
    Evaluator for narrative map content.

    Uses NarrativeMetrics to evaluate narrative maps against quality criteria
    including scene connectivity, completeness, and orphan detection.

    Example:
        &gt;&gt;&gt; evaluator = NarrativeMapEvaluator()
        &gt;&gt;&gt; score = evaluator.evaluate(narrative_yaml_content)
        &gt;&gt;&gt; print(f&#34;Score: {score.score}/10&#34;)
        &gt;&gt;&gt; print(score.feedback)
    &#34;&#34;&#34;

    def __init__(self, pass_threshold: float = 6.0):
        &#34;&#34;&#34;
        Initialize the narrative map evaluator.

        Args:
            pass_threshold: Minimum score required to pass (default: 6.0)
        &#34;&#34;&#34;
        super().__init__(pass_threshold)
        logger.info(f&#34;NarrativeMapEvaluator initialized with threshold {pass_threshold}&#34;)

    def evaluate(self, content: str) -&gt; QualityScore:
        &#34;&#34;&#34;
        Evaluate narrative map content.

        Args:
            content: YAML string containing narrative map

        Returns:
            QualityScore with evaluation results

        Raises:
            ValueError: If content cannot be parsed
        &#34;&#34;&#34;
        try:
            # Parse YAML content using metrics class
            metrics = NarrativeMetrics.from_yaml_content(content)

            # Get score and check if passes threshold
            score = metrics.get_score()
            passed = score &gt;= self.pass_threshold

            # Get failures for detailed feedback
            failures = metrics.get_failures()

            # Build feedback message
            feedback = self._build_feedback(score, failures)

            # Build details dictionary
            details = {
                &#34;total_scenes&#34;: metrics.total_scenes,
                &#34;scenes_with_descriptions&#34;: metrics.scenes_with_descriptions,
                &#34;all_connections_valid&#34;: metrics.all_connections_valid,
                &#34;has_orphaned_scenes&#34;: metrics.has_orphaned_scenes,
                &#34;orphaned_scenes&#34;: list(metrics.orphaned_scenes) if metrics.orphaned_scenes else [],
                &#34;invalid_connections&#34;: [],  # Legacy field, no longer tracked
                &#34;completeness_percentage&#34;: metrics.completeness_percentage,
                &#34;failures&#34;: failures,
                &#34;threshold&#34;: self.pass_threshold,
            }

            logger.info(
                f&#34;Narrative map evaluation complete: score={score:.1f}, passed={passed}, &#34;
                f&#34;scenes={metrics.total_scenes}, orphans={len(metrics.orphaned_scenes) if metrics.orphaned_scenes else 0}&#34;
            )

            return self._create_score(score, passed, feedback, details)

        except ValueError as e:
            logger.error(f&#34;Failed to evaluate narrative map content: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Failed to parse narrative map content: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )
        except Exception as e:
            logger.exception(f&#34;Unexpected error evaluating narrative map: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )

    def generate_detailed_feedback(self, content: str) -&gt; str:
        &#34;&#34;&#34;
        Generate detailed, actionable feedback for improvement.

        Args:
            content: YAML string containing narrative map

        Returns:
            Multi-line feedback with specific suggestions
        &#34;&#34;&#34;
        result = self.evaluate(content)

        lines = [
            f&#34;Narrative Map Quality Score: {result.score:.1f}/10.0&#34;,
            f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
            &#34;&#34;,
        ]

        if result.passed:
            lines.append(&#34;The narrative map meets all quality requirements!&#34;)
        else:
            lines.append(&#34;The narrative map needs improvement:&#34;)

        lines.append(&#34;&#34;)

        # Add specific findings
        details = result.details
        failures = details.get(&#34;failures&#34;, [])

        if failures:
            lines.append(&#34;Issues to address:&#34;)
            for failure in failures:
                lines.append(f&#34;  • {failure}&#34;)
            lines.append(&#34;&#34;)

        # Add scene statistics
        lines.append(f&#34;Scene count: {details.get(&#39;total_scenes&#39;, 0)} (min: 5)&#34;)
        lines.append(f&#34;Completeness: {details.get(&#39;completeness_percentage&#39;, 0):.1f}%&#34;)

        # List orphaned scenes if any
        orphaned = details.get(&#34;orphaned_scenes&#34;, [])
        if orphaned:
            lines.append(&#34;&#34;)
            lines.append(f&#34;Orphaned scenes ({len(orphaned)}):&#34;)
            for scene_id in orphaned:
                lines.append(f&#34;  • {scene_id}&#34;)

        # List invalid connections if any
        invalid = details.get(&#34;invalid_connections&#34;, [])
        if invalid:
            lines.append(&#34;&#34;)
            lines.append(f&#34;Invalid connections ({len(invalid)}):&#34;)
            for conn in invalid:
                lines.append(f&#34;  • {conn}&#34;)

        # Add positive feedback
        lines.append(&#34;&#34;)
        if details.get(&#34;scenes_with_descriptions&#34;, 0) == details.get(&#34;total_scenes&#34;, 0):
            lines.append(&#34;✓ All scenes have descriptions&#34;)
        if details.get(&#34;all_connections_valid&#34;):
            lines.append(&#34;✓ All connections are valid&#34;)
        if not details.get(&#34;has_orphaned_scenes&#34;):
            lines.append(&#34;✓ No orphaned scenes&#34;)

        return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Evaluator for narrative map content.</p>
<p>Uses NarrativeMetrics to evaluate narrative maps against quality criteria
including scene connectivity, completeness, and orphan detection.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; evaluator = NarrativeMapEvaluator()
&gt;&gt;&gt; score = evaluator.evaluate(narrative_yaml_content)
&gt;&gt;&gt; print(f&quot;Score: {score.score}/10&quot;)
&gt;&gt;&gt; print(score.feedback)
</code></pre>
<p>Initialize the narrative map evaluator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum score required to pass (default: 6.0)</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.NarrativeMapEvaluator.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, content: str) ‑> <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, content: str) -&gt; QualityScore:
    &#34;&#34;&#34;
    Evaluate narrative map content.

    Args:
        content: YAML string containing narrative map

    Returns:
        QualityScore with evaluation results

    Raises:
        ValueError: If content cannot be parsed
    &#34;&#34;&#34;
    try:
        # Parse YAML content using metrics class
        metrics = NarrativeMetrics.from_yaml_content(content)

        # Get score and check if passes threshold
        score = metrics.get_score()
        passed = score &gt;= self.pass_threshold

        # Get failures for detailed feedback
        failures = metrics.get_failures()

        # Build feedback message
        feedback = self._build_feedback(score, failures)

        # Build details dictionary
        details = {
            &#34;total_scenes&#34;: metrics.total_scenes,
            &#34;scenes_with_descriptions&#34;: metrics.scenes_with_descriptions,
            &#34;all_connections_valid&#34;: metrics.all_connections_valid,
            &#34;has_orphaned_scenes&#34;: metrics.has_orphaned_scenes,
            &#34;orphaned_scenes&#34;: list(metrics.orphaned_scenes) if metrics.orphaned_scenes else [],
            &#34;invalid_connections&#34;: [],  # Legacy field, no longer tracked
            &#34;completeness_percentage&#34;: metrics.completeness_percentage,
            &#34;failures&#34;: failures,
            &#34;threshold&#34;: self.pass_threshold,
        }

        logger.info(
            f&#34;Narrative map evaluation complete: score={score:.1f}, passed={passed}, &#34;
            f&#34;scenes={metrics.total_scenes}, orphans={len(metrics.orphaned_scenes) if metrics.orphaned_scenes else 0}&#34;
        )

        return self._create_score(score, passed, feedback, details)

    except ValueError as e:
        logger.error(f&#34;Failed to evaluate narrative map content: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Failed to parse narrative map content: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )
    except Exception as e:
        logger.exception(f&#34;Unexpected error evaluating narrative map: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )</code></pre>
</details>
<div class="desc"><p>Evaluate narrative map content.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing narrative map</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>QualityScore with evaluation results</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If content cannot be parsed</dd>
</dl></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMapEvaluator.generate_detailed_feedback"><code class="name flex">
<span>def <span class="ident">generate_detailed_feedback</span></span>(<span>self, content: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_detailed_feedback(self, content: str) -&gt; str:
    &#34;&#34;&#34;
    Generate detailed, actionable feedback for improvement.

    Args:
        content: YAML string containing narrative map

    Returns:
        Multi-line feedback with specific suggestions
    &#34;&#34;&#34;
    result = self.evaluate(content)

    lines = [
        f&#34;Narrative Map Quality Score: {result.score:.1f}/10.0&#34;,
        f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
        &#34;&#34;,
    ]

    if result.passed:
        lines.append(&#34;The narrative map meets all quality requirements!&#34;)
    else:
        lines.append(&#34;The narrative map needs improvement:&#34;)

    lines.append(&#34;&#34;)

    # Add specific findings
    details = result.details
    failures = details.get(&#34;failures&#34;, [])

    if failures:
        lines.append(&#34;Issues to address:&#34;)
        for failure in failures:
            lines.append(f&#34;  • {failure}&#34;)
        lines.append(&#34;&#34;)

    # Add scene statistics
    lines.append(f&#34;Scene count: {details.get(&#39;total_scenes&#39;, 0)} (min: 5)&#34;)
    lines.append(f&#34;Completeness: {details.get(&#39;completeness_percentage&#39;, 0):.1f}%&#34;)

    # List orphaned scenes if any
    orphaned = details.get(&#34;orphaned_scenes&#34;, [])
    if orphaned:
        lines.append(&#34;&#34;)
        lines.append(f&#34;Orphaned scenes ({len(orphaned)}):&#34;)
        for scene_id in orphaned:
            lines.append(f&#34;  • {scene_id}&#34;)

    # List invalid connections if any
    invalid = details.get(&#34;invalid_connections&#34;, [])
    if invalid:
        lines.append(&#34;&#34;)
        lines.append(f&#34;Invalid connections ({len(invalid)}):&#34;)
        for conn in invalid:
            lines.append(f&#34;  • {conn}&#34;)

    # Add positive feedback
    lines.append(&#34;&#34;)
    if details.get(&#34;scenes_with_descriptions&#34;, 0) == details.get(&#34;total_scenes&#34;, 0):
        lines.append(&#34;✓ All scenes have descriptions&#34;)
    if details.get(&#34;all_connections_valid&#34;):
        lines.append(&#34;✓ All connections are valid&#34;)
    if not details.get(&#34;has_orphaned_scenes&#34;):
        lines.append(&#34;✓ No orphaned scenes&#34;)

    return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Generate detailed, actionable feedback for improvement.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing narrative map</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Multi-line feedback with specific suggestions</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback">generate_feedback</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml">parse_yaml</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.score" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics"><code class="flex name class">
<span>class <span class="ident">NarrativeMetrics</span></span>
<span>(</span><span>total_scenes: int = 0,<br>scenes_with_descriptions: int = 0,<br>completeness_percentage: float = 0.0,<br>all_connections_valid: bool = True,<br>has_orphaned_scenes: bool = False,<br>orphaned_scenes: list[str] = &lt;factory&gt;,<br>has_start_scene: bool = False,<br>min_scenes: int = 5,<br>min_completeness: float = 90.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class NarrativeMetrics:
    &#34;&#34;&#34;
    Quality metrics for evaluating narrative map content.

    Attributes:
        total_scenes: Total number of scenes in the narrative
        scenes_with_descriptions: Number of scenes that have descriptions
        completeness_percentage: Percentage of scenes with descriptions
        all_connections_valid: Whether all scene connections reference valid scenes
        has_orphaned_scenes: Whether there are unreachable scenes
        orphaned_scenes: List of orphaned scene IDs
        scene_count: Number of scenes (for threshold check)
        has_start_scene: Whether a start scene is defined
        min_scenes: Minimum required scenes (default: 5)
    &#34;&#34;&#34;

    # Measured values
    total_scenes: int = 0
    scenes_with_descriptions: int = 0
    completeness_percentage: float = 0.0
    all_connections_valid: bool = True
    has_orphaned_scenes: bool = False
    orphaned_scenes: list[str] = field(default_factory=list)
    has_start_scene: bool = False

    # Thresholds
    min_scenes: int = 5
    min_completeness: float = 90.0  # 90% of scenes should have descriptions

    def __post_init__(self):
        &#34;&#34;&#34;Initialize mutable default values.&#34;&#34;&#34;
        if self.orphaned_scenes is None:
            self.orphaned_scenes = []

    @classmethod
    def from_yaml_content(cls, yaml_content: str) -&gt; &#34;NarrativeMetrics&#34;:
        &#34;&#34;&#34;
        Create NarrativeMetrics from YAML content string.

        Args:
            yaml_content: YAML string containing narrative map data

        Returns:
            NarrativeMetrics instance with measured values
        &#34;&#34;&#34;
        try:
            # Handle markdown-wrapped YAML
            content = yaml_content.strip()
            if content.startswith(&#34;```&#34;):
                lines = content.split(&#34;\n&#34;)
                content = &#34;\n&#34;.join(lines[1:-1])
                logger.debug(&#34;Stripped markdown fences from narrative YAML&#34;)

            data = yaml.safe_load(content)
            metrics = cls.from_dict(data)

            if metrics.has_orphaned_scenes:
                logger.warning(
                    f&#34;NarrativeMetrics: Found {len(metrics.orphaned_scenes)} orphaned scenes: {metrics.orphaned_scenes}&#34;
                )

            logger.info(
                f&#34;NarrativeMetrics parsed: score={metrics.get_score():.1f}/10, passes={metrics.passes_threshold()}&#34;
            )
            return metrics
        except Exception as e:
            logger.error(f&#34;Failed to parse narrative YAML content: {e}&#34;)
            raise ValueError(f&#34;Failed to parse YAML content: {e}&#34;) from e

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -&gt; &#34;NarrativeMetrics&#34;:
        &#34;&#34;&#34;
        Create NarrativeMetrics from a dictionary (parsed YAML).

        Args:
            data: Dictionary containing narrative map data

        Returns:
            NarrativeMetrics instance with measured values
        &#34;&#34;&#34;
        metrics = cls()

        # Get narrative map structure
        narrative_map = data.get(&#34;narrative_map&#34;, data)

        # Check for start scene
        metrics.has_start_scene = bool(narrative_map.get(&#34;start_scene&#34;))

        # Get scenes
        scenes = narrative_map.get(&#34;scenes&#34;, {})
        metrics.total_scenes = len(scenes)

        # Count scenes with descriptions
        for _scene_id, scene_data in scenes.items():
            if scene_data and scene_data.get(&#34;description&#34;):
                metrics.scenes_with_descriptions += 1

        # Calculate completeness percentage
        if metrics.total_scenes &gt; 0:
            metrics.completeness_percentage = (
                metrics.scenes_with_descriptions / metrics.total_scenes
            ) * 100.0

        # Validate connections
        metrics.all_connections_valid = cls._validate_connections(scenes)

        # Find orphaned scenes
        metrics.orphaned_scenes = cls._find_orphaned_scenes(
            scenes, narrative_map.get(&#34;start_scene&#34;)
        )
        metrics.has_orphaned_scenes = len(metrics.orphaned_scenes) &gt; 0

        return metrics

    @staticmethod
    def _validate_connections(scenes: dict[str, Any]) -&gt; bool:
        &#34;&#34;&#34;
        Validate that all scene connections reference valid scenes.

        Args:
            scenes: Dictionary of scenes

        Returns:
            True if all connections are valid, False otherwise
        &#34;&#34;&#34;
        scene_ids = set(scenes.keys())

        for _scene_id, scene_data in scenes.items():
            if not scene_data:
                continue

            connections = scene_data.get(&#34;connections&#34;, [])
            if not connections:
                continue

            for connection in connections:
                if isinstance(connection, dict):
                    target = connection.get(&#34;target&#34;)
                    if target and target not in scene_ids:
                        return False
                elif isinstance(connection, str):
                    if connection not in scene_ids:
                        return False

        return True

    @staticmethod
    def _find_orphaned_scenes(scenes: dict[str, Any], start_scene: str | None) -&gt; list[str]:
        &#34;&#34;&#34;
        Find scenes that are not reachable from the start scene.

        Args:
            scenes: Dictionary of scenes
            start_scene: ID of the starting scene

        Returns:
            List of orphaned scene IDs
        &#34;&#34;&#34;
        if not start_scene or start_scene not in scenes:
            # If no valid start scene, all scenes are potentially orphaned
            return list(scenes.keys())

        # Perform graph traversal to find reachable scenes (BFS)
        reachable = set()
        to_visit = deque([start_scene])

        while to_visit:
            current = to_visit.popleft()
            if current in reachable:
                continue

            reachable.add(current)

            # Get connections from current scene
            scene_data = scenes.get(current)
            if not scene_data:
                continue

            connections = scene_data.get(&#34;connections&#34;, [])
            for connection in connections:
                if isinstance(connection, dict):
                    target = connection.get(&#34;target&#34;)
                    if target and target not in reachable:
                        to_visit.append(target)
                elif isinstance(connection, str):
                    if connection not in reachable:
                        to_visit.append(connection)

        # Find orphaned scenes (all scenes minus reachable)
        all_scenes = set(scenes.keys())
        orphaned = all_scenes - reachable

        return sorted(orphaned)

    def passes_threshold(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if the narrative metrics pass all quality thresholds.

        Returns:
            True if all thresholds are met, False otherwise
        &#34;&#34;&#34;
        return (
            self.has_start_scene
            and self.total_scenes &gt;= self.min_scenes
            and self.completeness_percentage &gt;= self.min_completeness
            and self.all_connections_valid
            and not self.has_orphaned_scenes
        )

    def get_failures(self) -&gt; list[str]:
        &#34;&#34;&#34;
        Get list of failed quality checks.

        Returns:
            List of failure messages for metrics that don&#39;t meet thresholds
        &#34;&#34;&#34;
        failures = []

        if not self.has_start_scene:
            failures.append(&#34;No start scene defined&#34;)

        if self.total_scenes &lt; self.min_scenes:
            failures.append(
                f&#34;Insufficient scenes: {self.total_scenes} &#34; f&#34;(minimum: {self.min_scenes})&#34;
            )

        if self.completeness_percentage &lt; self.min_completeness:
            failures.append(
                f&#34;Scene descriptions incomplete: {self.completeness_percentage:.1f}% &#34;
                f&#34;(minimum: {self.min_completeness}%)&#34;
            )

        if not self.all_connections_valid:
            failures.append(&#34;Some scene connections reference invalid scenes&#34;)

        if self.has_orphaned_scenes:
            failures.append(f&#34;Orphaned scenes found: {&#39;, &#39;.join(self.orphaned_scenes)}&#34;)

        return failures

    def get_score(self) -&gt; float:
        &#34;&#34;&#34;
        Calculate overall quality score (0.0 to 10.0).

        Returns:
            Quality score based on met criteria
        &#34;&#34;&#34;
        score = 0.0

        # Start scene (2 points)
        if self.has_start_scene:
            score += 2.0

        # Minimum scenes (2 points)
        if self.total_scenes &gt;= self.min_scenes:
            score += 2.0
        elif self.total_scenes &gt; 0:
            # Partial credit
            score += 2.0 * (self.total_scenes / self.min_scenes)

        # Completeness (3 points)
        score += 3.0 * (self.completeness_percentage / 100.0)

        # Valid connections (2 points)
        if self.all_connections_valid:
            score += 2.0

        # No orphaned scenes (1 point)
        if not self.has_orphaned_scenes:
            score += 1.0

        return min(score, 10.0)

    def to_dict(self) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Convert metrics to dictionary for serialization.

        Returns:
            Dictionary representation of metrics
        &#34;&#34;&#34;
        return {
            &#34;total_scenes&#34;: self.total_scenes,
            &#34;scenes_with_descriptions&#34;: self.scenes_with_descriptions,
            &#34;completeness_percentage&#34;: round(self.completeness_percentage, 1),
            &#34;all_connections_valid&#34;: self.all_connections_valid,
            &#34;has_orphaned_scenes&#34;: self.has_orphaned_scenes,
            &#34;orphaned_scenes&#34;: self.orphaned_scenes,
            &#34;has_start_scene&#34;: self.has_start_scene,
            &#34;passes_threshold&#34;: self.passes_threshold(),
            &#34;score&#34;: self.get_score(),
            &#34;failures&#34;: self.get_failures(),
        }</code></pre>
</details>
<div class="desc"><p>Quality metrics for evaluating narrative map content.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>total_scenes</code></strong></dt>
<dd>Total number of scenes in the narrative</dd>
<dt><strong><code>scenes_with_descriptions</code></strong></dt>
<dd>Number of scenes that have descriptions</dd>
<dt><strong><code>completeness_percentage</code></strong></dt>
<dd>Percentage of scenes with descriptions</dd>
<dt><strong><code>all_connections_valid</code></strong></dt>
<dd>Whether all scene connections reference valid scenes</dd>
<dt><strong><code>has_orphaned_scenes</code></strong></dt>
<dd>Whether there are unreachable scenes</dd>
<dt><strong><code>orphaned_scenes</code></strong></dt>
<dd>List of orphaned scene IDs</dd>
<dt><strong><code>scene_count</code></strong></dt>
<dd>Number of scenes (for threshold check)</dd>
<dt><strong><code>has_start_scene</code></strong></dt>
<dd>Whether a start scene is defined</dd>
<dt><strong><code>min_scenes</code></strong></dt>
<dd>Minimum required scenes (default: 5)</dd>
</dl></div>
<h3>Static methods</h3>
<dl>
<dt id="space_hulk_game.quality.NarrativeMetrics.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>data: dict[str, typing.Any]) ‑> <a title="space_hulk_game.quality.narrative_metrics.NarrativeMetrics" href="narrative_metrics.html#space_hulk_game.quality.narrative_metrics.NarrativeMetrics">NarrativeMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create NarrativeMetrics from a dictionary (parsed YAML).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>Dictionary containing narrative map data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NarrativeMetrics instance with measured values</p></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.from_yaml_content"><code class="name flex">
<span>def <span class="ident">from_yaml_content</span></span>(<span>yaml_content: str) ‑> <a title="space_hulk_game.quality.narrative_metrics.NarrativeMetrics" href="narrative_metrics.html#space_hulk_game.quality.narrative_metrics.NarrativeMetrics">NarrativeMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create NarrativeMetrics from YAML content string.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yaml_content</code></strong></dt>
<dd>YAML string containing narrative map data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>NarrativeMetrics instance with measured values</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="space_hulk_game.quality.NarrativeMetrics.all_connections_valid"><code class="name">var <span class="ident">all_connections_valid</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.completeness_percentage"><code class="name">var <span class="ident">completeness_percentage</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.has_orphaned_scenes"><code class="name">var <span class="ident">has_orphaned_scenes</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.has_start_scene"><code class="name">var <span class="ident">has_start_scene</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.min_completeness"><code class="name">var <span class="ident">min_completeness</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.min_scenes"><code class="name">var <span class="ident">min_scenes</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.orphaned_scenes"><code class="name">var <span class="ident">orphaned_scenes</span> : list[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.scenes_with_descriptions"><code class="name">var <span class="ident">scenes_with_descriptions</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.total_scenes"><code class="name">var <span class="ident">total_scenes</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.NarrativeMetrics.get_failures"><code class="name flex">
<span>def <span class="ident">get_failures</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_failures(self) -&gt; list[str]:
    &#34;&#34;&#34;
    Get list of failed quality checks.

    Returns:
        List of failure messages for metrics that don&#39;t meet thresholds
    &#34;&#34;&#34;
    failures = []

    if not self.has_start_scene:
        failures.append(&#34;No start scene defined&#34;)

    if self.total_scenes &lt; self.min_scenes:
        failures.append(
            f&#34;Insufficient scenes: {self.total_scenes} &#34; f&#34;(minimum: {self.min_scenes})&#34;
        )

    if self.completeness_percentage &lt; self.min_completeness:
        failures.append(
            f&#34;Scene descriptions incomplete: {self.completeness_percentage:.1f}% &#34;
            f&#34;(minimum: {self.min_completeness}%)&#34;
        )

    if not self.all_connections_valid:
        failures.append(&#34;Some scene connections reference invalid scenes&#34;)

    if self.has_orphaned_scenes:
        failures.append(f&#34;Orphaned scenes found: {&#39;, &#39;.join(self.orphaned_scenes)}&#34;)

    return failures</code></pre>
</details>
<div class="desc"><p>Get list of failed quality checks.</p>
<h2 id="returns">Returns</h2>
<p>List of failure messages for metrics that don't meet thresholds</p></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.get_score"><code class="name flex">
<span>def <span class="ident">get_score</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_score(self) -&gt; float:
    &#34;&#34;&#34;
    Calculate overall quality score (0.0 to 10.0).

    Returns:
        Quality score based on met criteria
    &#34;&#34;&#34;
    score = 0.0

    # Start scene (2 points)
    if self.has_start_scene:
        score += 2.0

    # Minimum scenes (2 points)
    if self.total_scenes &gt;= self.min_scenes:
        score += 2.0
    elif self.total_scenes &gt; 0:
        # Partial credit
        score += 2.0 * (self.total_scenes / self.min_scenes)

    # Completeness (3 points)
    score += 3.0 * (self.completeness_percentage / 100.0)

    # Valid connections (2 points)
    if self.all_connections_valid:
        score += 2.0

    # No orphaned scenes (1 point)
    if not self.has_orphaned_scenes:
        score += 1.0

    return min(score, 10.0)</code></pre>
</details>
<div class="desc"><p>Calculate overall quality score (0.0 to 10.0).</p>
<h2 id="returns">Returns</h2>
<p>Quality score based on met criteria</p></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.passes_threshold"><code class="name flex">
<span>def <span class="ident">passes_threshold</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def passes_threshold(self) -&gt; bool:
    &#34;&#34;&#34;
    Check if the narrative metrics pass all quality thresholds.

    Returns:
        True if all thresholds are met, False otherwise
    &#34;&#34;&#34;
    return (
        self.has_start_scene
        and self.total_scenes &gt;= self.min_scenes
        and self.completeness_percentage &gt;= self.min_completeness
        and self.all_connections_valid
        and not self.has_orphaned_scenes
    )</code></pre>
</details>
<div class="desc"><p>Check if the narrative metrics pass all quality thresholds.</p>
<h2 id="returns">Returns</h2>
<p>True if all thresholds are met, False otherwise</p></div>
</dd>
<dt id="space_hulk_game.quality.NarrativeMetrics.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Convert metrics to dictionary for serialization.

    Returns:
        Dictionary representation of metrics
    &#34;&#34;&#34;
    return {
        &#34;total_scenes&#34;: self.total_scenes,
        &#34;scenes_with_descriptions&#34;: self.scenes_with_descriptions,
        &#34;completeness_percentage&#34;: round(self.completeness_percentage, 1),
        &#34;all_connections_valid&#34;: self.all_connections_valid,
        &#34;has_orphaned_scenes&#34;: self.has_orphaned_scenes,
        &#34;orphaned_scenes&#34;: self.orphaned_scenes,
        &#34;has_start_scene&#34;: self.has_start_scene,
        &#34;passes_threshold&#34;: self.passes_threshold(),
        &#34;score&#34;: self.get_score(),
        &#34;failures&#34;: self.get_failures(),
    }</code></pre>
</details>
<div class="desc"><p>Convert metrics to dictionary for serialization.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary representation of metrics</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.PlotEvaluator"><code class="flex name class">
<span>class <span class="ident">PlotEvaluator</span></span>
<span>(</span><span>pass_threshold: float = 6.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PlotEvaluator(QualityEvaluator):
    &#34;&#34;&#34;
    Evaluator for plot outline content.

    Uses PlotMetrics to evaluate plot outlines against quality criteria
    including setting clarity, branching paths, endings, themes, and word count.

    Example:
        &gt;&gt;&gt; evaluator = PlotEvaluator()
        &gt;&gt;&gt; score = evaluator.evaluate(plot_yaml_content)
        &gt;&gt;&gt; print(f&#34;Score: {score.score}/10&#34;)
        &gt;&gt;&gt; print(score.feedback)
    &#34;&#34;&#34;

    def __init__(self, pass_threshold: float = 6.0):
        &#34;&#34;&#34;
        Initialize the plot evaluator.

        Args:
            pass_threshold: Minimum score required to pass (default: 6.0)
        &#34;&#34;&#34;
        super().__init__(pass_threshold)
        logger.info(f&#34;PlotEvaluator initialized with threshold {pass_threshold}&#34;)

    def evaluate(self, content: str) -&gt; QualityScore:
        &#34;&#34;&#34;
        Evaluate plot outline content.

        Args:
            content: YAML string containing plot outline

        Returns:
            QualityScore with evaluation results

        Raises:
            ValueError: If content cannot be parsed
        &#34;&#34;&#34;
        try:
            # Parse YAML content using metrics class
            metrics = PlotMetrics.from_yaml_content(content)

            # Get score and check if passes threshold
            score = metrics.get_score()
            passed = score &gt;= self.pass_threshold

            # Get failures for detailed feedback
            failures = metrics.get_failures()

            # Build feedback message
            feedback = self._build_feedback(score, failures)

            # Build details dictionary
            details = {
                &#34;has_clear_setting&#34;: metrics.has_clear_setting,
                &#34;branching_paths_count&#34;: metrics.branching_paths_count,
                &#34;endings_count&#34;: metrics.endings_count,
                &#34;themes_defined&#34;: metrics.themes_defined,
                &#34;word_count&#34;: metrics.word_count,
                &#34;has_title&#34;: metrics.has_title,
                &#34;has_prologue&#34;: metrics.has_prologue,
                &#34;has_acts&#34;: metrics.has_acts,
                &#34;failures&#34;: failures,
                &#34;threshold&#34;: self.pass_threshold,
            }

            logger.info(
                f&#34;Plot evaluation complete: score={score:.1f}, passed={passed}, &#34;
                f&#34;failures={len(failures)}&#34;
            )

            return self._create_score(score, passed, feedback, details)

        except ValueError as e:
            logger.error(f&#34;Failed to evaluate plot content: {e}&#34;)
            # Return failing score with error message
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Failed to parse plot content: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )
        except Exception as e:
            logger.exception(f&#34;Unexpected error evaluating plot: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )

    def generate_detailed_feedback(self, content: str) -&gt; str:
        &#34;&#34;&#34;
        Generate detailed, actionable feedback for improvement.

        Args:
            content: YAML string containing plot outline

        Returns:
            Multi-line feedback with specific suggestions
        &#34;&#34;&#34;
        result = self.evaluate(content)

        lines = [
            f&#34;Plot Quality Score: {result.score:.1f}/10.0&#34;,
            f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
            &#34;&#34;,
        ]

        if result.passed:
            lines.append(&#34;The plot outline meets all quality requirements!&#34;)
        else:
            lines.append(&#34;The plot outline needs improvement:&#34;)

        lines.append(&#34;&#34;)

        # Add specific findings
        details = result.details
        failures = details.get(&#34;failures&#34;, [])

        if failures:
            lines.append(&#34;Issues to address:&#34;)
            for failure in failures:
                lines.append(f&#34;  • {failure}&#34;)
            lines.append(&#34;&#34;)

        # Add positive feedback
        if details.get(&#34;has_clear_setting&#34;):
            lines.append(&#34;✓ Clear setting defined&#34;)
        if details.get(&#34;has_title&#34;):
            lines.append(&#34;✓ Title present&#34;)
        if details.get(&#34;themes_defined&#34;):
            lines.append(&#34;✓ Themes clearly stated&#34;)
        if details.get(&#34;has_prologue&#34;):
            lines.append(&#34;✓ Prologue included&#34;)
        if details.get(&#34;has_acts&#34;):
            lines.append(&#34;✓ Structured into acts&#34;)

        lines.append(&#34;&#34;)
        lines.append(f&#34;Word count: {details.get(&#39;word_count&#39;, 0)} (min: 500)&#34;)
        lines.append(f&#34;Branching paths: {details.get(&#39;branching_paths_count&#39;, 0)} (min: 2)&#34;)
        lines.append(f&#34;Endings: {details.get(&#39;endings_count&#39;, 0)} (min: 2)&#34;)

        return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Evaluator for plot outline content.</p>
<p>Uses PlotMetrics to evaluate plot outlines against quality criteria
including setting clarity, branching paths, endings, themes, and word count.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; evaluator = PlotEvaluator()
&gt;&gt;&gt; score = evaluator.evaluate(plot_yaml_content)
&gt;&gt;&gt; print(f&quot;Score: {score.score}/10&quot;)
&gt;&gt;&gt; print(score.feedback)
</code></pre>
<p>Initialize the plot evaluator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum score required to pass (default: 6.0)</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.PlotEvaluator.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, content: str) ‑> <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, content: str) -&gt; QualityScore:
    &#34;&#34;&#34;
    Evaluate plot outline content.

    Args:
        content: YAML string containing plot outline

    Returns:
        QualityScore with evaluation results

    Raises:
        ValueError: If content cannot be parsed
    &#34;&#34;&#34;
    try:
        # Parse YAML content using metrics class
        metrics = PlotMetrics.from_yaml_content(content)

        # Get score and check if passes threshold
        score = metrics.get_score()
        passed = score &gt;= self.pass_threshold

        # Get failures for detailed feedback
        failures = metrics.get_failures()

        # Build feedback message
        feedback = self._build_feedback(score, failures)

        # Build details dictionary
        details = {
            &#34;has_clear_setting&#34;: metrics.has_clear_setting,
            &#34;branching_paths_count&#34;: metrics.branching_paths_count,
            &#34;endings_count&#34;: metrics.endings_count,
            &#34;themes_defined&#34;: metrics.themes_defined,
            &#34;word_count&#34;: metrics.word_count,
            &#34;has_title&#34;: metrics.has_title,
            &#34;has_prologue&#34;: metrics.has_prologue,
            &#34;has_acts&#34;: metrics.has_acts,
            &#34;failures&#34;: failures,
            &#34;threshold&#34;: self.pass_threshold,
        }

        logger.info(
            f&#34;Plot evaluation complete: score={score:.1f}, passed={passed}, &#34;
            f&#34;failures={len(failures)}&#34;
        )

        return self._create_score(score, passed, feedback, details)

    except ValueError as e:
        logger.error(f&#34;Failed to evaluate plot content: {e}&#34;)
        # Return failing score with error message
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Failed to parse plot content: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )
    except Exception as e:
        logger.exception(f&#34;Unexpected error evaluating plot: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )</code></pre>
</details>
<div class="desc"><p>Evaluate plot outline content.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing plot outline</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>QualityScore with evaluation results</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If content cannot be parsed</dd>
</dl></div>
</dd>
<dt id="space_hulk_game.quality.PlotEvaluator.generate_detailed_feedback"><code class="name flex">
<span>def <span class="ident">generate_detailed_feedback</span></span>(<span>self, content: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_detailed_feedback(self, content: str) -&gt; str:
    &#34;&#34;&#34;
    Generate detailed, actionable feedback for improvement.

    Args:
        content: YAML string containing plot outline

    Returns:
        Multi-line feedback with specific suggestions
    &#34;&#34;&#34;
    result = self.evaluate(content)

    lines = [
        f&#34;Plot Quality Score: {result.score:.1f}/10.0&#34;,
        f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
        &#34;&#34;,
    ]

    if result.passed:
        lines.append(&#34;The plot outline meets all quality requirements!&#34;)
    else:
        lines.append(&#34;The plot outline needs improvement:&#34;)

    lines.append(&#34;&#34;)

    # Add specific findings
    details = result.details
    failures = details.get(&#34;failures&#34;, [])

    if failures:
        lines.append(&#34;Issues to address:&#34;)
        for failure in failures:
            lines.append(f&#34;  • {failure}&#34;)
        lines.append(&#34;&#34;)

    # Add positive feedback
    if details.get(&#34;has_clear_setting&#34;):
        lines.append(&#34;✓ Clear setting defined&#34;)
    if details.get(&#34;has_title&#34;):
        lines.append(&#34;✓ Title present&#34;)
    if details.get(&#34;themes_defined&#34;):
        lines.append(&#34;✓ Themes clearly stated&#34;)
    if details.get(&#34;has_prologue&#34;):
        lines.append(&#34;✓ Prologue included&#34;)
    if details.get(&#34;has_acts&#34;):
        lines.append(&#34;✓ Structured into acts&#34;)

    lines.append(&#34;&#34;)
    lines.append(f&#34;Word count: {details.get(&#39;word_count&#39;, 0)} (min: 500)&#34;)
    lines.append(f&#34;Branching paths: {details.get(&#39;branching_paths_count&#39;, 0)} (min: 2)&#34;)
    lines.append(f&#34;Endings: {details.get(&#39;endings_count&#39;, 0)} (min: 2)&#34;)

    return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Generate detailed, actionable feedback for improvement.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing plot outline</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Multi-line feedback with specific suggestions</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback">generate_feedback</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml">parse_yaml</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.score" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics"><code class="flex name class">
<span>class <span class="ident">PlotMetrics</span></span>
<span>(</span><span>has_clear_setting: bool = False,<br>branching_paths_count: int = 0,<br>endings_count: int = 0,<br>themes_defined: bool = False,<br>word_count: int = 0,<br>has_title: bool = False,<br>has_prologue: bool = False,<br>has_acts: bool = False,<br>min_branching_paths: int = 2,<br>min_endings: int = 2,<br>min_word_count: int = 500)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class PlotMetrics:
    &#34;&#34;&#34;
    Quality metrics for evaluating plot outline content.

    Attributes:
        has_clear_setting: Whether the plot has a clearly defined setting (yes/no)
        branching_paths_count: Number of branching paths in the plot (count)
        endings_count: Number of defined endings (count)
        themes_defined: Whether themes are clearly stated (yes/no)
        word_count: Total word count of the plot outline (number)
        has_title: Whether the plot has a title (yes/no)
        has_prologue: Whether the plot includes a prologue (yes/no)
        has_acts: Whether the plot is structured into acts (yes/no)
        min_branching_paths: Minimum required branching paths (default: 2)
        min_endings: Minimum required endings (default: 2)
        min_word_count: Minimum required word count (default: 500)
    &#34;&#34;&#34;

    # Measured values
    has_clear_setting: bool = False
    branching_paths_count: int = 0
    endings_count: int = 0
    themes_defined: bool = False
    word_count: int = 0
    has_title: bool = False
    has_prologue: bool = False
    has_acts: bool = False

    # Thresholds
    min_branching_paths: int = 2
    min_endings: int = 2
    min_word_count: int = 500

    @classmethod
    def from_yaml_content(cls, yaml_content: str) -&gt; &#34;PlotMetrics&#34;:
        &#34;&#34;&#34;
        Create PlotMetrics from YAML content string.

        Args:
            yaml_content: YAML string containing plot outline data

        Returns:
            PlotMetrics instance with measured values

        Example:
            &gt;&gt;&gt; metrics = PlotMetrics.from_yaml_content(yaml_string)
            &gt;&gt;&gt; print(metrics.passes_threshold())
        &#34;&#34;&#34;
        try:
            # Handle markdown-wrapped YAML
            content = yaml_content.strip()
            if content.startswith(&#34;```&#34;):
                lines = content.split(&#34;\n&#34;)
                # Remove first and last lines (markdown fences)
                content = &#34;\n&#34;.join(lines[1:-1])
                logger.debug(&#34;Stripped markdown fences from YAML content&#34;)

            # Try to parse as-is first
            try:
                data = yaml.safe_load(content)
            except yaml.YAMLError as parse_error:
                # Attempt to fix common YAML syntax errors (e.g., unquoted colons in values)
                logger.debug(f&#34;Initial YAML parse failed, attempting to fix: {parse_error}&#34;)
                content = cls._fix_yaml_syntax(content)
                data = yaml.safe_load(content)
                logger.info(&#34;Successfully parsed YAML after fixing syntax errors&#34;)

            metrics = cls.from_dict(data)
            logger.info(
                f&#34;PlotMetrics parsed: score={metrics.get_score():.1f}/10, passes={metrics.passes_threshold()}&#34;
            )
            return metrics
        except Exception as e:
            logger.error(f&#34;Failed to parse plot YAML content: {e}&#34;)
            raise ValueError(f&#34;Failed to parse YAML content: {e}&#34;) from e

    @staticmethod
    def _fix_yaml_syntax(content: str) -&gt; str:
        &#34;&#34;&#34;
        Fix common YAML syntax errors like unquoted colons in values.

        Args:
            content: YAML string with potential syntax errors

        Returns:
            Fixed YAML string
        &#34;&#34;&#34;
        lines = content.split(&#34;\n&#34;)
        fixed_lines = []

        for line in lines:
            # Fix unquoted values with colons (e.g., &#34;title: Space Hulk: Derelict&#34;)
            if &#34;:&#34; in line and not line.strip().startswith(&#34;-&#34;):
                parts = line.split(&#34;:&#34;, 1)
                if len(parts) == 2:
                    key_part = parts[0]
                    value_part = parts[1].strip()

                    # If value has a colon and is not already quoted or is a nested structure
                    if (
                        &#34;:&#34; in value_part
                        and value_part
                        and not (
                            (value_part.startswith(&#39;&#34;&#39;) and value_part.endswith(&#39;&#34;&#39;))
                            or (value_part.startswith(&#34;&#39;&#34;) and value_part.endswith(&#34;&#39;&#34;))
                            or value_part.startswith(&#34;[&#34;)
                            or value_part.startswith(&#34;{&#34;)
                        )
                    ):
                        # Quote the value
                        fixed_line = f&#39;{key_part}: &#34;{value_part}&#34;&#39;
                        fixed_lines.append(fixed_line)
                        logger.debug(f&#34;Fixed YAML syntax: {line.strip()}&#34;)
                        continue

            fixed_lines.append(line)

        return &#34;\n&#34;.join(fixed_lines)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -&gt; &#34;PlotMetrics&#34;:
        &#34;&#34;&#34;
        Create PlotMetrics from a dictionary (parsed YAML).

        Args:
            data: Dictionary containing plot outline data

        Returns:
            PlotMetrics instance with measured values
        &#34;&#34;&#34;
        metrics = cls()

        # Check for title
        metrics.has_title = bool(data.get(&#34;title&#34;))

        # Check for clear setting
        setting = data.get(&#34;setting&#34;)
        if setting:
            if isinstance(setting, dict):
                # Setting is a structured dict with location, time, environment
                metrics.has_clear_setting = bool(
                    setting.get(&#34;location&#34;) or setting.get(&#34;time&#34;) or setting.get(&#34;environment&#34;)
                )
            elif isinstance(setting, str):
                # Setting is a simple string - any non-empty string is valid
                metrics.has_clear_setting = len(setting.strip()) &gt; 0

        # Check for themes
        themes = data.get(&#34;themes&#34;, [])
        metrics.themes_defined = bool(themes) and len(themes) &gt; 0

        # Count branching paths
        metrics.branching_paths_count = cls._count_branching_paths(data)

        # Count endings
        metrics.endings_count = cls._count_endings(data)

        # Check for prologue
        plot = data.get(&#34;plot&#34;, {})
        metrics.has_prologue = &#34;prologue&#34; in plot

        # Check for acts (act1, act2, act3, etc.)
        metrics.has_acts = any(key.startswith(&#34;act&#34;) for key in plot.keys())

        # Calculate word count
        metrics.word_count = cls._calculate_word_count(data)

        return metrics

    @staticmethod
    def _count_branching_paths(data: dict[str, Any]) -&gt; int:
        &#34;&#34;&#34;
        Count the number of branching paths in the plot.

        Searches for &#34;Branching Path&#34; mentions in the plot structure
        and validates them by checking for associated choice options.
        &#34;&#34;&#34;
        count = 0
        plot = data.get(&#34;plot&#34;, {})

        # Convert entire plot to string for searching
        plot_str = str(plot)

        # Count explicit &#34;Branching Path&#34; mentions (primary method)
        # This is more reliable than counting option patterns
        branching_path_count = plot_str.count(&#34;Branching Path&#34;)
        count += branching_path_count

        # If no explicit &#34;Branching Path&#34; mentions, look for structured choice patterns
        # Only as a fallback, and with more careful validation
        if branching_path_count == 0:
            lines = plot_str.split(&#34;\n&#34;)
            in_choice_block = False

            for i, line in enumerate(lines):
                line_stripped = line.strip()

                # Look for &#34;A)&#34; at start of line, preceded by some context
                # and followed by &#34;B)&#34; to confirm it&#39;s a real choice
                if line_stripped.startswith(&#34;A)&#34;) and not in_choice_block:
                    # Check if next few lines contain &#34;B)&#34; to validate this is a choice
                    is_valid_choice = False
                    for j in range(i + 1, min(i + 5, len(lines))):
                        if lines[j].strip().startswith(&#34;B)&#34;):
                            is_valid_choice = True
                            break

                    if is_valid_choice:
                        count += 1
                        in_choice_block = True
                elif not line_stripped.startswith((&#34;A)&#34;, &#34;B)&#34;, &#34;C)&#34;, &#34;D)&#34;)):
                    in_choice_block = False

        return count

    @staticmethod
    def _count_endings(data: dict[str, Any]) -&gt; int:
        &#34;&#34;&#34;
        Count the number of defined endings.

        Looks for &#39;endings&#39; key or ending-related content in the plot.
        &#34;&#34;&#34;
        # Check for explicit endings section
        endings = data.get(&#34;endings&#34;)
        if endings:
            if isinstance(endings, list):
                return len(endings)
            elif isinstance(endings, dict):
                return len(endings)

        # Check within plot structure
        plot = data.get(&#34;plot&#34;, {})
        if &#34;endings&#34; in plot:
            endings = plot[&#34;endings&#34;]
            if isinstance(endings, list):
                return len(endings)
            elif isinstance(endings, dict):
                return len(endings)

        # Count ending-related mentions
        plot_str = str(data).lower()
        ending_keywords = [&#34;good ending&#34;, &#34;bad ending&#34;, &#34;ending:&#34;]
        count = sum(plot_str.count(keyword) for keyword in ending_keywords)

        return max(count, 0)

    @staticmethod
    def _calculate_word_count(data: dict[str, Any]) -&gt; int:
        &#34;&#34;&#34;
        Calculate total word count of the plot outline.

        Counts all words in string values throughout the structure.
        &#34;&#34;&#34;

        def count_words_recursive(obj: Any) -&gt; int:
            &#34;&#34;&#34;Recursively count words in nested structures.&#34;&#34;&#34;
            if isinstance(obj, str):
                return len(obj.split())
            elif isinstance(obj, dict):
                return sum(count_words_recursive(v) for v in obj.values())
            elif isinstance(obj, list):
                return sum(count_words_recursive(item) for item in obj)
            else:
                return 0

        return count_words_recursive(data)

    def passes_threshold(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if the plot metrics pass all quality thresholds.

        Returns:
            True if all thresholds are met, False otherwise
        &#34;&#34;&#34;
        return (
            self.has_clear_setting
            and self.branching_paths_count &gt;= self.min_branching_paths
            and self.endings_count &gt;= self.min_endings
            and self.themes_defined
            and self.word_count &gt;= self.min_word_count
        )

    def get_failures(self) -&gt; list[str]:
        &#34;&#34;&#34;
        Get list of failed quality checks.

        Returns:
            List of failure messages for metrics that don&#39;t meet thresholds
        &#34;&#34;&#34;
        failures = []

        if not self.has_clear_setting:
            failures.append(&#34;Plot lacks a clear setting description&#34;)

        if self.branching_paths_count &lt; self.min_branching_paths:
            failures.append(
                f&#34;Insufficient branching paths: {self.branching_paths_count} &#34;
                f&#34;(minimum: {self.min_branching_paths})&#34;
            )

        if self.endings_count &lt; self.min_endings:
            failures.append(
                f&#34;Insufficient endings: {self.endings_count} &#34; f&#34;(minimum: {self.min_endings})&#34;
            )

        if not self.themes_defined:
            failures.append(&#34;Themes are not clearly defined&#34;)

        if self.word_count &lt; self.min_word_count:
            failures.append(
                f&#34;Word count too low: {self.word_count} &#34; f&#34;(minimum: {self.min_word_count})&#34;
            )

        return failures

    def get_score(self) -&gt; float:
        &#34;&#34;&#34;
        Calculate overall quality score (0.0 to 10.0).

        Returns:
            Quality score based on met criteria
        &#34;&#34;&#34;
        total_checks = 5  # Number of main quality checks
        passed_checks = 0

        if self.has_clear_setting:
            passed_checks += 1
        if self.branching_paths_count &gt;= self.min_branching_paths:
            passed_checks += 1
        if self.endings_count &gt;= self.min_endings:
            passed_checks += 1
        if self.themes_defined:
            passed_checks += 1
        if self.word_count &gt;= self.min_word_count:
            passed_checks += 1

        return (passed_checks / total_checks) * 10.0

    def to_dict(self) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Convert metrics to dictionary for serialization.

        Returns:
            Dictionary representation of metrics
        &#34;&#34;&#34;
        return {
            &#34;has_clear_setting&#34;: self.has_clear_setting,
            &#34;branching_paths_count&#34;: self.branching_paths_count,
            &#34;endings_count&#34;: self.endings_count,
            &#34;themes_defined&#34;: self.themes_defined,
            &#34;word_count&#34;: self.word_count,
            &#34;has_title&#34;: self.has_title,
            &#34;has_prologue&#34;: self.has_prologue,
            &#34;has_acts&#34;: self.has_acts,
            &#34;passes_threshold&#34;: self.passes_threshold(),
            &#34;score&#34;: self.get_score(),
            &#34;failures&#34;: self.get_failures(),
        }</code></pre>
</details>
<div class="desc"><p>Quality metrics for evaluating plot outline content.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>has_clear_setting</code></strong></dt>
<dd>Whether the plot has a clearly defined setting (yes/no)</dd>
<dt><strong><code>branching_paths_count</code></strong></dt>
<dd>Number of branching paths in the plot (count)</dd>
<dt><strong><code>endings_count</code></strong></dt>
<dd>Number of defined endings (count)</dd>
<dt><strong><code>themes_defined</code></strong></dt>
<dd>Whether themes are clearly stated (yes/no)</dd>
<dt><strong><code>word_count</code></strong></dt>
<dd>Total word count of the plot outline (number)</dd>
<dt><strong><code>has_title</code></strong></dt>
<dd>Whether the plot has a title (yes/no)</dd>
<dt><strong><code>has_prologue</code></strong></dt>
<dd>Whether the plot includes a prologue (yes/no)</dd>
<dt><strong><code>has_acts</code></strong></dt>
<dd>Whether the plot is structured into acts (yes/no)</dd>
<dt><strong><code>min_branching_paths</code></strong></dt>
<dd>Minimum required branching paths (default: 2)</dd>
<dt><strong><code>min_endings</code></strong></dt>
<dd>Minimum required endings (default: 2)</dd>
<dt><strong><code>min_word_count</code></strong></dt>
<dd>Minimum required word count (default: 500)</dd>
</dl></div>
<h3>Static methods</h3>
<dl>
<dt id="space_hulk_game.quality.PlotMetrics.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>data: dict[str, typing.Any]) ‑> <a title="space_hulk_game.quality.plot_metrics.PlotMetrics" href="plot_metrics.html#space_hulk_game.quality.plot_metrics.PlotMetrics">PlotMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create PlotMetrics from a dictionary (parsed YAML).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>Dictionary containing plot outline data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>PlotMetrics instance with measured values</p></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.from_yaml_content"><code class="name flex">
<span>def <span class="ident">from_yaml_content</span></span>(<span>yaml_content: str) ‑> <a title="space_hulk_game.quality.plot_metrics.PlotMetrics" href="plot_metrics.html#space_hulk_game.quality.plot_metrics.PlotMetrics">PlotMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create PlotMetrics from YAML content string.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yaml_content</code></strong></dt>
<dd>YAML string containing plot outline data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>PlotMetrics instance with measured values</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; metrics = PlotMetrics.from_yaml_content(yaml_string)
&gt;&gt;&gt; print(metrics.passes_threshold())
</code></pre></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="space_hulk_game.quality.PlotMetrics.branching_paths_count"><code class="name">var <span class="ident">branching_paths_count</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.endings_count"><code class="name">var <span class="ident">endings_count</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.has_acts"><code class="name">var <span class="ident">has_acts</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.has_clear_setting"><code class="name">var <span class="ident">has_clear_setting</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.has_prologue"><code class="name">var <span class="ident">has_prologue</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.has_title"><code class="name">var <span class="ident">has_title</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.min_branching_paths"><code class="name">var <span class="ident">min_branching_paths</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.min_endings"><code class="name">var <span class="ident">min_endings</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.min_word_count"><code class="name">var <span class="ident">min_word_count</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.themes_defined"><code class="name">var <span class="ident">themes_defined</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.word_count"><code class="name">var <span class="ident">word_count</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.PlotMetrics.get_failures"><code class="name flex">
<span>def <span class="ident">get_failures</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_failures(self) -&gt; list[str]:
    &#34;&#34;&#34;
    Get list of failed quality checks.

    Returns:
        List of failure messages for metrics that don&#39;t meet thresholds
    &#34;&#34;&#34;
    failures = []

    if not self.has_clear_setting:
        failures.append(&#34;Plot lacks a clear setting description&#34;)

    if self.branching_paths_count &lt; self.min_branching_paths:
        failures.append(
            f&#34;Insufficient branching paths: {self.branching_paths_count} &#34;
            f&#34;(minimum: {self.min_branching_paths})&#34;
        )

    if self.endings_count &lt; self.min_endings:
        failures.append(
            f&#34;Insufficient endings: {self.endings_count} &#34; f&#34;(minimum: {self.min_endings})&#34;
        )

    if not self.themes_defined:
        failures.append(&#34;Themes are not clearly defined&#34;)

    if self.word_count &lt; self.min_word_count:
        failures.append(
            f&#34;Word count too low: {self.word_count} &#34; f&#34;(minimum: {self.min_word_count})&#34;
        )

    return failures</code></pre>
</details>
<div class="desc"><p>Get list of failed quality checks.</p>
<h2 id="returns">Returns</h2>
<p>List of failure messages for metrics that don't meet thresholds</p></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.get_score"><code class="name flex">
<span>def <span class="ident">get_score</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_score(self) -&gt; float:
    &#34;&#34;&#34;
    Calculate overall quality score (0.0 to 10.0).

    Returns:
        Quality score based on met criteria
    &#34;&#34;&#34;
    total_checks = 5  # Number of main quality checks
    passed_checks = 0

    if self.has_clear_setting:
        passed_checks += 1
    if self.branching_paths_count &gt;= self.min_branching_paths:
        passed_checks += 1
    if self.endings_count &gt;= self.min_endings:
        passed_checks += 1
    if self.themes_defined:
        passed_checks += 1
    if self.word_count &gt;= self.min_word_count:
        passed_checks += 1

    return (passed_checks / total_checks) * 10.0</code></pre>
</details>
<div class="desc"><p>Calculate overall quality score (0.0 to 10.0).</p>
<h2 id="returns">Returns</h2>
<p>Quality score based on met criteria</p></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.passes_threshold"><code class="name flex">
<span>def <span class="ident">passes_threshold</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def passes_threshold(self) -&gt; bool:
    &#34;&#34;&#34;
    Check if the plot metrics pass all quality thresholds.

    Returns:
        True if all thresholds are met, False otherwise
    &#34;&#34;&#34;
    return (
        self.has_clear_setting
        and self.branching_paths_count &gt;= self.min_branching_paths
        and self.endings_count &gt;= self.min_endings
        and self.themes_defined
        and self.word_count &gt;= self.min_word_count
    )</code></pre>
</details>
<div class="desc"><p>Check if the plot metrics pass all quality thresholds.</p>
<h2 id="returns">Returns</h2>
<p>True if all thresholds are met, False otherwise</p></div>
</dd>
<dt id="space_hulk_game.quality.PlotMetrics.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Convert metrics to dictionary for serialization.

    Returns:
        Dictionary representation of metrics
    &#34;&#34;&#34;
    return {
        &#34;has_clear_setting&#34;: self.has_clear_setting,
        &#34;branching_paths_count&#34;: self.branching_paths_count,
        &#34;endings_count&#34;: self.endings_count,
        &#34;themes_defined&#34;: self.themes_defined,
        &#34;word_count&#34;: self.word_count,
        &#34;has_title&#34;: self.has_title,
        &#34;has_prologue&#34;: self.has_prologue,
        &#34;has_acts&#34;: self.has_acts,
        &#34;passes_threshold&#34;: self.passes_threshold(),
        &#34;score&#34;: self.get_score(),
        &#34;failures&#34;: self.get_failures(),
    }</code></pre>
</details>
<div class="desc"><p>Convert metrics to dictionary for serialization.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary representation of metrics</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.PuzzleEvaluator"><code class="flex name class">
<span>class <span class="ident">PuzzleEvaluator</span></span>
<span>(</span><span>pass_threshold: float = 6.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PuzzleEvaluator(QualityEvaluator):
    &#34;&#34;&#34;
    Evaluator for puzzle design content.

    Uses PuzzleMetrics to evaluate puzzle designs against quality criteria
    including clear solutions, narrative ties, and difficulty ratings.

    Example:
        &gt;&gt;&gt; evaluator = PuzzleEvaluator()
        &gt;&gt;&gt; score = evaluator.evaluate(puzzle_yaml_content)
        &gt;&gt;&gt; print(f&#34;Score: {score.score}/10&#34;)
        &gt;&gt;&gt; print(score.feedback)
    &#34;&#34;&#34;

    def __init__(self, pass_threshold: float = 6.0):
        &#34;&#34;&#34;
        Initialize the puzzle evaluator.

        Args:
            pass_threshold: Minimum score required to pass (default: 6.0)
        &#34;&#34;&#34;
        super().__init__(pass_threshold)
        logger.info(f&#34;PuzzleEvaluator initialized with threshold {pass_threshold}&#34;)

    def evaluate(self, content: str) -&gt; QualityScore:
        &#34;&#34;&#34;
        Evaluate puzzle design content.

        Args:
            content: YAML string containing puzzle design

        Returns:
            QualityScore with evaluation results

        Raises:
            ValueError: If content cannot be parsed
        &#34;&#34;&#34;
        try:
            # Parse YAML content using metrics class
            metrics = PuzzleMetrics.from_yaml_content(content)

            # Get score and check if passes threshold
            score = metrics.get_score()
            passed = score &gt;= self.pass_threshold

            # Get failures for detailed feedback
            failures = metrics.get_failures()

            # Build feedback message
            feedback = self._build_feedback(score, failures)

            # Build details dictionary
            details = {
                &#34;total_puzzles&#34;: metrics.total_puzzles,
                &#34;puzzles_with_solutions&#34;: metrics.puzzles_with_solutions,
                &#34;puzzles_with_narrative_ties&#34;: metrics.puzzles_with_narrative_ties,
                &#34;puzzles_with_difficulty&#34;: metrics.puzzles_with_difficulty,
                &#34;has_artifacts&#34;: metrics.has_artifacts,
                &#34;has_monsters&#34;: metrics.has_monsters,
                &#34;has_npcs&#34;: metrics.has_npcs,
                &#34;puzzles_without_solutions&#34;: [],  # Can be computed if needed
                &#34;puzzles_without_narrative_tie&#34;: [],
                &#34;puzzles_without_difficulty&#34;: [],
                &#34;failures&#34;: failures,
                &#34;threshold&#34;: self.pass_threshold,
            }

            logger.info(
                f&#34;Puzzle evaluation complete: score={score:.1f}, passed={passed}, &#34;
                f&#34;puzzles={metrics.total_puzzles}, with_solutions={metrics.puzzles_with_solutions}&#34;
            )

            return self._create_score(score, passed, feedback, details)

        except ValueError as e:
            logger.error(f&#34;Failed to evaluate puzzle content: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Failed to parse puzzle content: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )
        except Exception as e:
            logger.exception(f&#34;Unexpected error evaluating puzzle: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )

    def generate_detailed_feedback(self, content: str) -&gt; str:
        &#34;&#34;&#34;
        Generate detailed, actionable feedback for improvement.

        Args:
            content: YAML string containing puzzle design

        Returns:
            Multi-line feedback with specific suggestions
        &#34;&#34;&#34;
        result = self.evaluate(content)

        lines = [
            f&#34;Puzzle Design Quality Score: {result.score:.1f}/10.0&#34;,
            f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
            &#34;&#34;,
        ]

        if result.passed:
            lines.append(&#34;The puzzle design meets all quality requirements!&#34;)
        else:
            lines.append(&#34;The puzzle design needs improvement:&#34;)

        lines.append(&#34;&#34;)

        # Add specific findings
        details = result.details
        failures = details.get(&#34;failures&#34;, [])

        if failures:
            lines.append(&#34;Issues to address:&#34;)
            for failure in failures:
                lines.append(f&#34;  • {failure}&#34;)
            lines.append(&#34;&#34;)

        # Add content statistics
        lines.append(f&#34;Puzzle count: {details.get(&#39;total_puzzles&#39;, 0)} (min: 2)&#34;)
        lines.append(f&#34;Puzzles with solutions: {details.get(&#39;puzzles_with_solutions&#39;, 0)}&#34;)
        lines.append(
            f&#34;Puzzles with narrative ties: {details.get(&#39;puzzles_with_narrative_ties&#39;, 0)}&#34;
        )
        lines.append(f&#34;Puzzles with difficulty: {details.get(&#39;puzzles_with_difficulty&#39;, 0)}&#34;)

        # Add positive feedback
        lines.append(&#34;&#34;)
        if details.get(&#34;has_artifacts&#34;):
            lines.append(&#34;✓ Artifacts defined&#34;)
        if details.get(&#34;has_monsters&#34;):
            lines.append(&#34;✓ Monsters/enemies defined&#34;)
        if details.get(&#34;has_npcs&#34;):
            lines.append(&#34;✓ NPCs defined&#34;)

        total = details.get(&#34;total_puzzles&#34;, 0)
        if total &gt; 0:
            if details.get(&#34;puzzles_with_solutions&#34;, 0) == total:
                lines.append(&#34;✓ All puzzles have clear solutions&#34;)
            if details.get(&#34;puzzles_with_narrative_ties&#34;, 0) == total:
                lines.append(&#34;✓ All puzzles tied to narrative&#34;)
            if details.get(&#34;puzzles_with_difficulty&#34;, 0) == total:
                lines.append(&#34;✓ All puzzles have difficulty ratings&#34;)

        return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Evaluator for puzzle design content.</p>
<p>Uses PuzzleMetrics to evaluate puzzle designs against quality criteria
including clear solutions, narrative ties, and difficulty ratings.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; evaluator = PuzzleEvaluator()
&gt;&gt;&gt; score = evaluator.evaluate(puzzle_yaml_content)
&gt;&gt;&gt; print(f&quot;Score: {score.score}/10&quot;)
&gt;&gt;&gt; print(score.feedback)
</code></pre>
<p>Initialize the puzzle evaluator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum score required to pass (default: 6.0)</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.PuzzleEvaluator.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, content: str) ‑> <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, content: str) -&gt; QualityScore:
    &#34;&#34;&#34;
    Evaluate puzzle design content.

    Args:
        content: YAML string containing puzzle design

    Returns:
        QualityScore with evaluation results

    Raises:
        ValueError: If content cannot be parsed
    &#34;&#34;&#34;
    try:
        # Parse YAML content using metrics class
        metrics = PuzzleMetrics.from_yaml_content(content)

        # Get score and check if passes threshold
        score = metrics.get_score()
        passed = score &gt;= self.pass_threshold

        # Get failures for detailed feedback
        failures = metrics.get_failures()

        # Build feedback message
        feedback = self._build_feedback(score, failures)

        # Build details dictionary
        details = {
            &#34;total_puzzles&#34;: metrics.total_puzzles,
            &#34;puzzles_with_solutions&#34;: metrics.puzzles_with_solutions,
            &#34;puzzles_with_narrative_ties&#34;: metrics.puzzles_with_narrative_ties,
            &#34;puzzles_with_difficulty&#34;: metrics.puzzles_with_difficulty,
            &#34;has_artifacts&#34;: metrics.has_artifacts,
            &#34;has_monsters&#34;: metrics.has_monsters,
            &#34;has_npcs&#34;: metrics.has_npcs,
            &#34;puzzles_without_solutions&#34;: [],  # Can be computed if needed
            &#34;puzzles_without_narrative_tie&#34;: [],
            &#34;puzzles_without_difficulty&#34;: [],
            &#34;failures&#34;: failures,
            &#34;threshold&#34;: self.pass_threshold,
        }

        logger.info(
            f&#34;Puzzle evaluation complete: score={score:.1f}, passed={passed}, &#34;
            f&#34;puzzles={metrics.total_puzzles}, with_solutions={metrics.puzzles_with_solutions}&#34;
        )

        return self._create_score(score, passed, feedback, details)

    except ValueError as e:
        logger.error(f&#34;Failed to evaluate puzzle content: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Failed to parse puzzle content: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )
    except Exception as e:
        logger.exception(f&#34;Unexpected error evaluating puzzle: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )</code></pre>
</details>
<div class="desc"><p>Evaluate puzzle design content.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing puzzle design</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>QualityScore with evaluation results</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If content cannot be parsed</dd>
</dl></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleEvaluator.generate_detailed_feedback"><code class="name flex">
<span>def <span class="ident">generate_detailed_feedback</span></span>(<span>self, content: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_detailed_feedback(self, content: str) -&gt; str:
    &#34;&#34;&#34;
    Generate detailed, actionable feedback for improvement.

    Args:
        content: YAML string containing puzzle design

    Returns:
        Multi-line feedback with specific suggestions
    &#34;&#34;&#34;
    result = self.evaluate(content)

    lines = [
        f&#34;Puzzle Design Quality Score: {result.score:.1f}/10.0&#34;,
        f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
        &#34;&#34;,
    ]

    if result.passed:
        lines.append(&#34;The puzzle design meets all quality requirements!&#34;)
    else:
        lines.append(&#34;The puzzle design needs improvement:&#34;)

    lines.append(&#34;&#34;)

    # Add specific findings
    details = result.details
    failures = details.get(&#34;failures&#34;, [])

    if failures:
        lines.append(&#34;Issues to address:&#34;)
        for failure in failures:
            lines.append(f&#34;  • {failure}&#34;)
        lines.append(&#34;&#34;)

    # Add content statistics
    lines.append(f&#34;Puzzle count: {details.get(&#39;total_puzzles&#39;, 0)} (min: 2)&#34;)
    lines.append(f&#34;Puzzles with solutions: {details.get(&#39;puzzles_with_solutions&#39;, 0)}&#34;)
    lines.append(
        f&#34;Puzzles with narrative ties: {details.get(&#39;puzzles_with_narrative_ties&#39;, 0)}&#34;
    )
    lines.append(f&#34;Puzzles with difficulty: {details.get(&#39;puzzles_with_difficulty&#39;, 0)}&#34;)

    # Add positive feedback
    lines.append(&#34;&#34;)
    if details.get(&#34;has_artifacts&#34;):
        lines.append(&#34;✓ Artifacts defined&#34;)
    if details.get(&#34;has_monsters&#34;):
        lines.append(&#34;✓ Monsters/enemies defined&#34;)
    if details.get(&#34;has_npcs&#34;):
        lines.append(&#34;✓ NPCs defined&#34;)

    total = details.get(&#34;total_puzzles&#34;, 0)
    if total &gt; 0:
        if details.get(&#34;puzzles_with_solutions&#34;, 0) == total:
            lines.append(&#34;✓ All puzzles have clear solutions&#34;)
        if details.get(&#34;puzzles_with_narrative_ties&#34;, 0) == total:
            lines.append(&#34;✓ All puzzles tied to narrative&#34;)
        if details.get(&#34;puzzles_with_difficulty&#34;, 0) == total:
            lines.append(&#34;✓ All puzzles have difficulty ratings&#34;)

    return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Generate detailed, actionable feedback for improvement.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing puzzle design</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Multi-line feedback with specific suggestions</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback">generate_feedback</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml">parse_yaml</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.score" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics"><code class="flex name class">
<span>class <span class="ident">PuzzleMetrics</span></span>
<span>(</span><span>total_puzzles: int = 0,<br>puzzles_with_solutions: int = 0,<br>puzzles_with_narrative_ties: int = 0,<br>puzzles_with_difficulty: int = 0,<br>has_artifacts: bool = False,<br>has_monsters: bool = False,<br>has_npcs: bool = False,<br>min_puzzles: int = 2,<br>min_solution_percentage: float = 80.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class PuzzleMetrics:
    &#34;&#34;&#34;
    Quality metrics for evaluating puzzle and artifact content.

    Attributes:
        total_puzzles: Total number of puzzles defined
        puzzles_with_solutions: Number of puzzles with clear solutions
        puzzles_with_narrative_ties: Number of puzzles tied to narrative
        puzzles_with_difficulty: Number of puzzles with difficulty ratings
        has_artifacts: Whether artifacts are defined
        has_monsters: Whether monsters/enemies are defined
        has_npcs: Whether NPCs are defined
        min_puzzles: Minimum required puzzles (default: 2)
    &#34;&#34;&#34;

    # Measured values
    total_puzzles: int = 0
    puzzles_with_solutions: int = 0
    puzzles_with_narrative_ties: int = 0
    puzzles_with_difficulty: int = 0
    has_artifacts: bool = False
    has_monsters: bool = False
    has_npcs: bool = False

    # Thresholds
    min_puzzles: int = 2
    min_solution_percentage: float = 80.0  # 80% should have clear solutions

    @classmethod
    def from_yaml_content(cls, yaml_content: str) -&gt; &#34;PuzzleMetrics&#34;:
        &#34;&#34;&#34;
        Create PuzzleMetrics from YAML content string.

        Args:
            yaml_content: YAML string containing puzzle design data

        Returns:
            PuzzleMetrics instance with measured values
        &#34;&#34;&#34;
        try:
            # Handle markdown-wrapped YAML
            content = yaml_content.strip()
            if content.startswith(&#34;```&#34;):
                lines = content.split(&#34;\n&#34;)
                content = &#34;\n&#34;.join(lines[1:-1])

            data = yaml.safe_load(content)
            return cls.from_dict(data)
        except Exception as e:
            raise ValueError(f&#34;Failed to parse YAML content: {e}&#34;) from e

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -&gt; &#34;PuzzleMetrics&#34;:
        &#34;&#34;&#34;
        Create PuzzleMetrics from a dictionary (parsed YAML).

        Args:
            data: Dictionary containing puzzle design data

        Returns:
            PuzzleMetrics instance with measured values
        &#34;&#34;&#34;
        metrics = cls()

        # Check for artifacts
        artifacts = data.get(&#34;artifacts&#34;, [])
        metrics.has_artifacts = bool(artifacts) and len(artifacts) &gt; 0

        # Check for monsters
        monsters = data.get(&#34;monsters&#34;, [])
        metrics.has_monsters = bool(monsters) and len(monsters) &gt; 0

        # Check for NPCs
        npcs = data.get(&#34;npcs&#34;, [])
        metrics.has_npcs = bool(npcs) and len(npcs) &gt; 0

        # Analyze puzzles
        puzzles = data.get(&#34;puzzles&#34;, [])
        if not puzzles:
            # Try alternate key
            puzzles = data.get(&#34;puzzle_design&#34;, {}).get(&#34;puzzles&#34;, [])

        metrics.total_puzzles = len(puzzles) if isinstance(puzzles, list) else 0

        if isinstance(puzzles, list):
            for puzzle in puzzles:
                if not isinstance(puzzle, dict):
                    continue

                # Check for solution
                if cls._has_clear_solution(puzzle):
                    metrics.puzzles_with_solutions += 1

                # Check for narrative tie
                if cls._has_narrative_tie(puzzle):
                    metrics.puzzles_with_narrative_ties += 1

                # Check for difficulty
                if cls._has_difficulty(puzzle):
                    metrics.puzzles_with_difficulty += 1

        return metrics

    @staticmethod
    def _has_clear_solution(puzzle: dict[str, Any]) -&gt; bool:
        &#34;&#34;&#34;
        Check if puzzle has a clear solution described.

        Args:
            puzzle: Puzzle dictionary

        Returns:
            True if solution is clearly described
        &#34;&#34;&#34;
        # Check for explicit solution field
        solution = puzzle.get(&#34;solution&#34;)
        if solution:
            if isinstance(solution, str):
                return len(solution.strip()) &gt;= 20
            return True

        # Check for how_to_solve or similar fields
        how_to_solve = puzzle.get(&#34;how_to_solve&#34;)
        if how_to_solve:
            if isinstance(how_to_solve, str):
                return len(how_to_solve.strip()) &gt;= 20
            return True

        # Check description for solution hints
        description = puzzle.get(&#34;description&#34;, &#34;&#34;)
        if isinstance(description, str):
            # Look for solution-related keywords
            solution_keywords = [&#34;solve&#34;, &#34;answer&#34;, &#34;key&#34;, &#34;unlock&#34;, &#34;activate&#34;]
            return any(keyword in description.lower() for keyword in solution_keywords)

        return False

    @staticmethod
    def _has_narrative_tie(puzzle: dict[str, Any]) -&gt; bool:
        &#34;&#34;&#34;
        Check if puzzle is tied to the narrative.

        Args:
            puzzle: Puzzle dictionary

        Returns:
            True if puzzle has narrative integration
        &#34;&#34;&#34;
        # Check for explicit narrative fields
        narrative_fields = [&#34;narrative_tie&#34;, &#34;story_connection&#34;, &#34;lore&#34;, &#34;backstory&#34;]
        for field in narrative_fields:
            if puzzle.get(field):
                return True

        # Check description for narrative elements
        description = puzzle.get(&#34;description&#34;, &#34;&#34;)
        if isinstance(description, str) and len(description) &gt;= 30:
            # Descriptions with some detail likely include narrative context
            return True

        # Check for location or scene references
        if puzzle.get(&#34;location&#34;) or puzzle.get(&#34;scene&#34;):
            return True

        return False

    @staticmethod
    def _has_difficulty(puzzle: dict[str, Any]) -&gt; bool:
        &#34;&#34;&#34;
        Check if puzzle has difficulty rating.

        Args:
            puzzle: Puzzle dictionary

        Returns:
            True if difficulty is stated
        &#34;&#34;&#34;
        difficulty_fields = [&#34;difficulty&#34;, &#34;challenge_level&#34;, &#34;complexity&#34;]
        for field in difficulty_fields:
            if puzzle.get(field):
                return True

        return False

    def passes_threshold(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if the puzzle metrics pass all quality thresholds.

        Returns:
            True if all thresholds are met, False otherwise
        &#34;&#34;&#34;
        solution_percentage = 0.0
        if self.total_puzzles &gt; 0:
            solution_percentage = (self.puzzles_with_solutions / self.total_puzzles) * 100.0

        return (
            self.total_puzzles &gt;= self.min_puzzles
            and solution_percentage &gt;= self.min_solution_percentage
        )

    def get_failures(self) -&gt; list[str]:
        &#34;&#34;&#34;
        Get list of failed quality checks.

        Returns:
            List of failure messages for metrics that don&#39;t meet thresholds
        &#34;&#34;&#34;
        failures = []

        if self.total_puzzles &lt; self.min_puzzles:
            failures.append(
                f&#34;Insufficient puzzles: {self.total_puzzles} &#34; f&#34;(minimum: {self.min_puzzles})&#34;
            )

        solution_percentage = 0.0
        if self.total_puzzles &gt; 0:
            solution_percentage = (self.puzzles_with_solutions / self.total_puzzles) * 100.0

        if solution_percentage &lt; self.min_solution_percentage:
            failures.append(
                f&#34;Too few puzzles with clear solutions: {solution_percentage:.1f}% &#34;
                f&#34;(minimum: {self.min_solution_percentage}%)&#34;
            )

        # Warnings (not failures)
        if self.total_puzzles &gt; 0:
            narrative_percentage = (self.puzzles_with_narrative_ties / self.total_puzzles) * 100.0
            if narrative_percentage &lt; 50.0:
                failures.append(
                    f&#34;Warning: Only {narrative_percentage:.1f}% of puzzles &#34; f&#34;have narrative ties&#34;
                )

        return failures

    def get_score(self) -&gt; float:
        &#34;&#34;&#34;
        Calculate overall quality score (0.0 to 10.0).

        Returns:
            Quality score based on met criteria
        &#34;&#34;&#34;
        score = 0.0

        # Minimum puzzles (3 points)
        if self.total_puzzles &gt;= self.min_puzzles:
            score += 3.0
        elif self.total_puzzles &gt; 0:
            score += 3.0 * (self.total_puzzles / self.min_puzzles)

        # Solutions (3 points)
        if self.total_puzzles &gt; 0:
            solution_percentage = (self.puzzles_with_solutions / self.total_puzzles) * 100.0
            score += 3.0 * (solution_percentage / 100.0)

        # Narrative ties (2 points)
        if self.total_puzzles &gt; 0:
            narrative_percentage = (self.puzzles_with_narrative_ties / self.total_puzzles) * 100.0
            score += 2.0 * (narrative_percentage / 100.0)

        # Difficulty ratings (1 point)
        if self.total_puzzles &gt; 0:
            difficulty_percentage = (self.puzzles_with_difficulty / self.total_puzzles) * 100.0
            score += 1.0 * (difficulty_percentage / 100.0)

        # Bonus for having artifacts, monsters, NPCs (1 point)
        bonus_items = sum([self.has_artifacts, self.has_monsters, self.has_npcs])
        score += 1.0 * (bonus_items / 3.0)

        return min(score, 10.0)

    def to_dict(self) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Convert metrics to dictionary for serialization.

        Returns:
            Dictionary representation of metrics
        &#34;&#34;&#34;
        return {
            &#34;total_puzzles&#34;: self.total_puzzles,
            &#34;puzzles_with_solutions&#34;: self.puzzles_with_solutions,
            &#34;puzzles_with_narrative_ties&#34;: self.puzzles_with_narrative_ties,
            &#34;puzzles_with_difficulty&#34;: self.puzzles_with_difficulty,
            &#34;has_artifacts&#34;: self.has_artifacts,
            &#34;has_monsters&#34;: self.has_monsters,
            &#34;has_npcs&#34;: self.has_npcs,
            &#34;passes_threshold&#34;: self.passes_threshold(),
            &#34;score&#34;: self.get_score(),
            &#34;failures&#34;: self.get_failures(),
        }</code></pre>
</details>
<div class="desc"><p>Quality metrics for evaluating puzzle and artifact content.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>total_puzzles</code></strong></dt>
<dd>Total number of puzzles defined</dd>
<dt><strong><code>puzzles_with_solutions</code></strong></dt>
<dd>Number of puzzles with clear solutions</dd>
<dt><strong><code>puzzles_with_narrative_ties</code></strong></dt>
<dd>Number of puzzles tied to narrative</dd>
<dt><strong><code>puzzles_with_difficulty</code></strong></dt>
<dd>Number of puzzles with difficulty ratings</dd>
<dt><strong><code>has_artifacts</code></strong></dt>
<dd>Whether artifacts are defined</dd>
<dt><strong><code>has_monsters</code></strong></dt>
<dd>Whether monsters/enemies are defined</dd>
<dt><strong><code>has_npcs</code></strong></dt>
<dd>Whether NPCs are defined</dd>
<dt><strong><code>min_puzzles</code></strong></dt>
<dd>Minimum required puzzles (default: 2)</dd>
</dl></div>
<h3>Static methods</h3>
<dl>
<dt id="space_hulk_game.quality.PuzzleMetrics.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>data: dict[str, typing.Any]) ‑> <a title="space_hulk_game.quality.puzzle_metrics.PuzzleMetrics" href="puzzle_metrics.html#space_hulk_game.quality.puzzle_metrics.PuzzleMetrics">PuzzleMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create PuzzleMetrics from a dictionary (parsed YAML).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>Dictionary containing puzzle design data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>PuzzleMetrics instance with measured values</p></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.from_yaml_content"><code class="name flex">
<span>def <span class="ident">from_yaml_content</span></span>(<span>yaml_content: str) ‑> <a title="space_hulk_game.quality.puzzle_metrics.PuzzleMetrics" href="puzzle_metrics.html#space_hulk_game.quality.puzzle_metrics.PuzzleMetrics">PuzzleMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create PuzzleMetrics from YAML content string.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yaml_content</code></strong></dt>
<dd>YAML string containing puzzle design data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>PuzzleMetrics instance with measured values</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="space_hulk_game.quality.PuzzleMetrics.has_artifacts"><code class="name">var <span class="ident">has_artifacts</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.has_monsters"><code class="name">var <span class="ident">has_monsters</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.has_npcs"><code class="name">var <span class="ident">has_npcs</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.min_puzzles"><code class="name">var <span class="ident">min_puzzles</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.min_solution_percentage"><code class="name">var <span class="ident">min_solution_percentage</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.puzzles_with_difficulty"><code class="name">var <span class="ident">puzzles_with_difficulty</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.puzzles_with_narrative_ties"><code class="name">var <span class="ident">puzzles_with_narrative_ties</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.puzzles_with_solutions"><code class="name">var <span class="ident">puzzles_with_solutions</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.total_puzzles"><code class="name">var <span class="ident">total_puzzles</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.PuzzleMetrics.get_failures"><code class="name flex">
<span>def <span class="ident">get_failures</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_failures(self) -&gt; list[str]:
    &#34;&#34;&#34;
    Get list of failed quality checks.

    Returns:
        List of failure messages for metrics that don&#39;t meet thresholds
    &#34;&#34;&#34;
    failures = []

    if self.total_puzzles &lt; self.min_puzzles:
        failures.append(
            f&#34;Insufficient puzzles: {self.total_puzzles} &#34; f&#34;(minimum: {self.min_puzzles})&#34;
        )

    solution_percentage = 0.0
    if self.total_puzzles &gt; 0:
        solution_percentage = (self.puzzles_with_solutions / self.total_puzzles) * 100.0

    if solution_percentage &lt; self.min_solution_percentage:
        failures.append(
            f&#34;Too few puzzles with clear solutions: {solution_percentage:.1f}% &#34;
            f&#34;(minimum: {self.min_solution_percentage}%)&#34;
        )

    # Warnings (not failures)
    if self.total_puzzles &gt; 0:
        narrative_percentage = (self.puzzles_with_narrative_ties / self.total_puzzles) * 100.0
        if narrative_percentage &lt; 50.0:
            failures.append(
                f&#34;Warning: Only {narrative_percentage:.1f}% of puzzles &#34; f&#34;have narrative ties&#34;
            )

    return failures</code></pre>
</details>
<div class="desc"><p>Get list of failed quality checks.</p>
<h2 id="returns">Returns</h2>
<p>List of failure messages for metrics that don't meet thresholds</p></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.get_score"><code class="name flex">
<span>def <span class="ident">get_score</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_score(self) -&gt; float:
    &#34;&#34;&#34;
    Calculate overall quality score (0.0 to 10.0).

    Returns:
        Quality score based on met criteria
    &#34;&#34;&#34;
    score = 0.0

    # Minimum puzzles (3 points)
    if self.total_puzzles &gt;= self.min_puzzles:
        score += 3.0
    elif self.total_puzzles &gt; 0:
        score += 3.0 * (self.total_puzzles / self.min_puzzles)

    # Solutions (3 points)
    if self.total_puzzles &gt; 0:
        solution_percentage = (self.puzzles_with_solutions / self.total_puzzles) * 100.0
        score += 3.0 * (solution_percentage / 100.0)

    # Narrative ties (2 points)
    if self.total_puzzles &gt; 0:
        narrative_percentage = (self.puzzles_with_narrative_ties / self.total_puzzles) * 100.0
        score += 2.0 * (narrative_percentage / 100.0)

    # Difficulty ratings (1 point)
    if self.total_puzzles &gt; 0:
        difficulty_percentage = (self.puzzles_with_difficulty / self.total_puzzles) * 100.0
        score += 1.0 * (difficulty_percentage / 100.0)

    # Bonus for having artifacts, monsters, NPCs (1 point)
    bonus_items = sum([self.has_artifacts, self.has_monsters, self.has_npcs])
    score += 1.0 * (bonus_items / 3.0)

    return min(score, 10.0)</code></pre>
</details>
<div class="desc"><p>Calculate overall quality score (0.0 to 10.0).</p>
<h2 id="returns">Returns</h2>
<p>Quality score based on met criteria</p></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.passes_threshold"><code class="name flex">
<span>def <span class="ident">passes_threshold</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def passes_threshold(self) -&gt; bool:
    &#34;&#34;&#34;
    Check if the puzzle metrics pass all quality thresholds.

    Returns:
        True if all thresholds are met, False otherwise
    &#34;&#34;&#34;
    solution_percentage = 0.0
    if self.total_puzzles &gt; 0:
        solution_percentage = (self.puzzles_with_solutions / self.total_puzzles) * 100.0

    return (
        self.total_puzzles &gt;= self.min_puzzles
        and solution_percentage &gt;= self.min_solution_percentage
    )</code></pre>
</details>
<div class="desc"><p>Check if the puzzle metrics pass all quality thresholds.</p>
<h2 id="returns">Returns</h2>
<p>True if all thresholds are met, False otherwise</p></div>
</dd>
<dt id="space_hulk_game.quality.PuzzleMetrics.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Convert metrics to dictionary for serialization.

    Returns:
        Dictionary representation of metrics
    &#34;&#34;&#34;
    return {
        &#34;total_puzzles&#34;: self.total_puzzles,
        &#34;puzzles_with_solutions&#34;: self.puzzles_with_solutions,
        &#34;puzzles_with_narrative_ties&#34;: self.puzzles_with_narrative_ties,
        &#34;puzzles_with_difficulty&#34;: self.puzzles_with_difficulty,
        &#34;has_artifacts&#34;: self.has_artifacts,
        &#34;has_monsters&#34;: self.has_monsters,
        &#34;has_npcs&#34;: self.has_npcs,
        &#34;passes_threshold&#34;: self.passes_threshold(),
        &#34;score&#34;: self.get_score(),
        &#34;failures&#34;: self.get_failures(),
    }</code></pre>
</details>
<div class="desc"><p>Convert metrics to dictionary for serialization.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary representation of metrics</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.QualityCheckConfig"><code class="flex name class">
<span>class <span class="ident">QualityCheckConfig</span></span>
<span>(</span><span>config_path: str | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QualityCheckConfig:
    &#34;&#34;&#34;
    Configuration for quality checking system.

    Loads configuration from quality_config.yaml and environment variables.
    &#34;&#34;&#34;

    def __init__(self, config_path: str | None = None):
        &#34;&#34;&#34;
        Initialize quality check configuration.

        Args:
            config_path: Path to quality_config.yaml (optional)
        &#34;&#34;&#34;
        self.config = self._load_config(config_path)
        self.task_configs = self._create_task_configs()

    def _load_config(self, config_path: str | Path | None = None) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Load configuration from YAML file.

        Args:
            config_path: Path to config file (optional)

        Returns:
            Configuration dictionary
        &#34;&#34;&#34;
        if config_path is None:
            # Default path relative to this module
            module_dir = Path(__file__).parent.parent
            config_path = module_dir / &#34;config&#34; / &#34;quality_config.yaml&#34;

        try:
            with open(config_path) as f:
                config = yaml.safe_load(f)
            logger.info(f&#34;Loaded quality configuration from {config_path}&#34;)
            return config
        except FileNotFoundError:
            logger.warning(f&#34;Quality config file not found: {config_path}, using defaults&#34;)
            return self._default_config()
        except Exception as e:
            logger.error(f&#34;Error loading quality config: {e}, using defaults&#34;)
            return self._default_config()

    def _default_config(self) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Get default configuration.

        Returns:
            Default configuration dictionary
        &#34;&#34;&#34;
        return {
            &#34;global&#34;: {&#34;enabled&#34;: False, &#34;log_level&#34;: &#34;INFO&#34;, &#34;verbose_logging&#34;: True},
            &#34;thresholds&#34;: {
                &#34;plot&#34;: {&#34;enabled&#34;: True, &#34;pass_threshold&#34;: 6.0, &#34;max_retries&#34;: 3},
                &#34;narrative&#34;: {&#34;enabled&#34;: True, &#34;pass_threshold&#34;: 6.0, &#34;max_retries&#34;: 3},
                &#34;puzzle&#34;: {&#34;enabled&#34;: True, &#34;pass_threshold&#34;: 6.0, &#34;max_retries&#34;: 3},
                &#34;scene&#34;: {&#34;enabled&#34;: True, &#34;pass_threshold&#34;: 6.0, &#34;max_retries&#34;: 3},
                &#34;mechanics&#34;: {&#34;enabled&#34;: True, &#34;pass_threshold&#34;: 6.0, &#34;max_retries&#34;: 3},
            },
            &#34;retry&#34;: {&#34;provide_feedback&#34;: True, &#34;fail_on_evaluation_error&#34;: False},
        }

    def _create_task_configs(self) -&gt; dict[TaskType, dict[str, Any]]:
        &#34;&#34;&#34;
        Create task configuration dictionary from loaded config.

        Returns:
            Dictionary mapping task types to their configurations
        &#34;&#34;&#34;
        thresholds = self.config.get(&#34;thresholds&#34;, {})

        # Map YAML keys to TaskType enum
        task_map = {
            &#34;plot&#34;: TaskType.PLOT,
            &#34;narrative&#34;: TaskType.NARRATIVE,
            &#34;puzzle&#34;: TaskType.PUZZLE,
            &#34;scene&#34;: TaskType.SCENE,
            &#34;mechanics&#34;: TaskType.MECHANICS,
        }

        configs = {}
        for yaml_key, task_type in task_map.items():
            threshold_config = thresholds.get(yaml_key, {})
            configs[task_type] = {
                &#34;enabled&#34;: threshold_config.get(&#34;enabled&#34;, True),
                &#34;pass_threshold&#34;: self._get_env_override(
                    f&#34;QUALITY_{yaml_key.upper()}_THRESHOLD&#34;,
                    threshold_config.get(&#34;pass_threshold&#34;, 6.0),
                ),
                &#34;max_retries&#34;: self._get_env_override(
                    &#34;QUALITY_MAX_RETRIES&#34;, threshold_config.get(&#34;max_retries&#34;, 3)
                ),
            }

        return configs

    def _get_env_override(self, env_var: str, default: Any) -&gt; Any:
        &#34;&#34;&#34;
        Get environment variable override for configuration value.

        Args:
            env_var: Environment variable name
            default: Default value if not set

        Returns:
            Value from environment or default
        &#34;&#34;&#34;
        value = os.getenv(env_var)
        if value is None:
            return default

        # Try to convert to same type as default
        if isinstance(default, bool):
            return value.lower() in (&#34;true&#34;, &#34;1&#34;, &#34;yes&#34;)
        elif isinstance(default, int):
            return int(value)
        elif isinstance(default, float):
            return float(value)
        else:
            return value

    def is_enabled(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if quality checking is globally enabled.

        Returns:
            True if quality checking is enabled
        &#34;&#34;&#34;
        # Check environment variable first
        env_enabled = os.getenv(&#34;QUALITY_CHECK_ENABLED&#34;)
        if env_enabled is not None:
            return env_enabled.lower() in (&#34;true&#34;, &#34;1&#34;, &#34;yes&#34;)

        # Check config file
        return self.config.get(&#34;global&#34;, {}).get(&#34;enabled&#34;, False)

    def get_task_config(self, task_type: TaskType) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Get configuration for specific task type.

        Args:
            task_type: Type of task

        Returns:
            Configuration dictionary for the task
        &#34;&#34;&#34;
        return self.task_configs.get(
            task_type, {&#34;enabled&#34;: True, &#34;pass_threshold&#34;: 6.0, &#34;max_retries&#34;: 3}
        )</code></pre>
</details>
<div class="desc"><p>Configuration for quality checking system.</p>
<p>Loads configuration from quality_config.yaml and environment variables.</p>
<p>Initialize quality check configuration.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config_path</code></strong></dt>
<dd>Path to quality_config.yaml (optional)</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.QualityCheckConfig.get_task_config"><code class="name flex">
<span>def <span class="ident">get_task_config</span></span>(<span>self,<br>task_type: <a title="space_hulk_game.quality.retry.TaskType" href="retry.html#space_hulk_game.quality.retry.TaskType">TaskType</a>) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_task_config(self, task_type: TaskType) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Get configuration for specific task type.

    Args:
        task_type: Type of task

    Returns:
        Configuration dictionary for the task
    &#34;&#34;&#34;
    return self.task_configs.get(
        task_type, {&#34;enabled&#34;: True, &#34;pass_threshold&#34;: 6.0, &#34;max_retries&#34;: 3}
    )</code></pre>
</details>
<div class="desc"><p>Get configuration for specific task type.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task_type</code></strong></dt>
<dd>Type of task</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Configuration dictionary for the task</p></div>
</dd>
<dt id="space_hulk_game.quality.QualityCheckConfig.is_enabled"><code class="name flex">
<span>def <span class="ident">is_enabled</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_enabled(self) -&gt; bool:
    &#34;&#34;&#34;
    Check if quality checking is globally enabled.

    Returns:
        True if quality checking is enabled
    &#34;&#34;&#34;
    # Check environment variable first
    env_enabled = os.getenv(&#34;QUALITY_CHECK_ENABLED&#34;)
    if env_enabled is not None:
        return env_enabled.lower() in (&#34;true&#34;, &#34;1&#34;, &#34;yes&#34;)

    # Check config file
    return self.config.get(&#34;global&#34;, {}).get(&#34;enabled&#34;, False)</code></pre>
</details>
<div class="desc"><p>Check if quality checking is globally enabled.</p>
<h2 id="returns">Returns</h2>
<p>True if quality checking is enabled</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.QualityEvaluator"><code class="flex name class">
<span>class <span class="ident">QualityEvaluator</span></span>
<span>(</span><span>pass_threshold: float = 6.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QualityEvaluator(ABC):
    &#34;&#34;&#34;
    Abstract base class for all quality evaluators.

    All content-specific evaluators (Plot, Narrative, Puzzle, Scene, Mechanics)
    should inherit from this class and implement the required methods.

    Example:
        &gt;&gt;&gt; class PlotEvaluator(QualityEvaluator):
        ...     def evaluate(self, content: str) -&gt; QualityScore:
        ...         # Implementation here
        ...         pass
    &#34;&#34;&#34;

    def __init__(self, pass_threshold: float = 6.0):
        &#34;&#34;&#34;
        Initialize the evaluator.

        Args:
            pass_threshold: Minimum score required to pass (0.0-10.0)
        &#34;&#34;&#34;
        if not 0.0 &lt;= pass_threshold &lt;= 10.0:
            raise ValueError(f&#34;Pass threshold must be between 0.0 and 10.0, got {pass_threshold}&#34;)

        self.pass_threshold = pass_threshold
        logger.debug(f&#34;{self.__class__.__name__} initialized with threshold {pass_threshold}&#34;)

    @abstractmethod
    def evaluate(self, content: str) -&gt; QualityScore:
        &#34;&#34;&#34;
        Evaluate content and return quality score.

        Args:
            content: Content to evaluate (usually YAML string)

        Returns:
            QualityScore with evaluation results

        Raises:
            ValueError: If content cannot be parsed or evaluated
        &#34;&#34;&#34;
        pass

    def score(self, content: str) -&gt; float:
        &#34;&#34;&#34;
        Get numeric score for content (convenience method).

        Args:
            content: Content to evaluate

        Returns:
            Numeric score from 0.0 to 10.0
        &#34;&#34;&#34;
        result = self.evaluate(content)
        return result.score

    def generate_feedback(self, content: str) -&gt; str:
        &#34;&#34;&#34;
        Generate feedback message for content (convenience method).

        Args:
            content: Content to evaluate

        Returns:
            Human-readable feedback message
        &#34;&#34;&#34;
        result = self.evaluate(content)
        return result.feedback

    @staticmethod
    def _fix_common_yaml_errors(content: str) -&gt; str:
        &#34;&#34;&#34;
        Attempt to fix common YAML syntax errors.

        Args:
            content: YAML string that may have syntax errors

        Returns:
            Fixed YAML string
        &#34;&#34;&#34;
        lines = content.split(&#34;\n&#34;)
        fixed_lines = []

        for line in lines:
            # Fix unquoted values with colons (e.g., &#34;title: Space Hulk: Derelict&#34;)
            # Match pattern: &#34;key: value with : colon&#34; where value is not quoted
            if &#34;:&#34; in line and not line.strip().startswith(&#34;-&#34;):
                parts = line.split(&#34;:&#34;, 1)
                if len(parts) == 2:
                    key_part = parts[0]
                    value_part = parts[1].strip()

                    # If value has a colon and is not already quoted
                    if &#34;:&#34; in value_part and not (
                        (value_part.startswith(&#39;&#34;&#39;) and value_part.endswith(&#39;&#34;&#39;))
                        or (value_part.startswith(&#34;&#39;&#34;) and value_part.endswith(&#34;&#39;&#34;))
                    ):
                        # Quote the value
                        fixed_line = f&#39;{key_part}: &#34;{value_part}&#34;&#39;
                        fixed_lines.append(fixed_line)
                        logger.debug(f&#34;Fixed YAML line: {line.strip()} -&gt; {fixed_line}&#34;)
                        continue

            fixed_lines.append(line)

        return &#34;\n&#34;.join(fixed_lines)

    @staticmethod
    def parse_yaml(content: str) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Parse YAML content, handling markdown-wrapped YAML and common syntax errors.

        Args:
            content: YAML string, optionally wrapped in markdown code fences

        Returns:
            Parsed YAML as dictionary

        Raises:
            ValueError: If YAML cannot be parsed
        &#34;&#34;&#34;
        try:
            # Handle markdown-wrapped YAML
            content_stripped = content.strip()
            if content_stripped.startswith(&#34;```&#34;):
                lines = content_stripped.split(&#34;\n&#34;)
                # Remove first line (```yaml or ```) and last line (```)
                content_stripped = &#34;\n&#34;.join(lines[1:-1])
                logger.debug(&#34;Stripped markdown fences from YAML content&#34;)

            # Try to parse as-is first
            try:
                data = yaml.safe_load(content_stripped)
                if data is None:
                    raise ValueError(&#34;YAML content is empty or invalid&#34;)
                return data
            except yaml.YAMLError as parse_error:
                # Attempt to fix common YAML syntax errors and retry
                logger.debug(
                    f&#34;Initial YAML parse failed, attempting to fix common errors: {parse_error}&#34;
                )
                fixed_content = QualityEvaluator._fix_common_yaml_errors(content_stripped)
                data = yaml.safe_load(fixed_content)
                if data is None:
                    raise ValueError(&#34;YAML content is empty or invalid&#34;) from parse_error
                logger.info(&#34;Successfully parsed YAML after fixing common syntax errors&#34;)
                return data

        except yaml.YAMLError as e:
            logger.error(f&#34;Failed to parse YAML: {e}&#34;)
            raise ValueError(f&#34;Invalid YAML content: {e}&#34;) from e
        except Exception as e:
            logger.error(f&#34;Unexpected error parsing YAML: {e}&#34;)
            raise ValueError(f&#34;Failed to parse content: {e}&#34;) from e

    def _create_score(
        self, score: float, passed: bool, feedback: str, details: dict[str, Any] | None = None
    ) -&gt; QualityScore:
        &#34;&#34;&#34;
        Create a QualityScore instance (helper method).

        Args:
            score: Numeric score (0.0-10.0)
            passed: Whether quality threshold was met
            feedback: Feedback message
            details: Optional detailed metrics

        Returns:
            QualityScore instance
        &#34;&#34;&#34;
        return QualityScore(score=score, passed=passed, feedback=feedback, details=details or {})

    def _build_feedback(self, score: float, failures: list, successes: list | None = None) -&gt; str:
        &#34;&#34;&#34;
        Build feedback message from score and findings.

        Args:
            score: Numeric score
            failures: List of failure messages
            successes: Optional list of success messages

        Returns:
            Human-readable feedback message
        &#34;&#34;&#34;
        if score &gt;= 9.0:
            base = &#34;Excellent quality&#34;
        elif score &gt;= 7.0:
            base = &#34;Good quality&#34;
        elif score &gt;= 5.0:
            base = &#34;Acceptable quality&#34;
        elif score &gt;= 3.0:
            base = &#34;Poor quality&#34;
        else:
            base = &#34;Very poor quality&#34;

        if not failures:
            return f&#34;{base} - all checks passed&#34;

        if len(failures) == 1:
            return f&#34;{base} - {failures[0]}&#34;

        return f&#34;{base} - {len(failures)} issues found&#34;

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;String representation of evaluator.&#34;&#34;&#34;
        return f&#34;{self.__class__.__name__}(threshold={self.pass_threshold})&#34;</code></pre>
</details>
<div class="desc"><p>Abstract base class for all quality evaluators.</p>
<p>All content-specific evaluators (Plot, Narrative, Puzzle, Scene, Mechanics)
should inherit from this class and implement the required methods.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; class PlotEvaluator(QualityEvaluator):
...     def evaluate(self, content: str) -&gt; QualityScore:
...         # Implementation here
...         pass
</code></pre>
<p>Initialize the evaluator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum score required to pass (0.0-10.0)</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="space_hulk_game.quality.mechanics_evaluator.MechanicsEvaluator" href="mechanics_evaluator.html#space_hulk_game.quality.mechanics_evaluator.MechanicsEvaluator">MechanicsEvaluator</a></li>
<li><a title="space_hulk_game.quality.narrative_evaluator.NarrativeMapEvaluator" href="narrative_evaluator.html#space_hulk_game.quality.narrative_evaluator.NarrativeMapEvaluator">NarrativeMapEvaluator</a></li>
<li><a title="space_hulk_game.quality.plot_evaluator.PlotEvaluator" href="plot_evaluator.html#space_hulk_game.quality.plot_evaluator.PlotEvaluator">PlotEvaluator</a></li>
<li><a title="space_hulk_game.quality.puzzle_evaluator.PuzzleEvaluator" href="puzzle_evaluator.html#space_hulk_game.quality.puzzle_evaluator.PuzzleEvaluator">PuzzleEvaluator</a></li>
<li><a title="space_hulk_game.quality.scene_evaluator.SceneEvaluator" href="scene_evaluator.html#space_hulk_game.quality.scene_evaluator.SceneEvaluator">SceneEvaluator</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="space_hulk_game.quality.QualityEvaluator.parse_yaml"><code class="name flex">
<span>def <span class="ident">parse_yaml</span></span>(<span>content: str) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def parse_yaml(content: str) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Parse YAML content, handling markdown-wrapped YAML and common syntax errors.

    Args:
        content: YAML string, optionally wrapped in markdown code fences

    Returns:
        Parsed YAML as dictionary

    Raises:
        ValueError: If YAML cannot be parsed
    &#34;&#34;&#34;
    try:
        # Handle markdown-wrapped YAML
        content_stripped = content.strip()
        if content_stripped.startswith(&#34;```&#34;):
            lines = content_stripped.split(&#34;\n&#34;)
            # Remove first line (```yaml or ```) and last line (```)
            content_stripped = &#34;\n&#34;.join(lines[1:-1])
            logger.debug(&#34;Stripped markdown fences from YAML content&#34;)

        # Try to parse as-is first
        try:
            data = yaml.safe_load(content_stripped)
            if data is None:
                raise ValueError(&#34;YAML content is empty or invalid&#34;)
            return data
        except yaml.YAMLError as parse_error:
            # Attempt to fix common YAML syntax errors and retry
            logger.debug(
                f&#34;Initial YAML parse failed, attempting to fix common errors: {parse_error}&#34;
            )
            fixed_content = QualityEvaluator._fix_common_yaml_errors(content_stripped)
            data = yaml.safe_load(fixed_content)
            if data is None:
                raise ValueError(&#34;YAML content is empty or invalid&#34;) from parse_error
            logger.info(&#34;Successfully parsed YAML after fixing common syntax errors&#34;)
            return data

    except yaml.YAMLError as e:
        logger.error(f&#34;Failed to parse YAML: {e}&#34;)
        raise ValueError(f&#34;Invalid YAML content: {e}&#34;) from e
    except Exception as e:
        logger.error(f&#34;Unexpected error parsing YAML: {e}&#34;)
        raise ValueError(f&#34;Failed to parse content: {e}&#34;) from e</code></pre>
</details>
<div class="desc"><p>Parse YAML content, handling markdown-wrapped YAML and common syntax errors.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string, optionally wrapped in markdown code fences</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Parsed YAML as dictionary</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If YAML cannot be parsed</dd>
</dl></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.QualityEvaluator.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, content: str) ‑> <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def evaluate(self, content: str) -&gt; QualityScore:
    &#34;&#34;&#34;
    Evaluate content and return quality score.

    Args:
        content: Content to evaluate (usually YAML string)

    Returns:
        QualityScore with evaluation results

    Raises:
        ValueError: If content cannot be parsed or evaluated
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><p>Evaluate content and return quality score.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>Content to evaluate (usually YAML string)</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>QualityScore with evaluation results</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If content cannot be parsed or evaluated</dd>
</dl></div>
</dd>
<dt id="space_hulk_game.quality.QualityEvaluator.generate_feedback"><code class="name flex">
<span>def <span class="ident">generate_feedback</span></span>(<span>self, content: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_feedback(self, content: str) -&gt; str:
    &#34;&#34;&#34;
    Generate feedback message for content (convenience method).

    Args:
        content: Content to evaluate

    Returns:
        Human-readable feedback message
    &#34;&#34;&#34;
    result = self.evaluate(content)
    return result.feedback</code></pre>
</details>
<div class="desc"><p>Generate feedback message for content (convenience method).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>Content to evaluate</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Human-readable feedback message</p></div>
</dd>
<dt id="space_hulk_game.quality.QualityEvaluator.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, content: str) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score(self, content: str) -&gt; float:
    &#34;&#34;&#34;
    Get numeric score for content (convenience method).

    Args:
        content: Content to evaluate

    Returns:
        Numeric score from 0.0 to 10.0
    &#34;&#34;&#34;
    result = self.evaluate(content)
    return result.score</code></pre>
</details>
<div class="desc"><p>Get numeric score for content (convenience method).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>Content to evaluate</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Numeric score from 0.0 to 10.0</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.QualityScore"><code class="flex name class">
<span>class <span class="ident">QualityScore</span></span>
<span>(</span><span>score: float,<br>passed: bool,<br>feedback: str,<br>details: dict[str, typing.Any] = &lt;factory&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class QualityScore:
    &#34;&#34;&#34;
    Standardized quality score returned by all evaluators.

    Attributes:
        score: Quality score from 0.0 to 10.0
        passed: Boolean indicating if quality threshold was met
        feedback: Human-readable feedback message
        details: Dictionary with detailed metrics and findings

    Example:
        &gt;&gt;&gt; score = QualityScore(
        ...     score=8.5,
        ...     passed=True,
        ...     feedback=&#34;Good quality with minor issues&#34;,
        ...     details={&#34;word_count&#34;: 650, &#34;branching_paths&#34;: 3}
        ... )
        &gt;&gt;&gt; print(score)
        QualityScore(8.5/10, PASS)
    &#34;&#34;&#34;

    score: float
    passed: bool
    feedback: str
    details: dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        &#34;&#34;&#34;Validate score is in valid range.&#34;&#34;&#34;
        if not 0.0 &lt;= self.score &lt;= 10.0:
            raise ValueError(f&#34;Score must be between 0.0 and 10.0, got {self.score}&#34;)

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;String representation of the quality score.&#34;&#34;&#34;
        status = &#34;PASS&#34; if self.passed else &#34;FAIL&#34;
        return f&#34;QualityScore({self.score:.1f}/10, {status})&#34;

    def __repr__(self) -&gt; str:
        &#34;&#34;&#34;Detailed representation of the quality score.&#34;&#34;&#34;
        feedback_display = self.feedback[:50] + &#34;...&#34; if len(self.feedback) &gt; 50 else self.feedback
        return (
            f&#34;QualityScore(score={self.score:.1f}, passed={self.passed}, &#34;
            f&#34;feedback=&#39;{feedback_display}&#39;)&#34;
        )

    def to_dict(self) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Convert quality score to dictionary for serialization.

        Returns:
            Dictionary representation of the score
        &#34;&#34;&#34;
        return {
            &#34;score&#34;: self.score,
            &#34;passed&#34;: self.passed,
            &#34;feedback&#34;: self.feedback,
            &#34;details&#34;: self.details,
        }

    def to_json(self) -&gt; str:
        &#34;&#34;&#34;
        Convert quality score to JSON string.

        Returns:
            JSON string representation of the score
        &#34;&#34;&#34;
        return json.dumps(self.to_dict(), indent=2)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -&gt; &#34;QualityScore&#34;:
        &#34;&#34;&#34;
        Create QualityScore from dictionary.

        Args:
            data: Dictionary containing score data

        Returns:
            QualityScore instance
        &#34;&#34;&#34;
        return cls(
            score=data[&#34;score&#34;],
            passed=data[&#34;passed&#34;],
            feedback=data[&#34;feedback&#34;],
            details=data.get(&#34;details&#34;, {}),
        )

    def get_failures(self) -&gt; list[str]:
        &#34;&#34;&#34;
        Extract failure messages from details.

        Returns:
            List of failure messages if present in details
        &#34;&#34;&#34;
        return cast(list[str], self.details.get(&#34;failures&#34;, []))

    def get_summary(self) -&gt; str:
        &#34;&#34;&#34;
        Get a multi-line summary of the quality score.

        Returns:
            Formatted summary string
        &#34;&#34;&#34;
        lines = [
            f&#34;Quality Score: {self.score:.1f}/10.0&#34;,
            f&#34;Status: {&#39;PASS&#39; if self.passed else &#39;FAIL&#39;}&#34;,
            f&#34;Feedback: {self.feedback}&#34;,
        ]

        if self.details:
            lines.append(&#34;\nDetails:&#34;)
            for key, value in self.details.items():
                if key != &#34;failures&#34;:  # Failures shown separately
                    lines.append(f&#34;  {key}: {value}&#34;)

        failures = self.get_failures()
        if failures:
            lines.append(&#34;\nIssues Found:&#34;)
            for failure in failures:
                lines.append(f&#34;  - {failure}&#34;)

        return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Standardized quality score returned by all evaluators.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>score</code></strong></dt>
<dd>Quality score from 0.0 to 10.0</dd>
<dt><strong><code>passed</code></strong></dt>
<dd>Boolean indicating if quality threshold was met</dd>
<dt><strong><code>feedback</code></strong></dt>
<dd>Human-readable feedback message</dd>
<dt><strong><code>details</code></strong></dt>
<dd>Dictionary with detailed metrics and findings</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; score = QualityScore(
...     score=8.5,
...     passed=True,
...     feedback=&quot;Good quality with minor issues&quot;,
...     details={&quot;word_count&quot;: 650, &quot;branching_paths&quot;: 3}
... )
&gt;&gt;&gt; print(score)
QualityScore(8.5/10, PASS)
</code></pre></div>
<h3>Static methods</h3>
<dl>
<dt id="space_hulk_game.quality.QualityScore.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>data: dict[str, typing.Any]) ‑> <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create QualityScore from dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>Dictionary containing score data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>QualityScore instance</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="space_hulk_game.quality.QualityScore.details"><code class="name">var <span class="ident">details</span> : dict[str, typing.Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.QualityScore.feedback"><code class="name">var <span class="ident">feedback</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.QualityScore.passed"><code class="name">var <span class="ident">passed</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.QualityScore.score"><code class="name">var <span class="ident">score</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.QualityScore.get_failures"><code class="name flex">
<span>def <span class="ident">get_failures</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_failures(self) -&gt; list[str]:
    &#34;&#34;&#34;
    Extract failure messages from details.

    Returns:
        List of failure messages if present in details
    &#34;&#34;&#34;
    return cast(list[str], self.details.get(&#34;failures&#34;, []))</code></pre>
</details>
<div class="desc"><p>Extract failure messages from details.</p>
<h2 id="returns">Returns</h2>
<p>List of failure messages if present in details</p></div>
</dd>
<dt id="space_hulk_game.quality.QualityScore.get_summary"><code class="name flex">
<span>def <span class="ident">get_summary</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_summary(self) -&gt; str:
    &#34;&#34;&#34;
    Get a multi-line summary of the quality score.

    Returns:
        Formatted summary string
    &#34;&#34;&#34;
    lines = [
        f&#34;Quality Score: {self.score:.1f}/10.0&#34;,
        f&#34;Status: {&#39;PASS&#39; if self.passed else &#39;FAIL&#39;}&#34;,
        f&#34;Feedback: {self.feedback}&#34;,
    ]

    if self.details:
        lines.append(&#34;\nDetails:&#34;)
        for key, value in self.details.items():
            if key != &#34;failures&#34;:  # Failures shown separately
                lines.append(f&#34;  {key}: {value}&#34;)

    failures = self.get_failures()
    if failures:
        lines.append(&#34;\nIssues Found:&#34;)
        for failure in failures:
            lines.append(f&#34;  - {failure}&#34;)

    return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Get a multi-line summary of the quality score.</p>
<h2 id="returns">Returns</h2>
<p>Formatted summary string</p></div>
</dd>
<dt id="space_hulk_game.quality.QualityScore.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Convert quality score to dictionary for serialization.

    Returns:
        Dictionary representation of the score
    &#34;&#34;&#34;
    return {
        &#34;score&#34;: self.score,
        &#34;passed&#34;: self.passed,
        &#34;feedback&#34;: self.feedback,
        &#34;details&#34;: self.details,
    }</code></pre>
</details>
<div class="desc"><p>Convert quality score to dictionary for serialization.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary representation of the score</p></div>
</dd>
<dt id="space_hulk_game.quality.QualityScore.to_json"><code class="name flex">
<span>def <span class="ident">to_json</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_json(self) -&gt; str:
    &#34;&#34;&#34;
    Convert quality score to JSON string.

    Returns:
        JSON string representation of the score
    &#34;&#34;&#34;
    return json.dumps(self.to_dict(), indent=2)</code></pre>
</details>
<div class="desc"><p>Convert quality score to JSON string.</p>
<h2 id="returns">Returns</h2>
<p>JSON string representation of the score</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.SceneEvaluator"><code class="flex name class">
<span>class <span class="ident">SceneEvaluator</span></span>
<span>(</span><span>pass_threshold: float = 6.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SceneEvaluator(QualityEvaluator):
    &#34;&#34;&#34;
    Evaluator for scene text content.

    Uses SceneMetrics to evaluate scene texts against quality criteria
    including vivid descriptions, dialogue presence, tone consistency,
    and sensory details.

    Example:
        &gt;&gt;&gt; evaluator = SceneEvaluator()
        &gt;&gt;&gt; score = evaluator.evaluate(scene_yaml_content)
        &gt;&gt;&gt; print(f&#34;Score: {score.score}/10&#34;)
        &gt;&gt;&gt; print(score.feedback)
    &#34;&#34;&#34;

    def __init__(self, pass_threshold: float = 6.0):
        &#34;&#34;&#34;
        Initialize the scene evaluator.

        Args:
            pass_threshold: Minimum score required to pass (default: 6.0)
        &#34;&#34;&#34;
        super().__init__(pass_threshold)
        logger.info(f&#34;SceneEvaluator initialized with threshold {pass_threshold}&#34;)

    def evaluate(self, content: str) -&gt; QualityScore:
        &#34;&#34;&#34;
        Evaluate scene text content.

        Args:
            content: YAML string containing scene texts

        Returns:
            QualityScore with evaluation results

        Raises:
            ValueError: If content cannot be parsed
        &#34;&#34;&#34;
        try:
            # Parse YAML content using metrics class
            metrics = SceneMetrics.from_yaml_content(content)

            # Get score and check if passes threshold
            score = metrics.get_score()
            passed = score &gt;= self.pass_threshold

            # Get failures for detailed feedback
            failures = metrics.get_failures()

            # Build feedback message
            feedback = self._build_feedback(score, failures)

            # Build details dictionary
            details = {
                &#34;total_scenes&#34;: metrics.total_scenes,
                &#34;scenes_with_vivid_descriptions&#34;: metrics.scenes_with_vivid_descriptions,
                &#34;scenes_with_dialogue&#34;: metrics.scenes_with_dialogue,
                &#34;average_description_length&#34;: metrics.average_description_length,
                &#34;tone_consistency_score&#34;: metrics.tone_consistency_score,
                &#34;has_sensory_details&#34;: metrics.has_sensory_details,
                &#34;total_word_count&#34;: metrics.average_description_length * metrics.total_scenes
                if metrics.total_scenes &gt; 0
                else 0,
                &#34;scenes_without_descriptions&#34;: [],  # Can be computed if needed
                &#34;scenes_with_short_descriptions&#34;: [],
                &#34;failures&#34;: failures,
                &#34;threshold&#34;: self.pass_threshold,
            }

            logger.info(
                f&#34;Scene evaluation complete: score={score:.1f}, passed={passed}, &#34;
                f&#34;scenes={metrics.total_scenes}, avg_length={metrics.average_description_length:.0f}&#34;
            )

            return self._create_score(score, passed, feedback, details)

        except ValueError as e:
            logger.error(f&#34;Failed to evaluate scene content: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Failed to parse scene content: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )
        except Exception as e:
            logger.exception(f&#34;Unexpected error evaluating scene: {e}&#34;)
            return self._create_score(
                score=0.0,
                passed=False,
                feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
                details={&#34;error&#34;: str(e)},
            )

    def generate_detailed_feedback(self, content: str) -&gt; str:
        &#34;&#34;&#34;
        Generate detailed, actionable feedback for improvement.

        Args:
            content: YAML string containing scene texts

        Returns:
            Multi-line feedback with specific suggestions
        &#34;&#34;&#34;
        result = self.evaluate(content)

        lines = [
            f&#34;Scene Text Quality Score: {result.score:.1f}/10.0&#34;,
            f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
            &#34;&#34;,
        ]

        if result.passed:
            lines.append(&#34;The scene texts meet all quality requirements!&#34;)
        else:
            lines.append(&#34;The scene texts need improvement:&#34;)

        lines.append(&#34;&#34;)

        # Add specific findings
        details = result.details
        failures = details.get(&#34;failures&#34;, [])

        if failures:
            lines.append(&#34;Issues to address:&#34;)
            for failure in failures:
                lines.append(f&#34;  • {failure}&#34;)
            lines.append(&#34;&#34;)

        # Add content statistics
        lines.append(f&#34;Scene count: {details.get(&#39;total_scenes&#39;, 0)} (min: 5)&#34;)
        lines.append(f&#34;Total word count: {details.get(&#39;total_word_count&#39;, 0):.0f}&#34;)
        lines.append(
            f&#34;Average description length: {details.get(&#39;average_description_length&#39;, 0):.0f} words&#34;
        )
        lines.append(
            f&#34;Scenes with vivid descriptions: {details.get(&#39;scenes_with_vivid_descriptions&#39;, 0)}&#34;
        )
        lines.append(f&#34;Scenes with dialogue: {details.get(&#39;scenes_with_dialogue&#39;, 0)}&#34;)

        # Add positive feedback
        lines.append(&#34;&#34;)
        if details.get(&#34;scenes_with_vivid_descriptions&#34;, 0) &gt; 0:
            lines.append(&#34;✓ Vivid, detailed descriptions present&#34;)
        if details.get(&#34;scenes_with_dialogue&#34;, 0) &gt; 0:
            lines.append(&#34;✓ Dialogue present where appropriate&#34;)
        if details.get(&#34;tone_consistency_score&#34;, 0) &gt;= 7.0:
            lines.append(&#34;✓ Consistent tone throughout&#34;)
        if details.get(&#34;has_sensory_details&#34;):
            lines.append(&#34;✓ Rich sensory details included&#34;)

        return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Evaluator for scene text content.</p>
<p>Uses SceneMetrics to evaluate scene texts against quality criteria
including vivid descriptions, dialogue presence, tone consistency,
and sensory details.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; evaluator = SceneEvaluator()
&gt;&gt;&gt; score = evaluator.evaluate(scene_yaml_content)
&gt;&gt;&gt; print(f&quot;Score: {score.score}/10&quot;)
&gt;&gt;&gt; print(score.feedback)
</code></pre>
<p>Initialize the scene evaluator.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum score required to pass (default: 6.0)</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.SceneEvaluator.evaluate"><code class="name flex">
<span>def <span class="ident">evaluate</span></span>(<span>self, content: str) ‑> <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate(self, content: str) -&gt; QualityScore:
    &#34;&#34;&#34;
    Evaluate scene text content.

    Args:
        content: YAML string containing scene texts

    Returns:
        QualityScore with evaluation results

    Raises:
        ValueError: If content cannot be parsed
    &#34;&#34;&#34;
    try:
        # Parse YAML content using metrics class
        metrics = SceneMetrics.from_yaml_content(content)

        # Get score and check if passes threshold
        score = metrics.get_score()
        passed = score &gt;= self.pass_threshold

        # Get failures for detailed feedback
        failures = metrics.get_failures()

        # Build feedback message
        feedback = self._build_feedback(score, failures)

        # Build details dictionary
        details = {
            &#34;total_scenes&#34;: metrics.total_scenes,
            &#34;scenes_with_vivid_descriptions&#34;: metrics.scenes_with_vivid_descriptions,
            &#34;scenes_with_dialogue&#34;: metrics.scenes_with_dialogue,
            &#34;average_description_length&#34;: metrics.average_description_length,
            &#34;tone_consistency_score&#34;: metrics.tone_consistency_score,
            &#34;has_sensory_details&#34;: metrics.has_sensory_details,
            &#34;total_word_count&#34;: metrics.average_description_length * metrics.total_scenes
            if metrics.total_scenes &gt; 0
            else 0,
            &#34;scenes_without_descriptions&#34;: [],  # Can be computed if needed
            &#34;scenes_with_short_descriptions&#34;: [],
            &#34;failures&#34;: failures,
            &#34;threshold&#34;: self.pass_threshold,
        }

        logger.info(
            f&#34;Scene evaluation complete: score={score:.1f}, passed={passed}, &#34;
            f&#34;scenes={metrics.total_scenes}, avg_length={metrics.average_description_length:.0f}&#34;
        )

        return self._create_score(score, passed, feedback, details)

    except ValueError as e:
        logger.error(f&#34;Failed to evaluate scene content: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Failed to parse scene content: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )
    except Exception as e:
        logger.exception(f&#34;Unexpected error evaluating scene: {e}&#34;)
        return self._create_score(
            score=0.0,
            passed=False,
            feedback=f&#34;Unexpected error during evaluation: {e!s}&#34;,
            details={&#34;error&#34;: str(e)},
        )</code></pre>
</details>
<div class="desc"><p>Evaluate scene text content.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing scene texts</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>QualityScore with evaluation results</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If content cannot be parsed</dd>
</dl></div>
</dd>
<dt id="space_hulk_game.quality.SceneEvaluator.generate_detailed_feedback"><code class="name flex">
<span>def <span class="ident">generate_detailed_feedback</span></span>(<span>self, content: str) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_detailed_feedback(self, content: str) -&gt; str:
    &#34;&#34;&#34;
    Generate detailed, actionable feedback for improvement.

    Args:
        content: YAML string containing scene texts

    Returns:
        Multi-line feedback with specific suggestions
    &#34;&#34;&#34;
    result = self.evaluate(content)

    lines = [
        f&#34;Scene Text Quality Score: {result.score:.1f}/10.0&#34;,
        f&#34;Status: {&#39;PASS ✓&#39; if result.passed else &#39;FAIL ✗&#39;}&#34;,
        &#34;&#34;,
    ]

    if result.passed:
        lines.append(&#34;The scene texts meet all quality requirements!&#34;)
    else:
        lines.append(&#34;The scene texts need improvement:&#34;)

    lines.append(&#34;&#34;)

    # Add specific findings
    details = result.details
    failures = details.get(&#34;failures&#34;, [])

    if failures:
        lines.append(&#34;Issues to address:&#34;)
        for failure in failures:
            lines.append(f&#34;  • {failure}&#34;)
        lines.append(&#34;&#34;)

    # Add content statistics
    lines.append(f&#34;Scene count: {details.get(&#39;total_scenes&#39;, 0)} (min: 5)&#34;)
    lines.append(f&#34;Total word count: {details.get(&#39;total_word_count&#39;, 0):.0f}&#34;)
    lines.append(
        f&#34;Average description length: {details.get(&#39;average_description_length&#39;, 0):.0f} words&#34;
    )
    lines.append(
        f&#34;Scenes with vivid descriptions: {details.get(&#39;scenes_with_vivid_descriptions&#39;, 0)}&#34;
    )
    lines.append(f&#34;Scenes with dialogue: {details.get(&#39;scenes_with_dialogue&#39;, 0)}&#34;)

    # Add positive feedback
    lines.append(&#34;&#34;)
    if details.get(&#34;scenes_with_vivid_descriptions&#34;, 0) &gt; 0:
        lines.append(&#34;✓ Vivid, detailed descriptions present&#34;)
    if details.get(&#34;scenes_with_dialogue&#34;, 0) &gt; 0:
        lines.append(&#34;✓ Dialogue present where appropriate&#34;)
    if details.get(&#34;tone_consistency_score&#34;, 0) &gt;= 7.0:
        lines.append(&#34;✓ Consistent tone throughout&#34;)
    if details.get(&#34;has_sensory_details&#34;):
        lines.append(&#34;✓ Rich sensory details included&#34;)

    return &#34;\n&#34;.join(lines)</code></pre>
</details>
<div class="desc"><p>Generate detailed, actionable feedback for improvement.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>content</code></strong></dt>
<dd>YAML string containing scene texts</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Multi-line feedback with specific suggestions</p></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="space_hulk_game.quality.evaluator.QualityEvaluator" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator">QualityEvaluator</a></b></code>:
<ul class="hlist">
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.generate_feedback">generate_feedback</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.parse_yaml">parse_yaml</a></code></li>
<li><code><a title="space_hulk_game.quality.evaluator.QualityEvaluator.score" href="evaluator.html#space_hulk_game.quality.evaluator.QualityEvaluator.score">score</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics"><code class="flex name class">
<span>class <span class="ident">SceneMetrics</span></span>
<span>(</span><span>total_scenes: int = 0,<br>scenes_with_vivid_descriptions: int = 0,<br>scenes_with_dialogue: int = 0,<br>average_description_length: float = 0.0,<br>tone_consistency_score: float = 0.0,<br>has_sensory_details: bool = False,<br>min_scenes: int = 5,<br>min_description_length: int = 50,<br>min_vivid_percentage: float = 70.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class SceneMetrics:
    &#34;&#34;&#34;
    Quality metrics for evaluating scene text content.

    Attributes:
        total_scenes: Total number of scenes
        scenes_with_vivid_descriptions: Number of scenes with quality descriptions
        scenes_with_dialogue: Number of scenes containing dialogue
        average_description_length: Average length of scene descriptions (words)
        tone_consistency_score: Score for tone consistency (0.0 to 10.0)
        has_sensory_details: Whether scenes include sensory details
        min_scenes: Minimum required scenes (default: 5)
        min_description_length: Minimum description length in words (default: 50)
    &#34;&#34;&#34;

    # Measured values
    total_scenes: int = 0
    scenes_with_vivid_descriptions: int = 0
    scenes_with_dialogue: int = 0
    average_description_length: float = 0.0
    tone_consistency_score: float = 0.0
    has_sensory_details: bool = False

    # Thresholds
    min_scenes: int = 5
    min_description_length: int = 50
    min_vivid_percentage: float = 70.0  # 70% should be vivid

    @classmethod
    def from_yaml_content(cls, yaml_content: str) -&gt; &#34;SceneMetrics&#34;:
        &#34;&#34;&#34;
        Create SceneMetrics from YAML content string.

        Args:
            yaml_content: YAML string containing scene texts data

        Returns:
            SceneMetrics instance with measured values
        &#34;&#34;&#34;
        try:
            # Handle markdown-wrapped YAML
            content = yaml_content.strip()
            if content.startswith(&#34;```&#34;):
                lines = content.split(&#34;\n&#34;)
                content = &#34;\n&#34;.join(lines[1:-1])

            data = yaml.safe_load(content)
            return cls.from_dict(data)
        except Exception as e:
            raise ValueError(f&#34;Failed to parse YAML content: {e}&#34;) from e

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -&gt; &#34;SceneMetrics&#34;:
        &#34;&#34;&#34;
        Create SceneMetrics from a dictionary (parsed YAML).

        Args:
            data: Dictionary containing scene texts data

        Returns:
            SceneMetrics instance with measured values
        &#34;&#34;&#34;
        metrics = cls()

        # Get scenes
        scenes = data.get(&#34;scenes&#34;, {})
        if not scenes:
            # Try alternate structure
            scenes = data.get(&#34;scene_texts&#34;, {})

        metrics.total_scenes = len(scenes)

        if metrics.total_scenes == 0:
            return metrics

        # Analyze each scene
        total_description_length = 0
        tones = []

        for _scene_id, scene_data in scenes.items():
            if not isinstance(scene_data, dict):
                continue

            # Get description
            description = scene_data.get(&#34;description&#34;, &#34;&#34;)
            if not description:
                description = scene_data.get(&#34;text&#34;, &#34;&#34;)

            # Check for vivid description
            if cls._is_vivid_description(description):
                metrics.scenes_with_vivid_descriptions += 1

            # Check for dialogue
            if cls._has_dialogue(description, scene_data):
                metrics.scenes_with_dialogue += 1

            # Track description length
            desc_length = len(description.split()) if isinstance(description, str) else 0
            total_description_length += desc_length

            # Track tone
            tone = scene_data.get(&#34;tone&#34;, &#34;&#34;)
            if tone:
                tones.append(tone)

            # Check for sensory details
            if cls._has_sensory_details(description):
                metrics.has_sensory_details = True

        # Calculate averages
        metrics.average_description_length = total_description_length / metrics.total_scenes

        # Calculate tone consistency
        metrics.tone_consistency_score = cls._calculate_tone_consistency(tones)

        return metrics

    @staticmethod
    def _is_vivid_description(description: str) -&gt; bool:
        &#34;&#34;&#34;
        Check if description is vivid and engaging.

        A vivid description should:
        - Be at least 50 words
        - Contain descriptive adjectives
        - Include specific details

        Args:
            description: Scene description text

        Returns:
            True if description is vivid
        &#34;&#34;&#34;
        if not isinstance(description, str):
            return False

        # Check length
        words = description.split()
        if len(words) &lt; 50:
            return False

        # Check for descriptive words (simple heuristic)
        descriptive_patterns = [
            r&#34;\b(dark|ancient|massive|twisted|corrupted|flickering|ominous)\b&#34;,
            r&#34;\b(gleaming|rusted|broken|shattered|blood|shadows|decay)\b&#34;,
            r&#34;\b(echoing|silent|screaming|whispering|rumbling)\b&#34;,
        ]

        has_descriptive = False
        for pattern in descriptive_patterns:
            if re.search(pattern, description.lower()):
                has_descriptive = True
                break

        return has_descriptive

    @staticmethod
    def _has_dialogue(description: str, scene_data: dict[str, Any]) -&gt; bool:
        &#34;&#34;&#34;
        Check if scene contains dialogue.

        Args:
            description: Scene description text
            scene_data: Full scene data dictionary

        Returns:
            True if dialogue is present
        &#34;&#34;&#34;
        # Check for explicit dialogue field
        if scene_data.get(&#34;dialogue&#34;):
            return True

        if not isinstance(description, str):
            return False

        # Check for dialogue markers

        # Look for quoted text (primary indicator)
        if &#39;&#34;&#39; in description or &#34;&#39;&#34; in description:
            return True

        # Look for dialogue keywords (more specific patterns)
        desc_lower = description.lower()
        for marker in [&#34;says&#34;, &#34;said&#34;, &#34;asks&#34;, &#34;asked&#34;]:
            if marker in desc_lower:
                return True

        return False

    @staticmethod
    def _has_sensory_details(description: str) -&gt; bool:
        &#34;&#34;&#34;
        Check if description includes sensory details.

        Args:
            description: Scene description text

        Returns:
            True if sensory details are present
        &#34;&#34;&#34;
        if not isinstance(description, str):
            return False

        # Sensory detail patterns
        sensory_patterns = [
            # Visual
            r&#34;\b(see|sees|saw|look|looks|glimpse|observe|witness)\b&#34;,
            r&#34;\b(dark|bright|dim|glowing|shadowy|lit)\b&#34;,
            # Auditory
            r&#34;\b(hear|hears|heard|sound|noise|echo|silence)\b&#34;,
            r&#34;\b(loud|quiet|whisper|scream|rumble)\b&#34;,
            # Tactile
            r&#34;\b(feel|feels|felt|touch|cold|hot|warm)\b&#34;,
            # Olfactory
            r&#34;\b(smell|smells|stench|odor|scent)\b&#34;,
            # Other
            r&#34;\b(taste|atmosphere|sense)\b&#34;,
        ]

        desc_lower = description.lower()
        matches = sum(1 for pattern in sensory_patterns if re.search(pattern, desc_lower))

        # At least 2 different sensory references
        return matches &gt;= 2

    @staticmethod
    def _calculate_tone_consistency(tones: list[str]) -&gt; float:
        &#34;&#34;&#34;
        Calculate tone consistency score.

        Args:
            tones: List of tone descriptors from scenes

        Returns:
            Score from 0.0 to 10.0
        &#34;&#34;&#34;
        if not tones:
            return 5.0  # Neutral score if no tones defined

        # Count unique tones
        unique_tones = {tone.lower() for tone in tones}

        # More consistency = fewer unique tones relative to total
        # Perfect consistency would be 1 unique tone
        consistency_ratio = 1.0 - ((len(unique_tones) - 1) / max(len(tones), 1))

        return consistency_ratio * 10.0

    def passes_threshold(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if the scene metrics pass all quality thresholds.

        Returns:
            True if all thresholds are met, False otherwise
        &#34;&#34;&#34;
        vivid_percentage = 0.0
        if self.total_scenes &gt; 0:
            vivid_percentage = (self.scenes_with_vivid_descriptions / self.total_scenes) * 100.0

        return (
            self.total_scenes &gt;= self.min_scenes
            and self.average_description_length &gt;= self.min_description_length
            and vivid_percentage &gt;= self.min_vivid_percentage
        )

    def get_failures(self) -&gt; list[str]:
        &#34;&#34;&#34;
        Get list of failed quality checks.

        Returns:
            List of failure messages for metrics that don&#39;t meet thresholds
        &#34;&#34;&#34;
        failures = []

        if self.total_scenes &lt; self.min_scenes:
            failures.append(
                f&#34;Insufficient scenes: {self.total_scenes} &#34; f&#34;(minimum: {self.min_scenes})&#34;
            )

        if self.average_description_length &lt; self.min_description_length:
            failures.append(
                f&#34;Descriptions too short: avg {self.average_description_length:.0f} words &#34;
                f&#34;(minimum: {self.min_description_length})&#34;
            )

        vivid_percentage = 0.0
        if self.total_scenes &gt; 0:
            vivid_percentage = (self.scenes_with_vivid_descriptions / self.total_scenes) * 100.0

        if vivid_percentage &lt; self.min_vivid_percentage:
            failures.append(
                f&#34;Too few vivid descriptions: {vivid_percentage:.1f}% &#34;
                f&#34;(minimum: {self.min_vivid_percentage}%)&#34;
            )

        # Warnings
        if self.total_scenes &gt; 0:
            dialogue_percentage = (self.scenes_with_dialogue / self.total_scenes) * 100.0
            if dialogue_percentage &lt; 30.0:
                failures.append(
                    f&#34;Warning: Only {dialogue_percentage:.1f}% of scenes &#34; f&#34;contain dialogue&#34;
                )

        return failures

    def get_score(self) -&gt; float:
        &#34;&#34;&#34;
        Calculate overall quality score (0.0 to 10.0).

        Returns:
            Quality score based on met criteria
        &#34;&#34;&#34;
        score = 0.0

        # Minimum scenes (2 points)
        if self.total_scenes &gt;= self.min_scenes:
            score += 2.0
        elif self.total_scenes &gt; 0:
            score += 2.0 * (self.total_scenes / self.min_scenes)

        # Description length (2 points)
        if self.average_description_length &gt;= self.min_description_length:
            score += 2.0
        elif self.average_description_length &gt; 0:
            score += 2.0 * (self.average_description_length / self.min_description_length)

        # Vivid descriptions (3 points)
        if self.total_scenes &gt; 0:
            vivid_percentage = (self.scenes_with_vivid_descriptions / self.total_scenes) * 100.0
            score += 3.0 * (vivid_percentage / 100.0)

        # Dialogue (1 point)
        if self.total_scenes &gt; 0:
            dialogue_percentage = (self.scenes_with_dialogue / self.total_scenes) * 100.0
            score += 1.0 * (dialogue_percentage / 100.0)

        # Tone consistency (1 point)
        score += self.tone_consistency_score / 10.0

        # Sensory details (1 point)
        if self.has_sensory_details:
            score += 1.0

        return min(score, 10.0)

    def to_dict(self) -&gt; dict[str, Any]:
        &#34;&#34;&#34;
        Convert metrics to dictionary for serialization.

        Returns:
            Dictionary representation of metrics
        &#34;&#34;&#34;
        return {
            &#34;total_scenes&#34;: self.total_scenes,
            &#34;scenes_with_vivid_descriptions&#34;: self.scenes_with_vivid_descriptions,
            &#34;scenes_with_dialogue&#34;: self.scenes_with_dialogue,
            &#34;average_description_length&#34;: round(self.average_description_length, 1),
            &#34;tone_consistency_score&#34;: round(self.tone_consistency_score, 2),
            &#34;has_sensory_details&#34;: self.has_sensory_details,
            &#34;passes_threshold&#34;: self.passes_threshold(),
            &#34;score&#34;: self.get_score(),
            &#34;failures&#34;: self.get_failures(),
        }</code></pre>
</details>
<div class="desc"><p>Quality metrics for evaluating scene text content.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>total_scenes</code></strong></dt>
<dd>Total number of scenes</dd>
<dt><strong><code>scenes_with_vivid_descriptions</code></strong></dt>
<dd>Number of scenes with quality descriptions</dd>
<dt><strong><code>scenes_with_dialogue</code></strong></dt>
<dd>Number of scenes containing dialogue</dd>
<dt><strong><code>average_description_length</code></strong></dt>
<dd>Average length of scene descriptions (words)</dd>
<dt><strong><code>tone_consistency_score</code></strong></dt>
<dd>Score for tone consistency (0.0 to 10.0)</dd>
<dt><strong><code>has_sensory_details</code></strong></dt>
<dd>Whether scenes include sensory details</dd>
<dt><strong><code>min_scenes</code></strong></dt>
<dd>Minimum required scenes (default: 5)</dd>
<dt><strong><code>min_description_length</code></strong></dt>
<dd>Minimum description length in words (default: 50)</dd>
</dl></div>
<h3>Static methods</h3>
<dl>
<dt id="space_hulk_game.quality.SceneMetrics.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>data: dict[str, typing.Any]) ‑> <a title="space_hulk_game.quality.scene_metrics.SceneMetrics" href="scene_metrics.html#space_hulk_game.quality.scene_metrics.SceneMetrics">SceneMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create SceneMetrics from a dictionary (parsed YAML).</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>Dictionary containing scene texts data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>SceneMetrics instance with measured values</p></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.from_yaml_content"><code class="name flex">
<span>def <span class="ident">from_yaml_content</span></span>(<span>yaml_content: str) ‑> <a title="space_hulk_game.quality.scene_metrics.SceneMetrics" href="scene_metrics.html#space_hulk_game.quality.scene_metrics.SceneMetrics">SceneMetrics</a></span>
</code></dt>
<dd>
<div class="desc"><p>Create SceneMetrics from YAML content string.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>yaml_content</code></strong></dt>
<dd>YAML string containing scene texts data</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>SceneMetrics instance with measured values</p></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="space_hulk_game.quality.SceneMetrics.average_description_length"><code class="name">var <span class="ident">average_description_length</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.has_sensory_details"><code class="name">var <span class="ident">has_sensory_details</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.min_description_length"><code class="name">var <span class="ident">min_description_length</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.min_scenes"><code class="name">var <span class="ident">min_scenes</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.min_vivid_percentage"><code class="name">var <span class="ident">min_vivid_percentage</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.scenes_with_dialogue"><code class="name">var <span class="ident">scenes_with_dialogue</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.scenes_with_vivid_descriptions"><code class="name">var <span class="ident">scenes_with_vivid_descriptions</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.tone_consistency_score"><code class="name">var <span class="ident">tone_consistency_score</span> : float</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.total_scenes"><code class="name">var <span class="ident">total_scenes</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.SceneMetrics.get_failures"><code class="name flex">
<span>def <span class="ident">get_failures</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_failures(self) -&gt; list[str]:
    &#34;&#34;&#34;
    Get list of failed quality checks.

    Returns:
        List of failure messages for metrics that don&#39;t meet thresholds
    &#34;&#34;&#34;
    failures = []

    if self.total_scenes &lt; self.min_scenes:
        failures.append(
            f&#34;Insufficient scenes: {self.total_scenes} &#34; f&#34;(minimum: {self.min_scenes})&#34;
        )

    if self.average_description_length &lt; self.min_description_length:
        failures.append(
            f&#34;Descriptions too short: avg {self.average_description_length:.0f} words &#34;
            f&#34;(minimum: {self.min_description_length})&#34;
        )

    vivid_percentage = 0.0
    if self.total_scenes &gt; 0:
        vivid_percentage = (self.scenes_with_vivid_descriptions / self.total_scenes) * 100.0

    if vivid_percentage &lt; self.min_vivid_percentage:
        failures.append(
            f&#34;Too few vivid descriptions: {vivid_percentage:.1f}% &#34;
            f&#34;(minimum: {self.min_vivid_percentage}%)&#34;
        )

    # Warnings
    if self.total_scenes &gt; 0:
        dialogue_percentage = (self.scenes_with_dialogue / self.total_scenes) * 100.0
        if dialogue_percentage &lt; 30.0:
            failures.append(
                f&#34;Warning: Only {dialogue_percentage:.1f}% of scenes &#34; f&#34;contain dialogue&#34;
            )

    return failures</code></pre>
</details>
<div class="desc"><p>Get list of failed quality checks.</p>
<h2 id="returns">Returns</h2>
<p>List of failure messages for metrics that don't meet thresholds</p></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.get_score"><code class="name flex">
<span>def <span class="ident">get_score</span></span>(<span>self) ‑> float</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_score(self) -&gt; float:
    &#34;&#34;&#34;
    Calculate overall quality score (0.0 to 10.0).

    Returns:
        Quality score based on met criteria
    &#34;&#34;&#34;
    score = 0.0

    # Minimum scenes (2 points)
    if self.total_scenes &gt;= self.min_scenes:
        score += 2.0
    elif self.total_scenes &gt; 0:
        score += 2.0 * (self.total_scenes / self.min_scenes)

    # Description length (2 points)
    if self.average_description_length &gt;= self.min_description_length:
        score += 2.0
    elif self.average_description_length &gt; 0:
        score += 2.0 * (self.average_description_length / self.min_description_length)

    # Vivid descriptions (3 points)
    if self.total_scenes &gt; 0:
        vivid_percentage = (self.scenes_with_vivid_descriptions / self.total_scenes) * 100.0
        score += 3.0 * (vivid_percentage / 100.0)

    # Dialogue (1 point)
    if self.total_scenes &gt; 0:
        dialogue_percentage = (self.scenes_with_dialogue / self.total_scenes) * 100.0
        score += 1.0 * (dialogue_percentage / 100.0)

    # Tone consistency (1 point)
    score += self.tone_consistency_score / 10.0

    # Sensory details (1 point)
    if self.has_sensory_details:
        score += 1.0

    return min(score, 10.0)</code></pre>
</details>
<div class="desc"><p>Calculate overall quality score (0.0 to 10.0).</p>
<h2 id="returns">Returns</h2>
<p>Quality score based on met criteria</p></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.passes_threshold"><code class="name flex">
<span>def <span class="ident">passes_threshold</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def passes_threshold(self) -&gt; bool:
    &#34;&#34;&#34;
    Check if the scene metrics pass all quality thresholds.

    Returns:
        True if all thresholds are met, False otherwise
    &#34;&#34;&#34;
    vivid_percentage = 0.0
    if self.total_scenes &gt; 0:
        vivid_percentage = (self.scenes_with_vivid_descriptions / self.total_scenes) * 100.0

    return (
        self.total_scenes &gt;= self.min_scenes
        and self.average_description_length &gt;= self.min_description_length
        and vivid_percentage &gt;= self.min_vivid_percentage
    )</code></pre>
</details>
<div class="desc"><p>Check if the scene metrics pass all quality thresholds.</p>
<h2 id="returns">Returns</h2>
<p>True if all thresholds are met, False otherwise</p></div>
</dd>
<dt id="space_hulk_game.quality.SceneMetrics.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict[str, typing.Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict[str, Any]:
    &#34;&#34;&#34;
    Convert metrics to dictionary for serialization.

    Returns:
        Dictionary representation of metrics
    &#34;&#34;&#34;
    return {
        &#34;total_scenes&#34;: self.total_scenes,
        &#34;scenes_with_vivid_descriptions&#34;: self.scenes_with_vivid_descriptions,
        &#34;scenes_with_dialogue&#34;: self.scenes_with_dialogue,
        &#34;average_description_length&#34;: round(self.average_description_length, 1),
        &#34;tone_consistency_score&#34;: round(self.tone_consistency_score, 2),
        &#34;has_sensory_details&#34;: self.has_sensory_details,
        &#34;passes_threshold&#34;: self.passes_threshold(),
        &#34;score&#34;: self.get_score(),
        &#34;failures&#34;: self.get_failures(),
    }</code></pre>
</details>
<div class="desc"><p>Convert metrics to dictionary for serialization.</p>
<h2 id="returns">Returns</h2>
<p>Dictionary representation of metrics</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.TaskExecutor"><code class="flex name class">
<span>class <span class="ident">TaskExecutor</span></span>
<span>(</span><span>config: <a title="space_hulk_game.quality.integration.QualityCheckConfig" href="integration.html#space_hulk_game.quality.integration.QualityCheckConfig">QualityCheckConfig</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TaskExecutor:
    &#34;&#34;&#34;
    Task executor with optional quality checking.

    This class wraps task execution to add quality checking when enabled.
    &#34;&#34;&#34;

    def __init__(self, config: QualityCheckConfig | None = None):
        &#34;&#34;&#34;
        Initialize task executor.

        Args:
            config: Quality check configuration (creates default if None)
        &#34;&#34;&#34;
        self.config = config if config is not None else QualityCheckConfig()
        logger.debug(f&#34;TaskExecutor initialized (quality checks: {self.config.is_enabled()})&#34;)

    def execute_task(
        self,
        task_function: Callable[..., str],
        task_type: TaskType,
        task_name: str = &#34;Task&#34;,
        **kwargs,
    ) -&gt; str:
        &#34;&#34;&#34;
        Execute task with optional quality checking.

        Args:
            task_function: Function to execute
            task_type: Type of task (for evaluator selection)
            task_name: Human-readable task name
            **kwargs: Arguments to pass to task function

        Returns:
            Task output string
        &#34;&#34;&#34;
        # Check if quality checking is enabled globally and for this task
        if not self.config.is_enabled():
            logger.debug(f&#34;{task_name}: Quality checking disabled, executing directly&#34;)
            return task_function(**kwargs)

        task_config = self.config.get_task_config(task_type)
        if not task_config.get(&#34;enabled&#34;, True):
            logger.debug(f&#34;{task_name}: Quality checking disabled for {task_type.value}&#34;)
            return task_function(**kwargs)

        # Execute with quality checking
        logger.info(f&#34;{task_name}: Executing with quality checks&#34;)
        output, quality, attempts = execute_with_quality_check(
            task_function=task_function,
            task_type=task_type,
            task_name=task_name,
            pass_threshold=task_config[&#34;pass_threshold&#34;],
            max_retries=task_config[&#34;max_retries&#34;],
            **kwargs,
        )

        # Log results
        logger.info(
            f&#34;{task_name}: Completed in {attempts} attempt(s) &#34;
            f&#34;with quality score {quality.score:.1f}/10.0 &#34;
            f&#34;({&#39;PASS&#39; if quality.passed else &#39;FAIL&#39;})&#34;
        )

        return output</code></pre>
</details>
<div class="desc"><p>Task executor with optional quality checking.</p>
<p>This class wraps task execution to add quality checking when enabled.</p>
<p>Initialize task executor.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>config</code></strong></dt>
<dd>Quality check configuration (creates default if None)</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.TaskExecutor.execute_task"><code class="name flex">
<span>def <span class="ident">execute_task</span></span>(<span>self,<br>task_function: Callable[..., str],<br>task_type: <a title="space_hulk_game.quality.retry.TaskType" href="retry.html#space_hulk_game.quality.retry.TaskType">TaskType</a>,<br>task_name: str = 'Task',<br>**kwargs) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute_task(
    self,
    task_function: Callable[..., str],
    task_type: TaskType,
    task_name: str = &#34;Task&#34;,
    **kwargs,
) -&gt; str:
    &#34;&#34;&#34;
    Execute task with optional quality checking.

    Args:
        task_function: Function to execute
        task_type: Type of task (for evaluator selection)
        task_name: Human-readable task name
        **kwargs: Arguments to pass to task function

    Returns:
        Task output string
    &#34;&#34;&#34;
    # Check if quality checking is enabled globally and for this task
    if not self.config.is_enabled():
        logger.debug(f&#34;{task_name}: Quality checking disabled, executing directly&#34;)
        return task_function(**kwargs)

    task_config = self.config.get_task_config(task_type)
    if not task_config.get(&#34;enabled&#34;, True):
        logger.debug(f&#34;{task_name}: Quality checking disabled for {task_type.value}&#34;)
        return task_function(**kwargs)

    # Execute with quality checking
    logger.info(f&#34;{task_name}: Executing with quality checks&#34;)
    output, quality, attempts = execute_with_quality_check(
        task_function=task_function,
        task_type=task_type,
        task_name=task_name,
        pass_threshold=task_config[&#34;pass_threshold&#34;],
        max_retries=task_config[&#34;max_retries&#34;],
        **kwargs,
    )

    # Log results
    logger.info(
        f&#34;{task_name}: Completed in {attempts} attempt(s) &#34;
        f&#34;with quality score {quality.score:.1f}/10.0 &#34;
        f&#34;({&#39;PASS&#39; if quality.passed else &#39;FAIL&#39;})&#34;
    )

    return output</code></pre>
</details>
<div class="desc"><p>Execute task with optional quality checking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task_function</code></strong></dt>
<dd>Function to execute</dd>
<dt><strong><code>task_type</code></strong></dt>
<dd>Type of task (for evaluator selection)</dd>
<dt><strong><code>task_name</code></strong></dt>
<dd>Human-readable task name</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Arguments to pass to task function</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Task output string</p></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.TaskType"><code class="flex name class">
<span>class <span class="ident">TaskType</span></span>
<span>(</span><span>*args, **kwds)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TaskType(Enum):
    &#34;&#34;&#34;Enum for task types to map to appropriate evaluators.&#34;&#34;&#34;

    PLOT = &#34;plot&#34;
    NARRATIVE = &#34;narrative&#34;
    PUZZLE = &#34;puzzle&#34;
    SCENE = &#34;scene&#34;
    MECHANICS = &#34;mechanics&#34;</code></pre>
</details>
<div class="desc"><p>Enum for task types to map to appropriate evaluators.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="space_hulk_game.quality.TaskType.MECHANICS"><code class="name">var <span class="ident">MECHANICS</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.TaskType.NARRATIVE"><code class="name">var <span class="ident">NARRATIVE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.TaskType.PLOT"><code class="name">var <span class="ident">PLOT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.TaskType.PUZZLE"><code class="name">var <span class="ident">PUZZLE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="space_hulk_game.quality.TaskType.SCENE"><code class="name">var <span class="ident">SCENE</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="space_hulk_game.quality.TaskWithQualityCheck"><code class="flex name class">
<span>class <span class="ident">TaskWithQualityCheck</span></span>
<span>(</span><span>task_type: <a title="space_hulk_game.quality.retry.TaskType" href="retry.html#space_hulk_game.quality.retry.TaskType">TaskType</a>,<br>pass_threshold: float = 6.0,<br>max_retries: int = 3)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TaskWithQualityCheck:
    &#34;&#34;&#34;
    Wrapper for task execution with quality checking and retry logic.

    This class wraps task execution to add quality evaluation after execution,
    implementing a retry loop that provides feedback for improvement.

    Example:
        &gt;&gt;&gt; task_wrapper = TaskWithQualityCheck(
        ...     task_type=TaskType.PLOT,
        ...     pass_threshold=7.0,
        ...     max_retries=3
        ... )
        &gt;&gt;&gt; result = task_wrapper.execute(task_function, task_name=&#34;Plot Generation&#34;)
    &#34;&#34;&#34;

    def __init__(self, task_type: TaskType, pass_threshold: float = 6.0, max_retries: int = 3):
        &#34;&#34;&#34;
        Initialize the task wrapper with quality checking.

        Args:
            task_type: Type of task to execute (determines which evaluator to use)
            pass_threshold: Minimum quality score required to pass (0.0-10.0)
            max_retries: Maximum number of retry attempts (default: 3)

        Raises:
            ValueError: If parameters are invalid
        &#34;&#34;&#34;
        if not 0.0 &lt;= pass_threshold &lt;= 10.0:
            raise ValueError(f&#34;Pass threshold must be between 0.0 and 10.0, got {pass_threshold}&#34;)
        if max_retries &lt; 1:
            raise ValueError(f&#34;Max retries must be at least 1, got {max_retries}&#34;)

        self.task_type = task_type
        self.pass_threshold = pass_threshold
        self.max_retries = max_retries
        self.evaluator = self._create_evaluator()

        logger.debug(
            f&#34;TaskWithQualityCheck initialized for {task_type.value} &#34;
            f&#34;(threshold={pass_threshold}, max_retries={max_retries})&#34;
        )

    def _create_evaluator(self) -&gt; QualityEvaluator:
        &#34;&#34;&#34;
        Create the appropriate evaluator based on task type.

        Returns:
            QualityEvaluator instance for the task type
        &#34;&#34;&#34;
        evaluator_map = {
            TaskType.PLOT: PlotEvaluator,
            TaskType.NARRATIVE: NarrativeMapEvaluator,
            TaskType.PUZZLE: PuzzleEvaluator,
            TaskType.SCENE: SceneEvaluator,
            TaskType.MECHANICS: MechanicsEvaluator,
        }

        evaluator_class = evaluator_map[self.task_type]
        return evaluator_class(pass_threshold=self.pass_threshold)

    def execute(
        self, task_function: Callable[..., str], task_name: str = &#34;Task&#34;, **kwargs
    ) -&gt; tuple[str, QualityScore, int]:
        &#34;&#34;&#34;
        Execute task with quality checking and retry logic.

        Args:
            task_function: Function to execute (should return output string)
            task_name: Human-readable name for logging
            **kwargs: Additional arguments to pass to task_function

        Returns:
            Tuple of (output, final_quality_score, attempts_made)
        &#34;&#34;&#34;
        logger.info(f&#34;Executing {task_name} with quality checks&#34;)

        for attempt in range(1, self.max_retries + 1):
            logger.info(f&#34;{task_name}: Attempt {attempt}/{self.max_retries}&#34;)

            # Execute the task
            try:
                output = task_function(**kwargs)
            except Exception as e:
                logger.error(f&#34;{task_name}: Execution failed on attempt {attempt}: {e}&#34;)
                if attempt == self.max_retries:
                    raise
                continue

            # Evaluate quality
            try:
                quality = self.evaluator.evaluate(output)
                logger.info(
                    f&#34;{task_name}: Quality score {quality.score:.1f}/10.0 &#34;
                    f&#34;({&#39;PASS&#39; if quality.passed else &#39;FAIL&#39;})&#34;
                )
            except Exception as e:
                logger.error(f&#34;{task_name}: Quality evaluation failed: {e}&#34;)
                # If evaluation fails, accept output with warning
                quality = QualityScore(
                    score=0.0,
                    passed=False,
                    feedback=f&#34;Failed to parse: {e!s}&#34;,
                    details={&#34;evaluation_error&#34;: True},
                )

            # Check if quality threshold is met
            if quality.passed:
                logger.info(
                    f&#34;{task_name}: Quality threshold met on attempt {attempt}. &#34;
                    f&#34;Score: {quality.score:.1f}/10.0&#34;
                )
                return output, quality, attempt

            # Provide feedback for retry (if not last attempt)
            if attempt &lt; self.max_retries:
                logger.warning(
                    f&#34;{task_name}: Quality threshold not met &#34;
                    f&#34;(score: {quality.score:.1f}/{self.pass_threshold:.1f}). &#34;
                    f&#34;Feedback: {quality.feedback}&#34;
                )

                # Add feedback to kwargs for next iteration
                if &#34;feedback_history&#34; not in kwargs:
                    kwargs[&#34;feedback_history&#34;] = []
                kwargs[&#34;feedback_history&#34;].append(
                    {
                        &#34;attempt&#34;: attempt,
                        &#34;score&#34;: quality.score,
                        &#34;feedback&#34;: quality.feedback,
                        &#34;details&#34;: quality.details,
                    }
                )
            else:
                # Max retries reached
                logger.warning(
                    f&#34;{task_name}: Max retries ({self.max_retries}) reached. &#34;
                    f&#34;Accepting output with quality score {quality.score:.1f}/10.0. &#34;
                    f&#34;Feedback: {quality.feedback}&#34;
                )

        # Return final attempt even if it didn&#39;t pass
        return output, quality, self.max_retries</code></pre>
</details>
<div class="desc"><p>Wrapper for task execution with quality checking and retry logic.</p>
<p>This class wraps task execution to add quality evaluation after execution,
implementing a retry loop that provides feedback for improvement.</p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; task_wrapper = TaskWithQualityCheck(
...     task_type=TaskType.PLOT,
...     pass_threshold=7.0,
...     max_retries=3
... )
&gt;&gt;&gt; result = task_wrapper.execute(task_function, task_name=&quot;Plot Generation&quot;)
</code></pre>
<p>Initialize the task wrapper with quality checking.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task_type</code></strong></dt>
<dd>Type of task to execute (determines which evaluator to use)</dd>
<dt><strong><code>pass_threshold</code></strong></dt>
<dd>Minimum quality score required to pass (0.0-10.0)</dd>
<dt><strong><code>max_retries</code></strong></dt>
<dd>Maximum number of retry attempts (default: 3)</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If parameters are invalid</dd>
</dl></div>
<h3>Methods</h3>
<dl>
<dt id="space_hulk_game.quality.TaskWithQualityCheck.execute"><code class="name flex">
<span>def <span class="ident">execute</span></span>(<span>self, task_function: Callable[..., str], task_name: str = 'Task', **kwargs) ‑> tuple[str, <a title="space_hulk_game.quality.score.QualityScore" href="score.html#space_hulk_game.quality.score.QualityScore">QualityScore</a>, int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute(
    self, task_function: Callable[..., str], task_name: str = &#34;Task&#34;, **kwargs
) -&gt; tuple[str, QualityScore, int]:
    &#34;&#34;&#34;
    Execute task with quality checking and retry logic.

    Args:
        task_function: Function to execute (should return output string)
        task_name: Human-readable name for logging
        **kwargs: Additional arguments to pass to task_function

    Returns:
        Tuple of (output, final_quality_score, attempts_made)
    &#34;&#34;&#34;
    logger.info(f&#34;Executing {task_name} with quality checks&#34;)

    for attempt in range(1, self.max_retries + 1):
        logger.info(f&#34;{task_name}: Attempt {attempt}/{self.max_retries}&#34;)

        # Execute the task
        try:
            output = task_function(**kwargs)
        except Exception as e:
            logger.error(f&#34;{task_name}: Execution failed on attempt {attempt}: {e}&#34;)
            if attempt == self.max_retries:
                raise
            continue

        # Evaluate quality
        try:
            quality = self.evaluator.evaluate(output)
            logger.info(
                f&#34;{task_name}: Quality score {quality.score:.1f}/10.0 &#34;
                f&#34;({&#39;PASS&#39; if quality.passed else &#39;FAIL&#39;})&#34;
            )
        except Exception as e:
            logger.error(f&#34;{task_name}: Quality evaluation failed: {e}&#34;)
            # If evaluation fails, accept output with warning
            quality = QualityScore(
                score=0.0,
                passed=False,
                feedback=f&#34;Failed to parse: {e!s}&#34;,
                details={&#34;evaluation_error&#34;: True},
            )

        # Check if quality threshold is met
        if quality.passed:
            logger.info(
                f&#34;{task_name}: Quality threshold met on attempt {attempt}. &#34;
                f&#34;Score: {quality.score:.1f}/10.0&#34;
            )
            return output, quality, attempt

        # Provide feedback for retry (if not last attempt)
        if attempt &lt; self.max_retries:
            logger.warning(
                f&#34;{task_name}: Quality threshold not met &#34;
                f&#34;(score: {quality.score:.1f}/{self.pass_threshold:.1f}). &#34;
                f&#34;Feedback: {quality.feedback}&#34;
            )

            # Add feedback to kwargs for next iteration
            if &#34;feedback_history&#34; not in kwargs:
                kwargs[&#34;feedback_history&#34;] = []
            kwargs[&#34;feedback_history&#34;].append(
                {
                    &#34;attempt&#34;: attempt,
                    &#34;score&#34;: quality.score,
                    &#34;feedback&#34;: quality.feedback,
                    &#34;details&#34;: quality.details,
                }
            )
        else:
            # Max retries reached
            logger.warning(
                f&#34;{task_name}: Max retries ({self.max_retries}) reached. &#34;
                f&#34;Accepting output with quality score {quality.score:.1f}/10.0. &#34;
                f&#34;Feedback: {quality.feedback}&#34;
            )

    # Return final attempt even if it didn&#39;t pass
    return output, quality, self.max_retries</code></pre>
</details>
<div class="desc"><p>Execute task with quality checking and retry logic.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>task_function</code></strong></dt>
<dd>Function to execute (should return output string)</dd>
<dt><strong><code>task_name</code></strong></dt>
<dd>Human-readable name for logging</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Additional arguments to pass to task_function</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Tuple of (output, final_quality_score, attempts_made)</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="space_hulk_game" href="../index.html">space_hulk_game</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="space_hulk_game.quality.evaluator" href="evaluator.html">space_hulk_game.quality.evaluator</a></code></li>
<li><code><a title="space_hulk_game.quality.integration" href="integration.html">space_hulk_game.quality.integration</a></code></li>
<li><code><a title="space_hulk_game.quality.mechanics_evaluator" href="mechanics_evaluator.html">space_hulk_game.quality.mechanics_evaluator</a></code></li>
<li><code><a title="space_hulk_game.quality.mechanics_metrics" href="mechanics_metrics.html">space_hulk_game.quality.mechanics_metrics</a></code></li>
<li><code><a title="space_hulk_game.quality.narrative_evaluator" href="narrative_evaluator.html">space_hulk_game.quality.narrative_evaluator</a></code></li>
<li><code><a title="space_hulk_game.quality.narrative_metrics" href="narrative_metrics.html">space_hulk_game.quality.narrative_metrics</a></code></li>
<li><code><a title="space_hulk_game.quality.plot_evaluator" href="plot_evaluator.html">space_hulk_game.quality.plot_evaluator</a></code></li>
<li><code><a title="space_hulk_game.quality.plot_metrics" href="plot_metrics.html">space_hulk_game.quality.plot_metrics</a></code></li>
<li><code><a title="space_hulk_game.quality.puzzle_evaluator" href="puzzle_evaluator.html">space_hulk_game.quality.puzzle_evaluator</a></code></li>
<li><code><a title="space_hulk_game.quality.puzzle_metrics" href="puzzle_metrics.html">space_hulk_game.quality.puzzle_metrics</a></code></li>
<li><code><a title="space_hulk_game.quality.retry" href="retry.html">space_hulk_game.quality.retry</a></code></li>
<li><code><a title="space_hulk_game.quality.scene_evaluator" href="scene_evaluator.html">space_hulk_game.quality.scene_evaluator</a></code></li>
<li><code><a title="space_hulk_game.quality.scene_metrics" href="scene_metrics.html">space_hulk_game.quality.scene_metrics</a></code></li>
<li><code><a title="space_hulk_game.quality.score" href="score.html">space_hulk_game.quality.score</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="space_hulk_game.quality.create_quality_config" href="#space_hulk_game.quality.create_quality_config">create_quality_config</a></code></li>
<li><code><a title="space_hulk_game.quality.execute_with_optional_quality_check" href="#space_hulk_game.quality.execute_with_optional_quality_check">execute_with_optional_quality_check</a></code></li>
<li><code><a title="space_hulk_game.quality.execute_with_quality_check" href="#space_hulk_game.quality.execute_with_quality_check">execute_with_quality_check</a></code></li>
<li><code><a title="space_hulk_game.quality.get_default_executor" href="#space_hulk_game.quality.get_default_executor">get_default_executor</a></code></li>
<li><code><a title="space_hulk_game.quality.get_task_type_for_crew_task" href="#space_hulk_game.quality.get_task_type_for_crew_task">get_task_type_for_crew_task</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="space_hulk_game.quality.MechanicsEvaluator" href="#space_hulk_game.quality.MechanicsEvaluator">MechanicsEvaluator</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.MechanicsEvaluator.evaluate" href="#space_hulk_game.quality.MechanicsEvaluator.evaluate">evaluate</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsEvaluator.generate_detailed_feedback" href="#space_hulk_game.quality.MechanicsEvaluator.generate_detailed_feedback">generate_detailed_feedback</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.MechanicsMetrics" href="#space_hulk_game.quality.MechanicsMetrics">MechanicsMetrics</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.average_rule_clarity" href="#space_hulk_game.quality.MechanicsMetrics.average_rule_clarity">average_rule_clarity</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.completeness_percentage" href="#space_hulk_game.quality.MechanicsMetrics.completeness_percentage">completeness_percentage</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.from_dict" href="#space_hulk_game.quality.MechanicsMetrics.from_dict">from_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.from_yaml_content" href="#space_hulk_game.quality.MechanicsMetrics.from_yaml_content">from_yaml_content</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.get_failures" href="#space_hulk_game.quality.MechanicsMetrics.get_failures">get_failures</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.get_score" href="#space_hulk_game.quality.MechanicsMetrics.get_score">get_score</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.has_balance_notes" href="#space_hulk_game.quality.MechanicsMetrics.has_balance_notes">has_balance_notes</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.has_combat_system" href="#space_hulk_game.quality.MechanicsMetrics.has_combat_system">has_combat_system</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.has_inventory_system" href="#space_hulk_game.quality.MechanicsMetrics.has_inventory_system">has_inventory_system</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.has_movement_system" href="#space_hulk_game.quality.MechanicsMetrics.has_movement_system">has_movement_system</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.has_progression_system" href="#space_hulk_game.quality.MechanicsMetrics.has_progression_system">has_progression_system</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.min_clarity" href="#space_hulk_game.quality.MechanicsMetrics.min_clarity">min_clarity</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.min_completeness" href="#space_hulk_game.quality.MechanicsMetrics.min_completeness">min_completeness</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.min_systems" href="#space_hulk_game.quality.MechanicsMetrics.min_systems">min_systems</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.passes_threshold" href="#space_hulk_game.quality.MechanicsMetrics.passes_threshold">passes_threshold</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.systems_with_rules" href="#space_hulk_game.quality.MechanicsMetrics.systems_with_rules">systems_with_rules</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.to_dict" href="#space_hulk_game.quality.MechanicsMetrics.to_dict">to_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.MechanicsMetrics.total_systems" href="#space_hulk_game.quality.MechanicsMetrics.total_systems">total_systems</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.NarrativeMapEvaluator" href="#space_hulk_game.quality.NarrativeMapEvaluator">NarrativeMapEvaluator</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.NarrativeMapEvaluator.evaluate" href="#space_hulk_game.quality.NarrativeMapEvaluator.evaluate">evaluate</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMapEvaluator.generate_detailed_feedback" href="#space_hulk_game.quality.NarrativeMapEvaluator.generate_detailed_feedback">generate_detailed_feedback</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.NarrativeMetrics" href="#space_hulk_game.quality.NarrativeMetrics">NarrativeMetrics</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.all_connections_valid" href="#space_hulk_game.quality.NarrativeMetrics.all_connections_valid">all_connections_valid</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.completeness_percentage" href="#space_hulk_game.quality.NarrativeMetrics.completeness_percentage">completeness_percentage</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.from_dict" href="#space_hulk_game.quality.NarrativeMetrics.from_dict">from_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.from_yaml_content" href="#space_hulk_game.quality.NarrativeMetrics.from_yaml_content">from_yaml_content</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.get_failures" href="#space_hulk_game.quality.NarrativeMetrics.get_failures">get_failures</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.get_score" href="#space_hulk_game.quality.NarrativeMetrics.get_score">get_score</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.has_orphaned_scenes" href="#space_hulk_game.quality.NarrativeMetrics.has_orphaned_scenes">has_orphaned_scenes</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.has_start_scene" href="#space_hulk_game.quality.NarrativeMetrics.has_start_scene">has_start_scene</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.min_completeness" href="#space_hulk_game.quality.NarrativeMetrics.min_completeness">min_completeness</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.min_scenes" href="#space_hulk_game.quality.NarrativeMetrics.min_scenes">min_scenes</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.orphaned_scenes" href="#space_hulk_game.quality.NarrativeMetrics.orphaned_scenes">orphaned_scenes</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.passes_threshold" href="#space_hulk_game.quality.NarrativeMetrics.passes_threshold">passes_threshold</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.scenes_with_descriptions" href="#space_hulk_game.quality.NarrativeMetrics.scenes_with_descriptions">scenes_with_descriptions</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.to_dict" href="#space_hulk_game.quality.NarrativeMetrics.to_dict">to_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.NarrativeMetrics.total_scenes" href="#space_hulk_game.quality.NarrativeMetrics.total_scenes">total_scenes</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.PlotEvaluator" href="#space_hulk_game.quality.PlotEvaluator">PlotEvaluator</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.PlotEvaluator.evaluate" href="#space_hulk_game.quality.PlotEvaluator.evaluate">evaluate</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotEvaluator.generate_detailed_feedback" href="#space_hulk_game.quality.PlotEvaluator.generate_detailed_feedback">generate_detailed_feedback</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.PlotMetrics" href="#space_hulk_game.quality.PlotMetrics">PlotMetrics</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.PlotMetrics.branching_paths_count" href="#space_hulk_game.quality.PlotMetrics.branching_paths_count">branching_paths_count</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.endings_count" href="#space_hulk_game.quality.PlotMetrics.endings_count">endings_count</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.from_dict" href="#space_hulk_game.quality.PlotMetrics.from_dict">from_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.from_yaml_content" href="#space_hulk_game.quality.PlotMetrics.from_yaml_content">from_yaml_content</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.get_failures" href="#space_hulk_game.quality.PlotMetrics.get_failures">get_failures</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.get_score" href="#space_hulk_game.quality.PlotMetrics.get_score">get_score</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.has_acts" href="#space_hulk_game.quality.PlotMetrics.has_acts">has_acts</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.has_clear_setting" href="#space_hulk_game.quality.PlotMetrics.has_clear_setting">has_clear_setting</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.has_prologue" href="#space_hulk_game.quality.PlotMetrics.has_prologue">has_prologue</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.has_title" href="#space_hulk_game.quality.PlotMetrics.has_title">has_title</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.min_branching_paths" href="#space_hulk_game.quality.PlotMetrics.min_branching_paths">min_branching_paths</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.min_endings" href="#space_hulk_game.quality.PlotMetrics.min_endings">min_endings</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.min_word_count" href="#space_hulk_game.quality.PlotMetrics.min_word_count">min_word_count</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.passes_threshold" href="#space_hulk_game.quality.PlotMetrics.passes_threshold">passes_threshold</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.themes_defined" href="#space_hulk_game.quality.PlotMetrics.themes_defined">themes_defined</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.to_dict" href="#space_hulk_game.quality.PlotMetrics.to_dict">to_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.PlotMetrics.word_count" href="#space_hulk_game.quality.PlotMetrics.word_count">word_count</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.PuzzleEvaluator" href="#space_hulk_game.quality.PuzzleEvaluator">PuzzleEvaluator</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.PuzzleEvaluator.evaluate" href="#space_hulk_game.quality.PuzzleEvaluator.evaluate">evaluate</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleEvaluator.generate_detailed_feedback" href="#space_hulk_game.quality.PuzzleEvaluator.generate_detailed_feedback">generate_detailed_feedback</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.PuzzleMetrics" href="#space_hulk_game.quality.PuzzleMetrics">PuzzleMetrics</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.from_dict" href="#space_hulk_game.quality.PuzzleMetrics.from_dict">from_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.from_yaml_content" href="#space_hulk_game.quality.PuzzleMetrics.from_yaml_content">from_yaml_content</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.get_failures" href="#space_hulk_game.quality.PuzzleMetrics.get_failures">get_failures</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.get_score" href="#space_hulk_game.quality.PuzzleMetrics.get_score">get_score</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.has_artifacts" href="#space_hulk_game.quality.PuzzleMetrics.has_artifacts">has_artifacts</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.has_monsters" href="#space_hulk_game.quality.PuzzleMetrics.has_monsters">has_monsters</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.has_npcs" href="#space_hulk_game.quality.PuzzleMetrics.has_npcs">has_npcs</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.min_puzzles" href="#space_hulk_game.quality.PuzzleMetrics.min_puzzles">min_puzzles</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.min_solution_percentage" href="#space_hulk_game.quality.PuzzleMetrics.min_solution_percentage">min_solution_percentage</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.passes_threshold" href="#space_hulk_game.quality.PuzzleMetrics.passes_threshold">passes_threshold</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.puzzles_with_difficulty" href="#space_hulk_game.quality.PuzzleMetrics.puzzles_with_difficulty">puzzles_with_difficulty</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.puzzles_with_narrative_ties" href="#space_hulk_game.quality.PuzzleMetrics.puzzles_with_narrative_ties">puzzles_with_narrative_ties</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.puzzles_with_solutions" href="#space_hulk_game.quality.PuzzleMetrics.puzzles_with_solutions">puzzles_with_solutions</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.to_dict" href="#space_hulk_game.quality.PuzzleMetrics.to_dict">to_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.PuzzleMetrics.total_puzzles" href="#space_hulk_game.quality.PuzzleMetrics.total_puzzles">total_puzzles</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.QualityCheckConfig" href="#space_hulk_game.quality.QualityCheckConfig">QualityCheckConfig</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.QualityCheckConfig.get_task_config" href="#space_hulk_game.quality.QualityCheckConfig.get_task_config">get_task_config</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityCheckConfig.is_enabled" href="#space_hulk_game.quality.QualityCheckConfig.is_enabled">is_enabled</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.QualityEvaluator" href="#space_hulk_game.quality.QualityEvaluator">QualityEvaluator</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.QualityEvaluator.evaluate" href="#space_hulk_game.quality.QualityEvaluator.evaluate">evaluate</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityEvaluator.generate_feedback" href="#space_hulk_game.quality.QualityEvaluator.generate_feedback">generate_feedback</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityEvaluator.parse_yaml" href="#space_hulk_game.quality.QualityEvaluator.parse_yaml">parse_yaml</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityEvaluator.score" href="#space_hulk_game.quality.QualityEvaluator.score">score</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.QualityScore" href="#space_hulk_game.quality.QualityScore">QualityScore</a></code></h4>
<ul class="two-column">
<li><code><a title="space_hulk_game.quality.QualityScore.details" href="#space_hulk_game.quality.QualityScore.details">details</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.feedback" href="#space_hulk_game.quality.QualityScore.feedback">feedback</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.from_dict" href="#space_hulk_game.quality.QualityScore.from_dict">from_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.get_failures" href="#space_hulk_game.quality.QualityScore.get_failures">get_failures</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.get_summary" href="#space_hulk_game.quality.QualityScore.get_summary">get_summary</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.passed" href="#space_hulk_game.quality.QualityScore.passed">passed</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.score" href="#space_hulk_game.quality.QualityScore.score">score</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.to_dict" href="#space_hulk_game.quality.QualityScore.to_dict">to_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.QualityScore.to_json" href="#space_hulk_game.quality.QualityScore.to_json">to_json</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.SceneEvaluator" href="#space_hulk_game.quality.SceneEvaluator">SceneEvaluator</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.SceneEvaluator.evaluate" href="#space_hulk_game.quality.SceneEvaluator.evaluate">evaluate</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneEvaluator.generate_detailed_feedback" href="#space_hulk_game.quality.SceneEvaluator.generate_detailed_feedback">generate_detailed_feedback</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.SceneMetrics" href="#space_hulk_game.quality.SceneMetrics">SceneMetrics</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.SceneMetrics.average_description_length" href="#space_hulk_game.quality.SceneMetrics.average_description_length">average_description_length</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.from_dict" href="#space_hulk_game.quality.SceneMetrics.from_dict">from_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.from_yaml_content" href="#space_hulk_game.quality.SceneMetrics.from_yaml_content">from_yaml_content</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.get_failures" href="#space_hulk_game.quality.SceneMetrics.get_failures">get_failures</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.get_score" href="#space_hulk_game.quality.SceneMetrics.get_score">get_score</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.has_sensory_details" href="#space_hulk_game.quality.SceneMetrics.has_sensory_details">has_sensory_details</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.min_description_length" href="#space_hulk_game.quality.SceneMetrics.min_description_length">min_description_length</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.min_scenes" href="#space_hulk_game.quality.SceneMetrics.min_scenes">min_scenes</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.min_vivid_percentage" href="#space_hulk_game.quality.SceneMetrics.min_vivid_percentage">min_vivid_percentage</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.passes_threshold" href="#space_hulk_game.quality.SceneMetrics.passes_threshold">passes_threshold</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.scenes_with_dialogue" href="#space_hulk_game.quality.SceneMetrics.scenes_with_dialogue">scenes_with_dialogue</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.scenes_with_vivid_descriptions" href="#space_hulk_game.quality.SceneMetrics.scenes_with_vivid_descriptions">scenes_with_vivid_descriptions</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.to_dict" href="#space_hulk_game.quality.SceneMetrics.to_dict">to_dict</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.tone_consistency_score" href="#space_hulk_game.quality.SceneMetrics.tone_consistency_score">tone_consistency_score</a></code></li>
<li><code><a title="space_hulk_game.quality.SceneMetrics.total_scenes" href="#space_hulk_game.quality.SceneMetrics.total_scenes">total_scenes</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.TaskExecutor" href="#space_hulk_game.quality.TaskExecutor">TaskExecutor</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.TaskExecutor.execute_task" href="#space_hulk_game.quality.TaskExecutor.execute_task">execute_task</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.TaskType" href="#space_hulk_game.quality.TaskType">TaskType</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.TaskType.MECHANICS" href="#space_hulk_game.quality.TaskType.MECHANICS">MECHANICS</a></code></li>
<li><code><a title="space_hulk_game.quality.TaskType.NARRATIVE" href="#space_hulk_game.quality.TaskType.NARRATIVE">NARRATIVE</a></code></li>
<li><code><a title="space_hulk_game.quality.TaskType.PLOT" href="#space_hulk_game.quality.TaskType.PLOT">PLOT</a></code></li>
<li><code><a title="space_hulk_game.quality.TaskType.PUZZLE" href="#space_hulk_game.quality.TaskType.PUZZLE">PUZZLE</a></code></li>
<li><code><a title="space_hulk_game.quality.TaskType.SCENE" href="#space_hulk_game.quality.TaskType.SCENE">SCENE</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="space_hulk_game.quality.TaskWithQualityCheck" href="#space_hulk_game.quality.TaskWithQualityCheck">TaskWithQualityCheck</a></code></h4>
<ul class="">
<li><code><a title="space_hulk_game.quality.TaskWithQualityCheck.execute" href="#space_hulk_game.quality.TaskWithQualityCheck.execute">execute</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
