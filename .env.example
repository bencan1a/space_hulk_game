# Space Hulk Game - Environment Configuration Template
#
# Copy this file to .env and configure as needed:
#   cp .env.example .env
#
# Then edit .env with your actual values.

# ==============================================================================
# LLM Configuration
# ==============================================================================

# Option 1: Use Ollama (Local, Free) - DEFAULT
# Requires Ollama to be installed and running (ollama serve)
OPENAI_MODEL_NAME=ollama/qwen2.5
OLLAMA_BASE_URL=http://localhost:11434

# Option 2: Use OpenAI API (Cloud, Paid)
# Uncomment these lines and comment out the Ollama lines above
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL_NAME=gpt-4
# OPENAI_MODEL_NAME=gpt-3.5-turbo  # Alternative, cheaper option

# Option 3: Use Anthropic Claude (Cloud, Paid)
# Get your API key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# OPENAI_MODEL_NAME=claude-3-5-sonnet-20241022
# Available models:
# - claude-3-5-sonnet-20241022 (recommended, most capable)
# - claude-3-opus-20240229 (most powerful)
# - claude-3-sonnet-20240229 (balanced)
# - claude-3-haiku-20240307 (fastest, cheapest)

# Option 4: Use OpenRouter (Cloud, Paid - Access to multiple providers)
# Get your API key from: https://openrouter.ai/keys
# OpenRouter provides access to various models including Claude, GPT-4, Llama, etc.
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# OPENAI_MODEL_NAME=openrouter/anthropic/claude-3.5-sonnet
# OPENAI_MODEL_NAME=openrouter/openai/gpt-4-turbo
# OPENAI_MODEL_NAME=openrouter/meta-llama/llama-3.1-70b-instruct
# See all models at: https://openrouter.ai/models

# Option 5: Use Azure OpenAI (Enterprise)
# AZURE_API_KEY=your-azure-key-here
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-15-preview
# OPENAI_MODEL_NAME=azure/your-deployment-name

# More providers supported by litellm: https://docs.litellm.ai/docs/providers
# Including: Cohere, Hugging Face, Replicate, Vertex AI, Bedrock, and more

# ==============================================================================
# Memory Configuration
# ==============================================================================

# Option 1: Local Memory (Default)
# No configuration needed - uses local memory by default

# Option 2: Mem0 Cloud Memory
# Uncomment and add your Mem0 API key for cloud-based memory
# Get your key from: https://mem0.ai/
# MEM0_API_KEY=your-mem0-api-key-here

# ==============================================================================
# Application Configuration
# ==============================================================================

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# CrewAI Configuration (optional)
# Uncomment to customize CrewAI behavior
# CREWAI_VERBOSE=true
# CREWAI_MEMORY=true

# ==============================================================================
# Development Configuration
# ==============================================================================

# Enable debug mode (optional)
# DEBUG=true

# LiteLLM debug mode (optional, for debugging LLM calls)
# Uncomment to enable detailed LLM request/response logging
# LITELLM_LOG=DEBUG

# ==============================================================================
# CI/CD and Test Environment Configuration
# ==============================================================================

# For GitHub Actions, CI/CD pipelines, or containerized environments:
# Set these variables as environment secrets/variables in your CI platform
# instead of using a .env file.
#
# GitHub Actions example (in workflow file):
#   env:
#     OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
#     OPENAI_MODEL_NAME: gpt-4
#
# Docker/Docker Compose example:
#   docker run -e OPENAI_API_KEY=$OPENAI_API_KEY -e OPENAI_MODEL_NAME=gpt-4 ...
#
# The application will use environment variables directly if they're set,
# falling back to .env file values if not found in the environment.
