# Space Hulk Game - Environment Configuration Template
#
# Copy this file to .env and configure as needed:
#   cp .env.example .env
#
# Then edit .env with your actual values.

# ==============================================================================
# LLM Configuration
# ==============================================================================

# Option 1: Use Ollama (Local, Free) - DEFAULT
# Requires Ollama to be installed and running (ollama serve)
OPENAI_MODEL_NAME=ollama/qwen2.5
OLLAMA_BASE_URL=http://localhost:11434

# Option 2: Use OpenAI API (Cloud, Paid)
# Uncomment these lines and comment out the Ollama lines above
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL_NAME=gpt-4
# OPENAI_MODEL_NAME=gpt-3.5-turbo  # Alternative, cheaper option

# Option 3: Use other LLM providers supported by litellm
# See: https://docs.litellm.ai/docs/providers
# Examples:
# Anthropic Claude
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# OPENAI_MODEL_NAME=claude-3-opus-20240229

# Azure OpenAI
# AZURE_API_KEY=your-azure-key-here
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-15-preview
# OPENAI_MODEL_NAME=azure/your-deployment-name

# ==============================================================================
# Memory Configuration
# ==============================================================================

# Option 1: Local Memory (Default)
# No configuration needed - uses local memory by default

# Option 2: Mem0 Cloud Memory
# Uncomment and add your Mem0 API key for cloud-based memory
# Get your key from: https://mem0.ai/
# MEM0_API_KEY=your-mem0-api-key-here

# ==============================================================================
# Application Configuration
# ==============================================================================

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# CrewAI Configuration (optional)
# Uncomment to customize CrewAI behavior
# CREWAI_VERBOSE=true
# CREWAI_MEMORY=true

# ==============================================================================
# Development Configuration
# ==============================================================================

# Enable debug mode (optional)
# DEBUG=true

# LiteLLM debug mode (optional, for debugging LLM calls)
# Uncomment to enable detailed LLM request/response logging
# LITELLM_LOG=DEBUG
