# Space Hulk Game - Environment Configuration Template
#
# Copy this file to .env and configure as needed:
#   cp .env.example .env
#
# Then edit .env with your actual values.

# ==============================================================================
# LLM Configuration
# ==============================================================================

# Option 1: Use Ollama (Local, Free) - DEFAULT
# Requires Ollama to be installed and running (ollama serve)
OPENAI_MODEL_NAME=ollama/qwen2.5
OLLAMA_BASE_URL=http://localhost:11434

# Option 2: Use OpenAI API (Cloud, Paid)
# Uncomment these lines and comment out the Ollama lines above
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL_NAME=gpt-4
# OPENAI_MODEL_NAME=gpt-3.5-turbo  # Alternative, cheaper option

# Option 3: Use Anthropic Claude (Cloud, Paid)
# Get your API key from: https://console.anthropic.com/
# ANTHROPIC_API_KEY=sk-ant-your-key-here
# OPENAI_MODEL_NAME=claude-3-5-sonnet-20241022
# Available models:
# - claude-3-5-sonnet-20241022 (recommended, most capable)
# - claude-3-opus-20240229 (most powerful)
# - claude-3-sonnet-20240229 (balanced)
# - claude-3-haiku-20240307 (fastest, cheapest)

# Option 4: Use OpenRouter (Cloud, Paid - Access to multiple providers)
# Get your API key from: https://openrouter.ai/keys
# OpenRouter provides access to various models including Claude, GPT-4, Llama, etc.
# OPENROUTER_API_KEY=sk-or-v1-your-key-here
# OPENAI_MODEL_NAME=openrouter/anthropic/claude-3.5-sonnet
# OPENAI_MODEL_NAME=openrouter/openai/gpt-4-turbo
# OPENAI_MODEL_NAME=openrouter/meta-llama/llama-3.1-70b-instruct
# See all models at: https://openrouter.ai/models

# Option 5: Use Azure OpenAI (Enterprise)
# AZURE_API_KEY=your-azure-key-here
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-15-preview
# OPENAI_MODEL_NAME=azure/your-deployment-name

# More providers supported by litellm: https://docs.litellm.ai/docs/providers
# Including: Cohere, Hugging Face, Replicate, Vertex AI, Bedrock, and more

# ==============================================================================
# Memory Configuration
# ==============================================================================

# Memory improves narrative consistency and reduces token usage by 70-90%
# See docs/MEM0_SETUP_GUIDE.md for detailed setup instructions

# Option 1: Basic Memory (Default - Built-in CrewAI)
# No configuration needed - uses ChromaDB + SQLite locally
# Good for: Development, testing, learning
# To enable: Set memory=True in crew.py

# Option 2: Cloud Mem0 (Managed Service)
# Best for: Quick start, no infrastructure management
# Get your API key from: https://mem0.ai/
# MEM0_API_KEY=m0-your-api-key-here

# Option 3: Local Mem0 (Self-hosted, Full Privacy)
# Best for: Production, data privacy, offline use
# Requires: Qdrant (or ChromaDB) + Ollama embedding model
# Setup: python configure_mem0.py --mode local

# Memory Provider Selection
# MEM0_PROVIDER=local          # Use local mem0 (requires config below)
# MEM0_PROVIDER=cloud          # Use cloud mem0 (requires API key)

# Local Mem0 Configuration (Option 3)
# Vector Store Settings
# MEM0_VECTOR_STORE=qdrant                    # or 'chroma'
# MEM0_VECTOR_HOST=localhost
# MEM0_VECTOR_PORT=6333
# MEM0_COLLECTION_NAME=space_hulk_narratives

# Embedding Model Settings (for semantic search)
# MEM0_EMBEDDER_PROVIDER=ollama               # or 'openai'
# MEM0_EMBEDDER_MODEL=mxbai-embed-large       # Best for narratives
# MEM0_EMBEDDER_BASE_URL=http://localhost:11434

# Memory LLM Settings (for memory processing)
# MEM0_LLM_PROVIDER=ollama                    # or 'openai', 'anthropic'
# MEM0_LLM_MODEL=qwen2.5
# MEM0_LLM_BASE_URL=http://localhost:11434
# MEM0_LLM_TEMPERATURE=0.2
# MEM0_LLM_MAX_TOKENS=2000

# Quick Setup Commands:
# Cloud mem0:  python configure_mem0.py --mode cloud
# Local mem0:  python configure_mem0.py --mode local
# Basic mem0:  python configure_mem0.py --mode basic

# ==============================================================================
# Application Configuration
# ==============================================================================

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# CrewAI Configuration (optional)
# Uncomment to customize CrewAI behavior
# CREWAI_VERBOSE=true
# CREWAI_MEMORY=true

# Disable CrewAI telemetry (optional)
# Uncomment to disable telemetry collection and avoid connection errors
# to telemetry.crewai.com (useful in firewalled/restricted environments)
# OTEL_SDK_DISABLED=true

# ==============================================================================
# Development Configuration
# ==============================================================================

# Enable debug mode (optional)
# DEBUG=true

# LiteLLM debug mode (optional, for debugging LLM calls)
# Uncomment to enable detailed LLM request/response logging
# LITELLM_LOG=DEBUG

# ==============================================================================
# CI/CD and Test Environment Configuration
# ==============================================================================

# For GitHub Actions, CI/CD pipelines, or containerized environments:
# Set these variables as environment secrets/variables in your CI platform
# instead of using a .env file.
#
# GitHub Actions example (in workflow file):
#   env:
#     OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
#     OPENAI_MODEL_NAME: gpt-4
#
# Docker/Docker Compose example:
#   docker run -e OPENAI_API_KEY=$OPENAI_API_KEY -e OPENAI_MODEL_NAME=gpt-4 ...
#
# The application will use environment variables directly if they're set,
# falling back to .env file values if not found in the environment.

# ==============================================================================
# Docker Compose Configuration
# ==============================================================================

# Backend Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_ENVIRONMENT=development
LOG_LEVEL=INFO

# Database
DATABASE_URL=sqlite:///./data/database.db
# DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}

# PostgreSQL Configuration (when enabled in docker-compose.yml)
# POSTGRES_USER=space_hulk
# POSTGRES_PASSWORD=change_this_in_production
# POSTGRES_DB=space_hulk_db

# Redis
REDIS_URL=redis://redis:6379/0

# CORS
CORS_ORIGINS='["http://localhost:3000"]'

# Frontend Configuration
VITE_API_URL=http://localhost:8000
